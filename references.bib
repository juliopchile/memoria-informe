%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ACUICULTURA Y TÉCNICAS CON VISIÓN %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@book{FAO2018,
  author    = {{United Nations Food and Agriculture Organization (FAO)}},
  title     = {The State of World Fisheries and Aquaculture 2019 (SOFIA): Meeting the Sustainable Development Goals},
  year      = {2018},
  publisher = {FAO},
  address   = {Rome, Italy},
  url       = {https://openknowledge.fao.org/handle/20.500.14283/9540es}
}

@techreport{chile2017levantamiento,
  author      = {{Gobierno de Chile}},
  shortauthor = {{Gob. de Chile}},
  title       = {Levantamiento de Información de Pisciculturas en Chile y su Incorporación a la IDE de la División de Acuicultura},
  institution = {Fondo de Investigación Pesquera; Ministerio de Economía, Fomento y Turismo},
  year        = {2017}
}

@article{Tonachella2022,
  author   = {Tonachella, Nicolò and Martini, Arianna and Martinoli, Marco and Pulcini, Domitilla and Romano, Andrea and Capoccioni, Fabrizio},
  title    = {An affordable and easy-to-use tool for automatic fish length and weight estimation in mariculture},
  journal  = {Scientific Reports},
  date     = {2022-09-19},
  volume   = {12},
  number   = {1},
  pages    = {15642},
  doi      = {10.1038/s41598-022-19932-9},
  issn     = {2045-2322},
  abstract = {Common aquaculture practices involve measuring fish biometrics at different growth stages, which is crucial for feeding regime management and for improving farmed fish welfare. Fish measurements are usually carried out manually on individual fish. However, this process is laborious, time-consuming, and stressful to the fish. Therefore, the development of fast, precise, low cost and indirect measurement would be of great interest to the aquaculture sector. In this study, we explore a promising way to take fish measurements in a non-invasive approach through computer vision. Images captured by a stereoscopic camera are used by Artificial Intelligence algorithms in conjunction with computer vision to automatically obtain an accurate estimation of the characteristics of fish, such as body length and weight. We describe the development of a computer vision system for automated recognition of body traits through image processing and linear models for the measurement of fish length and prediction of body weight. The measurements are obtained through a relatively low-cost prototype consisting of a smart buoy equipped with stereo cameras, tested in a commercial mariculture cage in the Mediterranean Sea. Our findings suggest that this method can successfully estimate fish biometric parameters, with a mean error of ±1.15 cm.}
}

@article{Ahmed2022,
  author   = {Ahmed, Md Shoaib and Aurpa, Tanjim Taharat and Kalam Azad, Md Abul},
  title    = {Fish Disease Detection Using Image Based Machine Learning Technique in Aquaculture},
  journal  = {Journal of King Saud University - Computer and Information Sciences},
  volume   = {34},
  number   = {8, Part A},
  pages    = {5170--5182},
  year     = {2022},
  issn     = {1319-1578},
  doi      = {10.1016/j.jksuci.2021.05.003},
  abstract = {Fish diseases in aquaculture constitute a significant hazard to nutriment security. Identification of infected fishes in aquaculture remains challenging to find out at the early stage due to the dearth of necessary infrastructure. The identification of infected fish timely is an obligatory step to thwart from spreading disease. In this work, we want to find out the salmon fish disease in aquaculture, as salmon aquaculture is the fastest-growing food production system globally, accounting for 70 percent (2.5 million tons) of the market. In the alliance of flawless image processing and machine learning mechanism, we identify the infected fishes caused by the various pathogen. This work divides into two portions. In the rudimentary portion, image pre-processing and segmentation have been applied to reduce noise and exaggerate the image, respectively. In the second portion, we extract the involved features to classify the diseases with the help of the Support Vector Machine (SVM) algorithm of machine learning with a kernel function. The processed images of the first portion have passed through this (SVM) model. Then we harmonize a comprehensive experiment with the proposed combination of techniques on the salmon fish image dataset used to examine the fish disease. We have conveyed this work on a novel dataset compromising with and without image augmentation. The results have bought a judgment of our applied SVM performs notably with 91.42 and 94.12 percent of accuracy, respectively, with and without augmentation.},
  keywords = {Fish Diseases, Aquaculture, Image Processing, Machine Learning, Support Vector Machine, Salmon Fish}
}


%%%%%%%%%%%%%%%%%%%%%%
%% COSAS DESDE LA U %%
%%%%%%%%%%%%%%%%%%%%%%

@online{wildsense-1,
  author  = {{WildSense}},
  title   = {Robótica Submarina Automatizada para Salmonicultura de Alta Mar (R-SASA)},
  url     = {https://wildsense.ai/servicios/robotica-submarina-automatizada-para-salmonicultura-de-alta-mar-r-sasa},
  urldate = {2025-05-25}
}

@slide{wildsense-2,
  author       = {{WildSense}},
  title        = {Memoria Anual 2024},
  location     = {Valparaíso, Chile},
  year         = {2024},
  url          = {https://wildsense.ai/wp-content/uploads/2024/01/Memoria-Anual-2024.pdf},
  urldate      = {2025-05-25},
  howpublished = {Diagrama de flujo en la página 10 e imágenes en la página 14}
}

@thesis{Guerrero2024,
  author     = {Guerrero Loyola, Alejandro Felipe},
  title      = {Análisis del modelo YOLO-V8 para la segmentación de salmones en jaulas de cultivo en proceso estimador de biomasa, en tiempo real},
  type       = {Memoria de titulación},
  titleaddon = {Ing. Civil Electrónica},
  advisor    = {Zuñiga, Marcos},
  location   = {Valparaíso, Chile},
  school     = {UTFSM},
  date       = {2024-02},
  note       = {Advisor: Marcos Zuñiga; Prof. Correferente: Gonzalo Carvajal},
  url        = {https://repositorio.usm.cl/handle/123456789/74331}
}

@online{Lopez2024,
  author  = {López Blanche, Julio Eduardo},
  title   = {Segmentación de Salmones para estimación de masa en salmonicultura.},
  type    = {Informe \textit{Benchmark} del proyecto final en \textit{Visión por Computador}},
  date    = {2024-07-10},
  url     = {https://github.com/juliopchile/CV_Project},
  urldate = {2025-06-18}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% EVOLUCÓN DE SEGMENTACIÓN DE INSTANCIAS EN IMAGNES %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@book{Szeliski2022,
  author     = {Szeliski, Richard},
  title      = {Computer Vision: Algorithms and Applications},
  edition    = {2},
  series     = {Texts in Computer Science},
  publisher  = {Springer Cham},
  date       = {2022-01-03},
  doi        = {10.1007/978-3-030-34372-9},
  totalpages = {925},
  keywords   = {Image Processing and Computer Vision, Computer Imaging, Vision, Pattern Recognition and Graphics, Machine Learning, Signal, Image and Speech Processing, Materials Science}
}

@article{Hafiz2020,
  author   = {Hafiz, Abdul Mueed and Bhat, Ghulam Mohiuddin},
  title    = {A survey on instance segmentation: state of the art},
  journal  = {International Journal of Multimedia Information Retrieval},
  volume   = {9},
  number   = {3},
  pages    = {171--189},
  date     = {2020-09-01},
  issn     = {2192-662X},
  doi      = {10.1007/s13735-020-00195-x},
  abstract = {Object detection or localization is an incremental step in progression from coarse to fine digital image inference. It not only provides the classes of the image objects, but also provides the location of the image objects which have been classified. The location is given in the form of bounding boxes or centroids. Semantic segmentation gives fine inference by predicting labels for every pixel in the input image. Each pixel is labelled according to the object class within which it is enclosed. Furthering this evolution, instance segmentation gives different labels for separate instances of objects belonging to the same class. Hence, instance segmentation may be defined as the technique of simultaneously solving the problem of object detection as well as that of semantic segmentation. In this survey paper on instance segmentation, its background, issues, techniques, evolution, popular datasets, related work up to the state of the art and future scope have been discussed. The paper provides valuable information for those who want to do research in the field of instance segmentation.}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SEGMENTACIÓN DE INSTANCIAS EN VIDEO (VIS) %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{Yang2019,
  author    = {Yang, Linjie and Fan, Yuchen and Xu, Ning},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Video Instance Segmentation},
  date      = {2019-10-27/2019-11-02},
  date      = {2020-02-27},
  pages     = {5187--5196},
  venue     = {Seoul, Korea},
  publisher = {IEEE},
  doi       = {10.1109/ICCV.2019.00529},
  issn      = {2380-7504},
  abstract  = {In this paper we present a new computer vision task, named video instance segmentation. The goal of this new task is simultaneous detection, segmentation and tracking of instances in videos. In words, it is the first time that the image instance segmentation problem is extended to the video domain. To facilitate research on this new task, we propose a large-scale benchmark called YouTube-VIS, which consists of 2,883 high-resolution YouTube videos, a 40-category label set and 131k high-quality instance masks. In addition, we propose a novel algorithm called MaskTrack R-CNN for this task. Our new method introduces a new tracking branch to Mask R-CNN to jointly perform the detection, segmentation and tracking tasks simultaneously. Finally, we evaluate the proposed method and several strong baselines on our new dataset. Experimental results clearly demonstrate the advantages of the proposed algorithm and reveal insight for future improvement. We believe the video instance segmentation task will motivate the community along the line of research for video understanding.},
  keywords  = {Image segmentation;Task analysis;Motion segmentation;Semantics;Benchmark testing;Object detection;Object segmentation}
}

@inproceedings{Voigtlaender2019,
  author    = {Voigtlaender, Paul and Krause, Michael and Osep, Aljosa and Luiten, Jonathon and Sekar, Berin Balachandar Gnana and Geiger, Andreas and Leibe, Bastian},
  booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {MOTS: Multi-Object Tracking and Segmentation},
  eventdate = {2019-06-15/2019-06-20},
  date      = {2020-01-09},
  pages     = {7934--7943},
  venue     = {Long Beach, CA, USA},
  publisher = {IEEE},
  doi       = {10.1109/CVPR.2019.00813},
  issn      = {2575-7075},
  abstract  = {This paper extends the popular task of multi-object tracking to multi-object tracking and segmentation (MOTS). Towards this goal, we create dense pixel-level annotations for two existing tracking datasets using a semi-automatic annotation procedure. Our new annotations comprise 65,213 pixel masks for 977 distinct objects (cars and pedestrians) in 10,870 video frames. For evaluation, we extend existing multi-object tracking metrics to this new task. Moreover, we propose a new baseline method which jointly addresses detection, tracking, and segmentation with a single convolutional network. We demonstrate the value of our datasets by achieving improvements in performance when training on MOTS annotations. We believe that our datasets, metrics and baseline will become a valuable resource towards developing multi-object tracking approaches that go beyond 2D bounding boxes. We make our annotations, code, and models available at https://www.vision.rwth-aachen.de/page/mots.},
  keywords  = {Motion and Tracking;Deep Learning ; Segmentation;Grouping and Shape}
}

@article{Zhou2023,
  author    = {Zhou, Tianfei and Porikli, Fatih and Crandall, David J. and Van Gool, Luc and Wang, Wenguan},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title     = {A Survey on Deep Learning Technique for Video Segmentation},
  date      = {2023-06-01},
  volume    = {45},
  number    = {6},
  pages     = {7099--7122},
  publisher = {IEEE},
  doi       = {10.1109/TPAMI.2022.3225573},
  issn      = {1939-3539},
  abstract  = {Video segmentation—partitioning video frames into multiple segments or objects—plays a critical role in a broad range of practical applications, from enhancing visual effects in movie, to understanding scenes in autonomous driving, to creating virtual background in video conferencing. Recently, with the renaissance of connectionism in computer vision, there has been an influx of deep learning based approaches for video segmentation that have delivered compelling performance. In this survey, we comprehensively review two basic lines of research — generic object segmentation (of unknown categories) in videos, and video semantic segmentation — by introducing their respective task settings, background concepts, perceived need, development history, and main challenges. We also offer a detailed overview of representative literature on both methods and datasets. We further benchmark the reviewed methods on several well-known datasets. Finally, we point out open issues in this field, and suggest opportunities for further research. We also provide a public website to continuously track developments in this fast advancing field: https://github.com/tfzhou/VS-Survey.},
  keywords  = {Object segmentation;Automobiles;Semantic segmentation;Task analysis;Motion segmentation;Deep learning;Roads;Video segmentation;video object segmentation;video semantic segmentation;deep learning}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% EVOLUCIÓN DE MODELOS DE DETECCIÓN Y SEGMENTACIÓN %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{SIFT,
  author    = {Lowe, D.G.},
  title     = {Object recognition from local scale-invariant features},
  booktitle = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
  eventdate = {1999-09-20/1999-09-27},
  date      = {2002-08-06},
  volume    = {2},
  pages     = {1150--1157},
  venue     = {Kerkyra, Greece},
  publisher = {IEEE},
  doi       = {10.1109/ICCV.1999.790410},
  abstract  = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds.},
  keywords  = {Object recognition;Electrical capacitance tomography;Image recognition;Lighting;Neurons;Computer science;Reactive power;Filters;Programmable logic arrays;Layout}
}

@inproceedings{HOG,
  author    = {Dalal, N. and Triggs, Bill},
  booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  title     = {Histograms of oriented gradients for human detection},
  eventdate = {2005-06-20/2005-06-25},
  volume    = {1},
  pages     = {886--893},
  doi       = {10.1109/CVPR.2005.177},
  venue     = {San Diego, CA, USA},
  publisher = {IEEE},
  date      = {2005-07-25},
  issn      = {1063-6919},
  isbn      = {0-7695-2372-2},
  abstract  = {We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
  keywords  = {Histograms;Humans;Robustness;Object recognition;Support vector machines;Object detection;Testing;Image edge detection;High performance computing;Image databases}
}

@inproceedings{BagOfWords,
  author    = {Sivic, J. and Zisserman, A.},
  booktitle = {Proceedings Ninth IEEE International Conference on Computer Vision},
  title     = {Video Google: a text retrieval approach to object matching in videos},
  eventdate = {2003-11-13/2003-11-16},
  date      = {2008-04-03},
  volume    = {2},
  pages     = {1470--1477},
  venue     = {Nice, France},
  publisher = {IEEE},
  doi       = {10.1109/ICCV.2003.1238663},
  isbn      = {0-7695-1950-4},
  abstract  = {We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieved is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching in two full length feature films.},
  keywords  = {Web pages;Lighting;Vector quantization;Image databases;Robots;Layout;Noise reduction;File systems;Object recognition;Visual databases}
}

@inproceedings{FisherVector,
  author     = {Perronnin, Florent and S{\'a}nchez, Jorge and Mensink, Thomas},
  editor     = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
  title      = {Improving the Fisher Kernel for Large-Scale Image Classification},
  eventtitle = {Computer Vision -- ECCV 2010},
  booktitle  = {Proceedings of IEEE European Conference on Computer Vision, 2010},
  eventdate  = {2010-09-05/2010-09-11},
  date       = {2010},
  venue      = {Heraklion, Crete, Greece},
  publisher  = {Springer Berlin Heidelberg},
  address    = {Berlin, Heidelberg},
  pages      = {143--156},
  volume     = {6314},
  doi        = {10.1007/978-3-642-15561-1_11},
  abstract   = {The Fisher kernel (FK) is a generic framework which combines the benefits of generative and discriminative approaches. In the context of image classification the FK was shown to extend the popular bag-of-visual-words (BOV) by going beyond count statistics. However, in practice, this enriched representation has not yet shown its superiority over the BOV. In the first part we show that with several well-motivated modifications over the original framework we can boost the accuracy of the FK. On PASCAL VOC 2007 we increase the Average Precision (AP) from 47.9{\%} to 58.3{\%}. Similarly, we demonstrate state-of-the-art accuracy on CalTech 256. A major advantage is that these results are obtained using only SIFT descriptors and costless linear classifiers. Equipped with this representation, we can now explore image classification on a larger scale. In the second part, as an application, we compare two abundant resources of labeled images to learn classifiers: ImageNet and Flickr groups. In an evaluation involving hundreds of thousands of training images we show that classifiers learned on Flickr groups perform surprisingly well (although they were not intended for this purpose) and that they can complement classifiers learned on more carefully annotated datasets.}
}

@inproceedings{AlexNet,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {25},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  pages     = {1097--1105},
  eid       = {534},
  eventdate = {2012},
  year      = {2012},
  isbn      = {9781627480031},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  abstract  = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\% and 18.9\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.}
}

@inproceedings{VGGNet,
  author     = {Simonyan, Karen and Zisserman, Andrew},
  editor     = {Bengio, Yoshua and LeCun, Yann},
  title      = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  eventtitle = {3rd International Conference on Learning Representations (ICLR 2015)},
  booktitle  = {{ICLR} 2015 Conference Track Proceedings},
  publisher  = {Computational and Biological Learning Society},
  venue      = {San Diego, CA, USA},
  eventdate  = {2015-05-07/2015-05-09},
  year       = {2015},
  pages      = {1--14},
  doi        = {10.48550/arXiv.1409.1556},
  abstract   = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.}
}

@inproceedings{ResNet,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Deep Residual Learning for Image Recognition},
  eventdate = {2016-06-27/2016-06-30},
  year      = {2016},
  venue     = {Las Vegas, NV, USA},
  pages     = {770--778},
  publisher = {IEEE},
  doi       = {10.1109/CVPR.2016.90},
  issn      = {1063-6919},
  isbn      = {978-1-4673-8851-1},
  abstract  = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords  = {Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation}
}

@inproceedings{DenseNet,
  author    = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Densely Connected Convolutional Networks},
  eventdate = {2017-07-21/2017-07-26},
  date      = {2017-11-09},
  venue     = {Honolulu, HI, USA},
  publisher = {IEEE},
  pages     = {2261-2269},
  doi       = {10.1109/CVPR.2017.243},
  issn      = {1063-6919},
  isbn      = {978-1-5386-0457-1},
  abstract  = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections-one between each layer and its subsequent layer-our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
  keywords  = {Training;Convolution;Network architecture;Convolutional codes;Neural networks;Road transportation}
}

@inproceedings{GoogleNet,
  author    = {Szegedy, Christian and Wei Liu and Yangqing Jia and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Going deeper with convolutions},
  eventdate = {2015-06-07/2015-06-12},
  date      = {2015-10-15},
  venue     = {Boston, MA, USA},
  publisher = {IEEE},
  pages     = {1--9},
  doi       = {10.1109/CVPR.2015.7298594},
  issn      = {1063-6919},
  isbn      = {978-1-4673-6964-0},
  abstract  = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  keywords  = {Computer architecture;Convolutional codes;Sparse matrices;Neural networks;Visualization;Object detection;Computer vision}
}

@article{OverFeat,
  author   = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and Lecun, Yann},
  date     = {2014-02-24},
  location = {719 Broadway, 12th Floor, New York, NY 10003},
  title    = {OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks},
  journal  = {International Conference on Learning Representations (ICLR) (Banff)},
  doi      = {10.48550/arXiv.1312.6229}
}

@inproceedings{ZFNet,
  author    = {Zeiler, Matthew D. and Fergus, Rob},
  title     = {Visualizing and Understanding Convolutional Networks},
  booktitle = {Computer Vision -- ECCV 2014},
  year      = {2014},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {818--833},
  isbn      = {978-3-319-10590-1},
  url       = {https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf},
  abstract  = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.}
}

@article{MaskRCNN,
  author    = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title     = {Mask R-CNN},
  date      = {2018-06-06},
  volume    = {42},
  number    = {2},
  pages     = {386--397},
  publisher = {IEEE},
  doi       = {10.1109/TPAMI.2018.2844175},
  issn      = {1939-3539},
  abstract  = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron.},
  keywords  = {Task analysis;Semantics;Feature extraction;Object detection;Proposals;Image segmentation;Quantization (signal);Instance segmentation;object detection;pose estimation;convolutional neural network}
}

@inproceedings{YOLO,
  author    = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {You Only Look Once: Unified, Real-Time Object Detection},
  eventdate = {2016-06-27/2016-06-30},
  date      = {2016-12-12},
  venue     = {Las Vegas, NV, USA},
  pages     = {779--788},
  publisher = {IEEE},
  doi       = {10.1109/CVPR.2016.91},
  issn      = {1063-6919},
  isbn      = {978-1-4673-8851-1},
  abstract  = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
  keywords  = {Computer architecture;Microprocessors;Object detection;Training;Real-time systems;Neural networks;Pipelines}
}

@online{yolov8_ultralytics,
  author  = {Glenn Jocher and Ayush Chaurasia and Jing Qiu},
  title   = {Ultralytics YOLOv8},
  version = {8.0.0},
  year    = {2023},
  url     = {https://github.com/ultralytics/ultralytics},
  orcid   = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},
  license = {AGPL-3.0}
}

@inproceedings{YOLOv8,
  author    = {Sohan, Mupparaju and Ram, Thotakura and Ch, Venkata},
  booktitle = {Data Intelligence and Cognitive Informatics},
  date      = {2024-01},
  eventdate = {2023-06-27/2023-06-28},
  pages     = {529--545},
  title     = {A Review on YOLOv8 and Its Advancements},
  publisher = {Springer Nature Singapore},
  venue     = {SCAD College of Engineering and Technology, Tirunelveli, India},
  doi       = {10.1007/978-981-99-7962-2_39},
  abstract  = {Object detection is a crucial task in computer vision that has its application in various fields like robotics, medical imaging, surveillance systems, and autonomous vehicles. The newest version of the YOLO model, YOLOv8 is an advanced real-time object detection framework, which has attracted the attention of the research community. Of all the popular object identification methods and machine-learning models such as Faster RCNN, SSD, and RetinaNet, YOLO is the most popularly known method in terms of accuracy, speed, and efficiency. This research study provides an analysis of YOLO v8 by highlighting its innovative features, improvements, applicability in different environments, and a detailed comparison of its performance metrics to other versions and models.}
}

@article{YOLOv9,
  title    = {YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information},
  author   = {Wang, Chien-Yao and Liao, Hong-Yuan Mark},
  doi      = {10.48550/arXiv.2402.13616},
  journal  = {ArXiv},
  date     = {2024-02-29},
  abstract = {Today's deep learning methods focus on how to design the most appropriate objective functions so that the prediction results of the model can be closest to the ground truth. Meanwhile, an appropriate architecture that can facilitate acquisition of enough information for prediction has to be designed. Existing methods ignore a fact that when input data undergoes layer-by-layer feature extraction and spatial transformation, large amount of information will be lost. This paper will delve into the important issues of data loss when data is transmitted through deep networks, namely information bottleneck and reversible functions. We proposed the concept of programmable gradient information (PGI) to cope with the various changes required by deep networks to achieve multiple objectives. PGI can provide complete input information for the target task to calculate objective function, so that reliable gradient information can be obtained to update network weights. In addition, a new lightweight network architecture -- Generalized Efficient Layer Aggregation Network (GELAN), based on gradient path planning is designed. GELAN's architecture confirms that PGI has gained superior results on lightweight models. We verified the proposed GELAN and PGI on MS COCO dataset based object detection. The results show that GELAN only uses conventional convolution operators to achieve better parameter utilization than the state-of-the-art methods developed based on depth-wise convolution. PGI can be used for variety of models from lightweight to large. It can be used to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the comparison results are shown in Figure 1. The source codes are at: https://github.com/WongKinYiu/yolov9.}
}

@online{yolo11_ultralytics,
  author  = {Glenn Jocher and Jing Qiu},
  title   = {Ultralytics YOLO11},
  version = {11.0.0},
  year    = {2024},
  url     = {https://github.com/ultralytics/ultralytics},
  orcid   = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},
  license = {AGPL-3.0}
}

@misc{YOLOv11,
  title    = {YOLOv11: An Overview of the Key Architectural Enhancements},
  author   = {Khanam, Rahima and Hussain, Muhammad},
  year     = {2024},
  date     = {2024-10-23},
  doi      = {10.48550/arXiv.2410.17725},
  abstract = {This study presents an architectural analysis of YOLOv11, the latest iteration in the YOLO (You Only Look Once) series of object detection models. We examine the models architectural innovations, including the introduction of the C3k2 (Cross Stage Partial with kernel size 2) block, SPPF (Spatial Pyramid Pooling - Fast), and C2PSA (Convolutional block with Parallel Spatial Attention) components, which contribute in improving the models performance in several ways such as enhanced feature extraction. The paper explores YOLOv11's expanded capabilities across various computer vision tasks, including object detection, instance segmentation, pose estimation, and oriented object detection (OBB). We review the model's performance improvements in terms of mean Average Precision (mAP) and computational efficiency compared to its predecessors, with a focus on the trade-off between parameter count and accuracy. Additionally, the study discusses YOLOv11's versatility across different model sizes, from nano to extra-large, catering to diverse application needs from edge devices to high-performance computing environments. Our research provides insights into YOLOv11's position within the broader landscape of object detection and its potential impact on real-time computer vision applications.}
}

@inproceedings{RCNN,
  author    = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  eventdate = {2014-06-23/2014-06-28},
  date      = {2014-09-25},
  venue     = {Columbus, OH, USA},
  pages     = {580--587},
  publisher = {IEEE},
  doi       = {10.1109/CVPR.2014.81},
  issn      = {1063-6919},
  isbn      = {978-1-4799-5118-5},
  abstract  = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.},
  keywords  = {Proposals;Feature extraction;Training;Visualization;Object detection;Vectors;Support vector machines}
}

@inproceedings{FastRCNN,
  author    = {Girshick, Ross},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Fast R-CNN},
  eventdate = {2015-12-07/2015-12-13},
  date      = {2016-02-18},
  venue     = {Santiago, Chile},
  pages     = {1440--1448},
  publisher = {IEEE},
  doi       = {10.1109/ICCV.2015.169},
  issn      = {2380-7504},
  isbn      = {978-1-4673-8391-2},
  abstract  = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  keywords  = {Training;Proposals;Feature extraction;Object detection;Pipelines;Computer architecture;Open source software}
}

@article{FasterRCNN,
  author   = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  date     = {2017-06-01},
  volume   = {39},
  number   = {6},
  pages    = {1137--1149},
  doi      = {10.1109/TPAMI.2016.2577031},
  issn     = {1939-3539},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network(RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  keywords = {Proposals;Object detection;Convolutional codes;Feature extraction;Search problems;Detectors;Training;Object detection;region proposal;convolutional neural network}
}

@inproceedings{PANet,
  author    = {Liu, Shu and Qi, Lu and Qin, Haifang and Shi, Jianping and Jia, Jiaya},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {Path Aggregation Network for Instance Segmentation},
  eventdate = {2018-06-18/2018-06-23},
  date      = {2018-12-16},
  pages     = {8759--8768},
  venue     = {Salt Lake City, UT, USA},
  publisher = {IEEE},
  doi       = {10.1109/CVPR.2018.00913},
  issn      = {2575-7075},
  isbn      = {978-1-5386-6420-9},
  abstract  = {The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction. These improvements are simple to implement, with subtle extra computational overhead. Yet they are useful and make our PANet reach the 1st place in the COCO 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. PANet is also state-of-the-art on MVD and Cityscapes.},
  keywords  = {Proposals;Feature extraction;Task analysis;Image segmentation;Object detection;Training;Semantics}
}

@inproceedings{MaskScoringRCNN,
  author    = {Huang, Zhaojin and Huang, Lichao and Gong, Yongchao and Huang, Chang and Wang, Xinggang},
  booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Mask Scoring R-CNN},
  eventdate = {2019-06-15/2019-06-20},
  date      = {2020-01-09},
  pages     = {6402--6411},
  venue     = {Long Beach, CA, USA},
  publisher = {IEEE},
  doi       = {10.1109/CVPR.2019.00657},
  issn      = {2575-7075},
  isbn      = {978-1-7281-3293-8},
  abstract  = {Letting a deep network be aware of the quality of its own predictions is an interesting yet important problem. In the task of instance segmentation, the confidence of instance classification is used as mask quality score in most instance segmentation frameworks. However, the mask quality, quantified as the IoU between the instance mask and its ground truth, is usually not well correlated with classification score. In this paper, we study this problem and propose Mask Scoring R-CNN which contains a network block to learn the quality of the predicted instance masks. The proposed network block takes the instance feature and the corresponding predicted mask together to regress the mask IoU. The mask scoring strategy calibrates the misalignment between mask quality and mask score, and improves instance segmentation performance by prioritizing more accurate mask predictions during COCO AP evaluation. By extensive evaluations on the COCO dataset, Mask Scoring R-CNN brings consistent and noticeable gain with different models and outperforms the state-of-the-art Mask R-CNN. We hope our simple and effective approach will provide a new direction for improving instance segmentation. The source code of our method is available at \url{https://github.com/zjhuang22/maskscoring_rcnn}.},
  keywords  = {Recognition: Detection;Categorization;Retrieval}
}

@inproceedings{YOLACT,
  author    = {Bolya, Daniel and Zhou, Chong and Xiao, Fanyi and Lee, Yong Jae},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {YOLACT: Real-Time Instance Segmentation},
  eventdate = {2019-10-27/2019-11-02},
  date      = {2020-02-17},
  pages     = {9156--9165},
  venue     = {Seoul, Korea},
  publisher = {IEEE},
  doi       = {10.1109/ICCV.2019.00925},
  issn      = {2380-7504},
  isbn      = {978-1-7281-4803-8},
  abstract  = {We present a simple, fully-convolutional model for real-time instance segmentation that achieves 29.8 mAP on MS COCO at 33.5 fps evaluated on a single Titan Xp, which is significantly faster than any previous competitive approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn't depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. Finally, we also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty.},
  keywords  = {Prototypes;Real-time systems;Image segmentation;Object detection;Detectors;Computational modeling;Task analysis}
}

@article{DeepWatershed,
  author   = {Bai, Min and Urtasun, Raquel},
  date     = {2016-11-24},
  title    = {Deep Watershed Transform for Instance Segmentation},
  doi      = {10.48550/arXiv.1611.08303},
  journal  = {CoRR},
  abstract = {Most contemporary approaches to instance segmentation use complex pipelines involving conditional random fields, recurrent neural networks, object proposals, or template matching schemes. In our paper, we present a simple yet powerful end-to-end convolutional neural network to tackle this task. Our approach combines intuitions from the classical watershed transform and modern deep learning to produce an energy map of the image where object instances are unambiguously represented as basins in the energy map. We then perform a cut at a single energy level to directly yield connected components corresponding to object instances. Our model more than doubles the performance of the state-of-the-art on the challenging Cityscapes Instance Level Segmentation task.}
}

@inproceedings{DeepMask,
  author    = {Pinheiro, Pedro O. and Collobert, Ronan and Doll\'{a}r, Piotr},
  title     = {Learning to segment object candidates},
  eventdate = {2015-06-01},
  venue     = {Montreal, Canada},
  date      = {2015},
  publisher = {MIT Press},
  location  = {Cambridge, MA, USA},
  booktitle = {Proceedings of the 29th International Conference on Neural Information Processing Systems - Volume 2},
  pages     = {1990-–1998},
  numpages  = {9},
  series    = {NIPS'15},
  doi       = {10.48550/arXiv.1506.06204},
  abstract  = {Recent object detection systems rely on two critical steps: (1) a set of object proposals is predicted as efficiently as possible, and (2) this set of candidate proposals is then passed to an object classifier. Such approaches have been shown they can be fast, while achieving the state of the art in detection performance. In this paper, we propose a new way to generate object proposals, introducing an approach based on a discriminative convolutional network. Our model is trained jointly with two objectives: given an image patch, the first part of the system outputs a class-agnostic segmentation mask, while the second part of the system outputs the likelihood of the patch being centered on a full object. At test time, the model is efficiently applied on the whole test image and generates a set of segmentation masks, each of them being assigned with a corresponding object likelihood score. We show that our model yields significant improvements over state-of-the-art object proposal algorithms. In particular, compared to previous approaches, our model obtains substantially higher object recall using fewer proposals. We also show that our model is able to generalize to unseen categories it has not seen during training. Unlike all previous approaches for generating object masks, we do not rely on edges, superpixels, or any other form of low-level segmentation.}
}

@inproceedings{InstanceFCN,
  author    = {Dai, Jifeng and He, Kaiming and Li, Yi and Ren, Shaoqing and Sun, Jian},
  title     = {Instance-Sensitive Fully Convolutional Networks},
  booktitle = {Computer Vision -- ECCV 2016},
  pages     = {534--549},
  volume    = {9910},
  eventdate = {2016-10-11/2016-10-14},
  venue     = {Amsterdam, The Netherlands},
  date      = {2016-09-17},
  publisher = {Springer International Publishing},
  location  = {Cham},
  isbn      = {978-3-319-46466-4},
  doi       = {10.1007/978-3-319-46466-4_32},
  abstract  = {Fully convolutional networks (FCNs) have been proven very successful for semantic segmentation, but the FCN outputs are unaware of object instances. In this paper, we develop FCNs that are capable of proposing instance-level segment candidates. In contrast to the previous FCN that generates one score map, our FCN is designed to compute a small set of instance-sensitive score maps, each of which is the outcome of a pixel-wise classifier of a relative position to instances. On top of these instance-sensitive score maps, a simple assembling module is able to output instance candidate at each position. In contrast to the recent DeepMask method for segmenting instances, our method does not have any high-dimensional layer related to the mask resolution, but instead exploits image local coherence for estimating instances. We present competitive results of instance segment proposal on both PASCAL VOC and MS COCO.}
}

@inproceedings{TensorMask,
  author    = {Chen, Xinlei and Girshick, Ross and He, Kaiming and Dollar, Piotr},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {TensorMask: A Foundation for Dense Object Segmentation},
  eventdate = {2019-10-27/2019-11-02},
  date      = {2020-02-27},
  pages     = {2061--2069},
  venue     = {Seoul, Korea},
  publisehr = {IEEE},
  doi       = {10.1109/ICCV.2019.00215},
  issn      = {2380-7504},
  isbn      = {978-1-7281-4803-8},
  abstract  = {Sliding-window object detectors that generate bounding-box object predictions over a dense, regular grid have advanced rapidly and proven popular. In contrast, modern instance segmentation approaches are dominated by methods that first detect object bounding boxes, and then crop and segment these regions, as popularized by Mask R-CNN. In this work, we investigate the paradigm of dense sliding-window instance segmentation, which is surprisingly under-explored. Our core observation is that this task is fundamentally different than other dense prediction tasks such as semantic segmentation or bounding-box object detection, as the output at every spatial location is itself a geometric structure with its own spatial dimensions. To formalize this, we treat dense instance segmentation as a prediction task over 4D tensors and present a general framework called TensorMask that explicitly captures this geometry and enables novel operators on 4D tensors. We demonstrate that the tensor view leads to large gains over baselines that ignore this structure, and leads to results comparable to Mask R-CNN. These promising results suggest that TensorMask can serve as a foundation for novel advances in dense mask prediction and a more complete understanding of the task. Code will be made available.},
  keywords  = {Tensile stress;Task analysis;Windows;Shape;Proposals;Two dimensional displays;Image segmentation}
}

@inproceedings{SOLOv1,
  author    = {Wang, Xinlong and Kong, Tao and Shen, Chunhua and Jiang, Yuning and Li, Lei},
  editor    = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  title     = {SOLO: Segmenting Objects by Locations},
  booktitle = {Computer Vision -- ECCV 2020},
  eventdate = {2020-08-23/2020-08-28},
  date      = {2020-12-04},
  venue     = {Glasgow, UK},
  publisher = {Springer International Publishing},
  location  = {Cham},
  pages     = {649--665},
  doi       = {10.1007/978-3-030-58523-5_38},
  abstract  = {We present a new, embarrassingly simple approach to instance segmentation. Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that have made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the ``detect-then-segment'' strategy (e.g., Mask R-CNN), or predict embedding vectors first then use clustering techniques to group pixels into individual instances. We view the task of instance segmentation from a completely new perspective by introducing the notion of ``instance categories'', which assigns categories to each pixel within an instance according to the instance's location and size, thus nicely converting instance segmentation into a single-shot classification-solvable problem. We demonstrate a much simpler and flexible instance segmentation framework with strong performance, achieving on par accuracy with Mask R-CNN and outperforming recent single-shot instance segmenters in accuracy. We hope that this simple and strong framework can serve as a baseline for many instance-level recognition tasks besides instance segmentation. Code is available at https://git.io/AdelaiDet.}
}

@inproceedings{SOLOv2,
  author    = {Wang, Xinlong and Zhang, Rufeng and Kong, Tao and Li, Lei and Shen, Chunhua},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  volume    = {33},
  eventdate = {2020-12-06/2020-12-12},
  date      = {2020},
  pages     = {17721--17732},
  publisher = {Curran Associates, Inc.},
  title     = {SOLOv2: Dynamic and Fast Instance Segmentation},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/cd3afef9b8b89558cd56638c3631868a-Paper.pdf},
  abstract  = {In this work, we design a simple, direct, and fast framework for instance segmentation with strong performance. To this end, we propose a novel and effective approach, termed SOLOv2, following the principle of the SOLO method [32]. First, our new framework is empowered by an efficient and holistic instance mask representation scheme, which dynamically segments each instance in the image, without resorting to bounding box detection. Specifically, the object mask generation is decoupled into a mask kernel prediction and mask feature learning, which are responsible for generating convolution kernels and the feature maps to be convolved with, respectively. Second, SOLOv2 significantly reduces inference overhead with our novel matrix non-maximum suppression (NMS) technique. Our Matrix NMS performs NMS with parallel matrix operations in one shot, and yields better results. We demonstrate that the proposed SOLOv2 achieves the state-of-the- art performance with high efficiency, making it suitable for both mobile and cloud applications. A light-weight version of SOLOv2 executes at 31.3 FPS and yields 37.1\% AP on COCO test-dev. Moreover, our state-of-the-art results in object detection (from our mask byproduct) and panoptic segmentation show the potential of SOLOv2 to serve as a new strong baseline for many instance-level recognition tasks. Code is available at https://git.io/AdelaiDet}
}

@article{FastSAM,
  author      = {Zhao, Xu and Ding, Wenchao and An, Yongqi and Du, Yinglong and Yu, Tao and Li, Min and Tang, Ming and Wang, Jinqiao},
  title       = {Fast Segment Anything},
  date        = {2023-06-21},
  doi         = {10.48550/arXiv.2306.12156},
  institution = {Institute of Automation, Chinese Academy of Sciences},
  location    = {Beijing, China},
  journal     = {ArXiv},
  abstract    = {The recently proposed segment anything model (SAM) has made a significant influence in many computer vision tasks. It is becoming a foundation step for many high-level tasks, like image segmentation, image caption, and image editing. However, its huge computation costs prevent it from wider applications in industry scenarios. The computation mainly comes from the Transformer architecture at high-resolution inputs. In this paper, we propose a speed-up alternative method for this fundamental task with comparable performance. By reformulating the task as segments-generation and prompting, we find that a regular CNN detector with an instance segmentation branch can also accomplish this task well. Specifically, we convert this task to the well-studied instance segmentation task and directly train the existing instance segmentation method using only 1/50 of the SA-1B dataset published by SAM authors. With our method, we achieve a comparable performance with the SAM method at 50 times higher run-time speed. We give sufficient experimental results to demonstrate its effectiveness. The codes and demos will be released at https://github.com/CASIA-IVA-Lab/FastSAM.}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% HIPERPARÁMETROS DE ENTRENAMIENTO %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@book{Goodfellow-et-al-2016,
  title     = {Deep Learning},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher = {MIT Press},
  url       = {http://www.deeplearningbook.org},
  date      = {2016}
}

%---------------%
% Optimizadores %
%---------------%

@article{Keskar2017,
  author  = {Keskar, Nitish Shirish and Socher, Richard},
  title   = {Improving Generalization Performance by Switching from Adam to SGD},
  date    = {2017-12},
  journal = {ArXiv},
  doi     = {10.48550/arXiv.1712.07628}
}

@inproceedings{Deng2009,
  author    = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {ImageNet: A large-scale hierarchical image database},
  eventdate = {2009-06-20/2009-06-25},
  date      = {2009-08-18},
  pages     = {248--255},
  venue     = {Miami, FL, USA},
  publisehr = {IEEE},
  doi       = {10.1109/CVPR.2009.5206848},
  issn      = {1063-6919},
  isbn      = {978-1-4244-3992-8},
  abstract  = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  keywords  = {Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine}
}

@article{Goyal2017,
  title   = {Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},
  author  = {Priya Goyal and Piotr Doll{\'a}r and Ross B. Girshick and Pieter Noordhuis and Lukasz Wesolowski and Aapo Kyrola and Andrew Tulloch and Yangqing Jia and Kaiming He},
  journal = {ArXiv},
  date    = {2017},
  doi     = {10.48550/arXiv.1706.02677}
}

@inproceedings{He2016,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Deep Residual Learning for Image Recognition},
  date      = {2016-12-12},
  eventdate = {2016-06-27/2016-06-30},
  publisher = {IEEE},
  venue     = {Las Vegas, NV, USA},
  pages     = {770--778},
  doi       = {10.1109/CVPR.2016.90},
  issn      = {1063-6919},
  isbn      = {978-1-4673-8851-1},
  abstract  = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords  = {Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation}
}

%-------------------------%
% Learning Rate Scheduler %
%-------------------------%

@article{Bengio2012,
  author  = {Bengio, Yoshua},
  date    = {2012-06-24},
  doi     = {10.48550/arXiv.1206.5533},
  title   = {Practical recommendations for gradient-based training of deep architectures},
  journal = {Arxiv}
}

%------------%
% Batch Size %
%------------%

@article{Shallue2018,
  title   = {Measuring the Effects of Data Parallelism on Neural Network Training},
  author  = {Chris Shallue and Jaehoon Lee and Joseph Antognini and Jascha Sohl-dickstein and Roy Frostig and George Dahl},
  date    = {2019-07-19},
  doi     = {10.48550/arXiv.1811.03600},
  journal = {Journal of Machine Learning Research (JMLR)}
}

@article{Cheng2018,
  author    = {Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
  journal   = {IEEE Signal Processing Magazine},
  title     = {Model Compression and Acceleration for Deep Neural Networks: The Principles, Progress, and Challenges},
  date      = {2018-0-10},
  volume    = {35},
  number    = {1},
  pages     = {126-136},
  doi       = {10.1109/MSP.2017.2765695},
  issn      = {1558-0792},
  publisher = {IEEE},
  abstract  = {In recent years, deep neural networks (DNNs) have received increased attention, have been applied to different applications, and achieved dramatic accuracy improvements in many tasks. These works rely on deep networks with millions or even billions of parameters, and the availability of graphics processing units (GPUs) with very high computation capability plays a key role in their success. For example, Krizhevsky et al. [1] achieved breakthrough results in the 2012 ImageNet Challenge using a network containing 60 million parameters with five convolutional layers and three fully connected layers. Usually, it takes two to three days to train the whole model on the ImagetNet data set with an NVIDIA K40 machine. In another example, the top face-verification results from the Labeled Faces in the Wild (LFW) data set were obtained with networks containing hundreds of millions of parameters, using a mix of convolutional, locally connected, and fully connected layers [2], [3]. It is also very time-consuming to train such a model to obtain a reasonable performance. In architectures that only rely on fully connected layers, the number of parameters can grow to billions [4].},
  keywords  = {Convolution;Training data;Neural networks;Quantization (signal);Convolutional codes;Computational modeling;Machine learning}
}


@article{Breuel2015,
  author  = {Breuel, Thomas},
  year    = {2015},
  month   = {08},
  journal = {ArXiv},
  pages   = {},
  title   = {The Effects of Hyperparameters on SGD Training of Neural Networks}
}

%-------------------------%
% Learning Rate Scheduler %
%-------------------------%

@inproceedings{Jacob2018,
  author    = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
  eventdate = {2018-06-18/2018-05-23},
  date      = {2018-12-16},
  publisher = {IEEE},
  venue     = {Salt Lake City, UT, USA},
  pages     = {2704--2713},
  doi       = {10.1109/CVPR.2018.00286},
  issn      = {2575--7075},
  abstract  = {The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based models call for efficient and accurate on-device inference schemes. We propose a quantization scheme that allows inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point inference on commonly available integer-only hardware. We also co-design a training procedure to preserve end-to-end model accuracy post quantization. As a result, the proposed quantization scheme improves the tradeoff between accuracy and on-device latency. The improvements are significant even on MobileNets, a model family known for run-time efficiency, and are demonstrated in ImageNet classification and COCO detection on popular CPUs.},
  keywords  = {Quantization (signal);Training;Arrays;Computational modeling;Hardware;Neural networks}
}

@article{Yao2024,
  author  = {Yao, Zhewei and Wu, Xiaoxia and Li, Cheng and Youn, Stephen and He, Yuxiong},
  year    = {2024},
  month   = {03},
  pages   = {19377-19385},
  title   = {Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation},
  volume  = {38},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi     = {10.1609/aaai.v38i17.29908}
}

%----------%
% Tracking %
%----------%

@article{Smeulders2014,
  author    = {Smeulders, Arnold W. M. and Chu, Dung M. and Cucchiara, Rita and Calderara, Simone and Dehghan, Afshin and Shah, Mubarak},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title     = {Visual Tracking: An Experimental Survey},
  date      = {2014-07},
  volume    = {36},
  number    = {7},
  pages     = {1442-1468},
  doi       = {10.1109/TPAMI.2013.230},
  issn      = {1939-3539},
  publisher = {IEEE},
  abstract  = {There is a large variety of trackers, which have been proposed in the literature during the last two decades with some mixed success. Object tracking in realistic scenarios is a difficult problem, therefore, it remains a most active area of research in computer vision. A good tracker should perform well in a large number of videos involving illumination changes, occlusion, clutter, camera motion, low contrast, specularities, and at least six more aspects. However, the performance of proposed trackers have been evaluated typically on less than ten videos, or on the special purpose datasets. In this paper, we aim to evaluate trackers systematically and experimentally on 315 video fragments covering above aspects. We selected a set of nineteen trackers to include a wide variety of algorithms often cited in literature, supplemented with trackers appearing in 2010 and 2011 for which the code was publicly available. We demonstrate that trackers can be evaluated objectively by survival curves, Kaplan Meier statistics, and Grubs testing. We find that in the evaluation practice the F-score is as effective as the object tracking accuracy (OTA) score. The analysis under a large variety of circumstances provides objective insight into the strengths and weaknesses of trackers.},
  keywords  = {Target tracking;Videos;Radar tracking;Educational institutions;Robustness;Object tracking;Object tracking;Tracking evaluation;Tracking dataset;Camera surveillance;Video understanding;Computer vision;Image processing;Object tracking;tracking evaluation;tracking dataset;camera surveillance;video understanding;computer vision;image processing}
}

@article{Yilmaz2006,
  author  = {Yilmaz, Alper and Javed, Omar and Shah, Mubarak},
  year    = {2006},
  month   = {12},
  title   = {Object tracking: a survey. ACM Comput Surv},
  volume  = {38},
  journal = {ACM Comput. Surv.},
  doi     = {10.1145/1177352.1177355}
}

@inproceedings{BotSORT,
  author    = {Zhao, Junbo and Chen, Jinxiang},
  title     = {YOLOv8 Detection and Improved BOT-SORT Tracking Algorithm for Iron Ladles},
  booktitle = {Proceedings of the 2024 7th International Conference on Image and Graphics Processing},
  pages     = {409--415},
  numpages  = {7},
  eventdate = {2024-01-19/2024-01-21},
  date      = {2024-05-03},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  venue     = {Beijing, China},
  series    = {ICIGP '24},
  isbn      = {9798400716720},
  doi       = {10.1145/3647649.3647713},
  abstract  = {Iron ladles play a significant role in the industrial intelligence upgrade of steel plants. Accurate recognition and tracking for moving iron ladles can provide the location, speed, and operations information of iron ladles, which are essential for making scheduling plans for steel production. YOLOv8 detection and state-of-the-art (SOTA) tracking algorithms for iron ladles are presented in this paper. The Video data sets with or without shelters are constructed by collecting the actual iron ladles working data. Some own image and video datasets are added to the above datasets by using Segment Anything (SAM) and DarkLabel due to lack of iron ladles data. The YOLOv8 detection model is applied to detect the iron ladles, and three trackers, which are the StrongSORT, OC-SORT, and BOT-SORT, are applied to achieve real-time position information of iron ladles, respectively. In order to improve the identification and tracking accuracy for iron ladles, a genetic algorithm is used to optimize the parameter of the above three trackers. The training and testing results of the above model show that the BOT-SORT tracking model with genetic optimization achieves the highest accuracy that HOTA score is 97.49, both MOTA and IDF1 are 100.},
  keywords  = {BOT-SORT, SAM, YOLOv8, iron ladle tracking, object detection}
}

@inproceedings{OCSORT,
  author    = {Cao, Jinkun and Pang, Jiangmiao and Weng, Xinshuo and Khirodkar, Rawal and Kitani, Kris},
  booktitle = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking},
  eventdate = {2023-06-17/2023-06-24},
  date      = {2023-08-22},
  publisher = {IEEE},
  doi       = {10.1109/CVPR52729.2023.00934},
  issn      = {2575-7075},
  isbn      = {979-8-3503-0129-8},
  venue     = {Vancouver, BC, Canada},
  volume    = {},
  number    = {},
  pages     = {9686-9696},
  abstract  = {Kalman filter (KF) based methods for multi-object tracking (MOT) make an assumption that objects move linearly. While this assumption is acceptable for very short periods of occlusion, linear estimates of motion for prolonged time can be highly inaccurate. Moreover, when there is no measurement available to update Kalman filter parameters, the standard convention is to trust the priori state estimations for posteriori update. This leads to the accumulation of errors during a period of occlusion. The error causes significant motion direction variance in practice. In this work, we show that a basic Kalman filter can still obtain state-of-the-art tracking performance if proper care is taken to fix the noise accumulated during occlusion. Instead of relying only on the linear state estimate (i.e., estimation-centric approach), we use object observations (i.e., the measurements by object detector) to compute a virtual trajectory over the occlusion period to fix the error accumulation of filter parameters. This allows more time steps to correct errors accumulated during occlusion. We name our method Observation-Centric SORT (OC-SORT). It remains Simple, Online, and Real-Time but improves robustness during occlusion and non-linear motion. Given off-the-shelf detections as input, OC-SORT runs at 700+ FPS on a single CPU. It achieves state-of-the-art on multiple datasets, including MOT17, MOT20, KITTI, head tracking, and especially DanceTrack where the object motion is highly non-linear. The code and models are available at \url{https://github.com/noahcao/OC_SORT}.},
  keywords  = {Target tracking;Detectors;Nonlinear filters;Real-time systems;Robustness;Trajectory;Pattern recognition;Vision applications and systems}
}


@inproceedings{SORT,
  author    = {Bewley, Alex and Ge, Zongyuan and Ott, Lionel and Ramos, Fabio and Upcroft, Ben},
  booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
  title     = {Simple online and realtime tracking},
  eventdate = {2016-09-25/2016-09-28},
  date      = {2016-08-19},
  volume    = {},
  number    = {},
  pages     = {3464-3468},
  venue     = {Phoenix, AZ, USA},
  publisher = {IEEE},
  doi       = {10.1109/ICIP.2016.7533003},
  issn      = {2381-8549},
  isbn      = {978-1-4673-9961-6},
  abstract  = {This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications. To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9\%. Despite only using a rudimentary combination of familiar techniques such as the Kalman Filter and Hungarian algorithm for the tracking components, this approach achieves an accuracy comparable to state-of-the-art online trackers. Furthermore, due to the simplicity of our tracking method, the tracker updates at a rate of 260 Hz which is over 20x faster than other state-of-the-art trackers.},
  keywords  = {Target tracking;Detectors;Benchmark testing;Kalman filters;Visualization;Complexity theory;Computer Vision;Multiple Object Tracking;Detection;Data Association}
}

@inproceedings{DeepOCSORT,
  author    = {Maggiolino, Gerard and Ahmad, Adnan and Cao, Jinkun and Kitani, Kris},
  booktitle = {2023 IEEE International Conference on Image Processing (ICIP)},
  title     = {Deep OC-Sort: Multi-Pedestrian Tracking by Adaptive Re-Identification},
  eventdate = {2023-10-08/2023-10-11},
  date      = {2023-09-11},
  volume    = {},
  number    = {},
  pages     = {3025-3029},
  venue     = {Kuala Lumpur, Malaysia},
  publisher = {IEEE},
  doi       = {10.1109/ICIP49359.2023.10222576},
  isbn      = {978-1-7281-9835-4},
  abstract  = {Motion-based association for Multi-Object Tracking (MOT) has recently re-achieved prominence with the rise of powerful object detectors. Despite this, little work has been done to incorporate appearance cues beyond simple heuristic models that lack robustness to feature degradation. In this paper, we propose a novel way to leverage objects’ appearances to adaptively integrate appearance matching into existing high-performance motion-based methods. Building upon the pure motion-based method OC-SORT, we achieve 1st place on MOT20 and 2nd place on MOT17 with 63.9 and 64.9 HOTA, respectively. We also achieve 61.3 HOTA on the challenging DanceTrack benchmark as a new state-of-the-art even compared to more heavily-designed methods. The code and models are available at https://github.com/GerardMaggiolino/Deep-OC-SORT.},
  keywords  = {Degradation;Visualization;Adaptation models;Tracking;Image processing;Image matching;Detectors;multi-object tracking;Kalman filter}
}

@inproceedings{HybridSORT,
  title     = {Hybrid-SORT: Weak Cues Matter for Online Multi-Object Tracking},
  author    = {Yang, Mingzhan and Han, Guangxin and Yan, Bin and Zhang, Wenhua and Qi, Jinqing and Lu, Huchuan and Wang, Dong},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence},
  volume    = {38},
  number    = {7},
  pages     = {6504--6512},
  eventdate = {2024-02-20/2024-02-27},
  date      = {2024-03-25},
  doi       = {10.1609/aaai.v38i7.28471},
  abstract  = {Multi-Object Tracking (MOT) aims to detect and associate all desired objects across frames. Most methods accomplish the task by explicitly or implicitly leveraging strong cues (i.e., spatial and appearance information), which exhibit powerful instance-level discrimination. However, when object occlusion and clustering occur, spatial and appearance information will become ambiguous simultaneously due to the high overlap among objects. In this paper, we demonstrate this long-standing challenge in MOT can be efficiently and effectively resolved by incorporating weak cues to compensate for strong cues. Along with velocity direction, we introduce the confidence and height state as potential weak cues. With superior performance, our method still maintains Simple, Online and Real-Time (SORT) characteristics. Also, our method shows strong generalization for diverse trackers and scenarios in a plug-and-play and training-free manner. Significant and consistent improvements are observed when applying our method to 5 different representative trackers. Further, with both strong and weak cues, our method Hybrid-SORT achieves superior performance on diverse benchmarks, including MOT17, MOT20, and especially DanceTrack where interaction and severe occlusion frequently happen with complex motions. The code and models are available at https://github.com/ymzis69/HybridSORT.}
}

@inproceedings{ByteTrack,
  author    = {Zhang, Yifu and Sun, Peize and Jiang, Yi and Yu, Dongdong and Weng, Fucheng and Yuan, Zehuan and Luo, Ping and Liu, Wenyu and Wang, Xinggang},
  title     = {ByteTrack: Multi-object Tracking by Associating Every Detection Box},
  booktitle = {Computer Vision -- ECCV 2022},
  eventdate = {2022-10-23/2022-10-27},
  date      = {2022-10-23},
  venue     = {Tel Aviv, Israel},
  address   = {Cham, Switzerland},
  publisher = {Springer Nature},
  pages     = {1--21},
  doi       = {10.1007/978-3-031-20047-2_1},
  abstract  = {Multi-object tracking (MOT) aims at estimating bounding boxes and identities of objects in videos. Most methods obtain identities by associating detection boxes whose scores are higher than a threshold. The objects with low detection scores, e.g. occluded objects, are simply thrown away, which brings non-negligible true object missing and fragmented trajectories. To solve this problem, we present a simple, effective and generic association method, tracking by associating almost every detection box instead of only the high score ones. For the low score detection boxes, we utilize their similarities with tracklets to recover true objects and filter out the background detections. When applied to 9 different state-of-the-art trackers, our method achieves consistent improvement on IDF1 score ranging from 1 to 10 points. To put forwards the state-of-the-art performance of MOT, we design a simple and strong tracker, named ByteTrack. For the first time, we achieve 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA on the test set of MOT17 with 30 FPS running speed on a single V100 GPU. ByteTrack also achieves state-of-the-art performance on MOT20, HiEve and BDD100K tracking benchmarks. The source code, pre-trained models with deploy versions and tutorials of applying to other trackers are released at https://github.com/ifzhang/ByteTrack.}
}


%--------------------------------------------%
% Herramientas de despliegue y entrenamiento %
%--------------------------------------------%

@online{TensorFlow,
  title  = {TensorFlow},
  author = {{TensorFlow Developers}},
  type   = {Software},
  doi    = {10.5281/zenodo.4724125}
}

@report{TensorFlow-whitepaper,
  title     = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  author    = {Abadi, Mart\'{i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and S.~Corrado, Greg and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man\'{e}, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi\'{e}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang and Google Research},
  publisher = {Google},
  date      = {2015-11-09},
  type      = {Whitepaper},
  url       = {https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45166.pdf},
  abstract  = {TensorFlow is an interface for expressing machine learning algorithms and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.}
}

@online{Pytorch,
  author  = {Bergmann, Dave and Stryker, Cole},
  title   = {What is PyTorch?},
  url     = {https://www.ibm.com/think/topics/pytorch},
  urldate = {2025-07-01}
}

@inproceedings{Pytorch2,
  author    = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, CK and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Suo, Michael and Tillet, Phil and Wang, Eikan and Wang, Xiaodong and Wen, William and Zhang, Shunting and Zhao, Xu and Zhou, Keren and Zou, Richard and Mathews, Ajit and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
  title     = {PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation},
  booktitle = {29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS ’24)},
  pages     = {929--947},
  numpages  = {19},
  eventdate = {2024-04-27/2024-05-01},
  date      = {2024-04},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  venue     = {La Jolla, CA, USA},
  doi       = {10.1145/3620665.3640366},
  isbn      = {9798400703850},
  abstract  = {This paper introduces two extensions to the popular PyTorch machine learning framework, TorchDynamo and TorchInductor, which implement the torch.compile feature released in PyTorch 2. TorchDynamo is a Python-level just-in-time (JIT) compiler that enables graph compilation in PyTorch programs without sacrificing the flexibility of Python. It achieves this by dynamically modifying Python bytecode before execution and extracting sequences of PyTorch operations into an FX graph, which is then JIT compiled using one of many extensible backends. TorchInductor is the default compiler backend for TorchDynamo, which translates PyTorch programs into OpenAI's Triton for GPUs and C++ for CPUs. Results show that TorchDynamo is able to capture graphs more robustly than prior approaches while adding minimal overhead, and TorchInductor is able to provide a 2.27\texttimes{} inference and 1.41\texttimes{} training geometric mean speedup on an NVIDIA A100 GPU across 180+ real-world models, which outperforms six other compilers. These extensions provide a new way to apply optimizations through compilers in eager mode frameworks like PyTorch.}
}


@online{Supergradients,
  doi       = {10.5281/ZENODO.7789328},
  author    = {Aharon,  Shay and {Louis-Dupont} and {Ofri Masad} and Yurkova,  Kate and {Lotem Fridman} and {Lkdci} and Khvedchenya,  Eugene and Rubin,  Ran and Bagrov,  Natan and Tymchenko,  Borys and Keren,  Tomer and Zhilko,  Alexander and {Eran-Deci}},
  title     = {Super-Gradients},
  publisher = {GitHub},
  journal   = {GitHub repository},
  date      = {2021},
  type      = {Software}
}

@online{Ultralytics,
  author  = {Jocher, Glenn and Qiu, Jing and Chaurasia, Ayush},
  license = {AGPL-3.0},
  title   = {{Ultralytics YOLO}},
  url     = {https://github.com/ultralytics/ultralytics},
  version = {8.0.0},
  date    = {2023-01}
}


%----------------------------%
% Herramientas de etiquetado %
%----------------------------%

@online{Roboflow,
  author = {Dwyer, B. and Nelson, J. and Hansen, T. and et al.},
  date   = {2025},
  type   = {Software},
  title  = {Roboflow (Version 1.0)},
  url    = {https://roboflow.com},
  note   = {Computer vision}
}

@online{CVAT,
  author    = {CVAT.ai Corporation},
  title     = {Computer Vision Annotation Tool (CVAT)},
  license   = {MIT},
  date      = {2024-06},
  publisher = {Zenodo},
  version   = {v2.16.1},
  doi       = {10.5281/zenodo.12771595}
}

@online{Labellerr,
  title = {Labellerr},
  type  = {Computer Vision Tools},
  url   = {https://www.labellerr.com},
}

@online{V7Labs,
  title = {{V7 Labs - Darwin}},
  type  = {Computer Vision Tools},
  url   = {https://www.v7labs.com/darwin},
}

@online{Supervisely,
  title     = {Supervisely Computer Vision platform},
  type      = {Computer Vision Tools},
  author    = {Supervisely},
  url       = {https://supervisely.com},
  journal   = {Supervisely Ecosystem},
  publisher = {Supervisely},
  date      = {2023-07}
}

@article{SAM2,
  title    = {SAM 2: Segment Anything in Images and Videos},
  author   = {Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Doll{\'a}r, Piotr and Feichtenhofer, Christoph},
  journal  = {ArXiv},
  doi      = {10.48550/arXiv.2408.00714},
  date     = {2024},
  abstract = {We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3\texttimes{} fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6\texttimes{} faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.}
}