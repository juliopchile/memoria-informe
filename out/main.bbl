% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{none/global//global/global}
    \entry{wildsense-1}{online}{}
      \name{author}{1}{}{%
        {{hash=159128084613cd3714b9bcd52c6c532d}{%
           family={{WildSense}},
           familyi={W\bibinitperiod}}}%
      }
      \strng{namehash}{159128084613cd3714b9bcd52c6c532d}
      \strng{fullhash}{159128084613cd3714b9bcd52c6c532d}
      \strng{bibnamehash}{159128084613cd3714b9bcd52c6c532d}
      \strng{authorbibnamehash}{159128084613cd3714b9bcd52c6c532d}
      \strng{authornamehash}{159128084613cd3714b9bcd52c6c532d}
      \strng{authorfullhash}{159128084613cd3714b9bcd52c6c532d}
      \field{extraname}{1}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Robótica Submarina Automatizada para Salmonicultura de Alta Mar (R-SASA)}
      \field{urlday}{25}
      \field{urlmonth}{5}
      \field{urlyear}{2025}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://wildsense.ai/servicios/robotica-submarina-automatizada-para-salmonicultura-de-alta-mar-r-sasa
      \endverb
      \verb{url}
      \verb https://wildsense.ai/servicios/robotica-submarina-automatizada-para-salmonicultura-de-alta-mar-r-sasa
      \endverb
    \endentry
    \entry{wildsense-2}{slide}{}
      \name{author}{1}{}{%
        {{hash=159128084613cd3714b9bcd52c6c532d}{%
           family={{WildSense}},
           familyi={W\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Valparaíso, Chile}%
      }
      \strng{namehash}{159128084613cd3714b9bcd52c6c532d}
      \strng{fullhash}{159128084613cd3714b9bcd52c6c532d}
      \strng{bibnamehash}{159128084613cd3714b9bcd52c6c532d}
      \strng{authorbibnamehash}{159128084613cd3714b9bcd52c6c532d}
      \strng{authornamehash}{159128084613cd3714b9bcd52c6c532d}
      \strng{authorfullhash}{159128084613cd3714b9bcd52c6c532d}
      \field{extraname}{2}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{howpublished}{Diagrama de flujo en la página 10 e imágenes en la página 14}
      \field{title}{Memoria Anual 2024}
      \field{type}{slide}
      \field{urlday}{25}
      \field{urlmonth}{5}
      \field{urlyear}{2025}
      \field{year}{2024}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://wildsense.ai/wp-content/uploads/2024/01/Memoria-Anual-2024.pdf
      \endverb
      \verb{url}
      \verb https://wildsense.ai/wp-content/uploads/2024/01/Memoria-Anual-2024.pdf
      \endverb
    \endentry
    \entry{FAO2018}{book}{}
      \name{author}{1}{}{%
        {{hash=d6edc5a90c5bc49cb2c1714e8f5320b1}{%
           family={{United Nations Food and Agriculture Organization (FAO)}},
           familyi={U\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Rome, Italy}%
      }
      \list{publisher}{1}{%
        {FAO}%
      }
      \strng{namehash}{d6edc5a90c5bc49cb2c1714e8f5320b1}
      \strng{fullhash}{d6edc5a90c5bc49cb2c1714e8f5320b1}
      \strng{bibnamehash}{d6edc5a90c5bc49cb2c1714e8f5320b1}
      \strng{authorbibnamehash}{d6edc5a90c5bc49cb2c1714e8f5320b1}
      \strng{authornamehash}{d6edc5a90c5bc49cb2c1714e8f5320b1}
      \strng{authorfullhash}{d6edc5a90c5bc49cb2c1714e8f5320b1}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{The State of World Fisheries and Aquaculture 2019 (SOFIA): Meeting the Sustainable Development Goals}
      \field{year}{2018}
      \verb{urlraw}
      \verb https://openknowledge.fao.org/handle/20.500.14283/9540es
      \endverb
      \verb{url}
      \verb https://openknowledge.fao.org/handle/20.500.14283/9540es
      \endverb
    \endentry
    \entry{chile2017levantamiento}{report}{}
      \name{author}{1}{}{%
        {{hash=3ffc03be27286762a5cd646bb3a03c44}{%
           family={{Gobierno de Chile}},
           familyi={G\bibinitperiod}}}%
      }
      \name{shortauthor}{1}{}{%
        {{hash=78cf6bc9a4fec975e148edeabf5220c1}{%
           family={{Gob. de Chile}},
           familyi={G\bibinitperiod}}}%
      }
      \list{institution}{1}{%
        {Fondo de Investigación Pesquera; Ministerio de Economía, Fomento y Turismo}%
      }
      \strng{namehash}{78cf6bc9a4fec975e148edeabf5220c1}
      \strng{fullhash}{3ffc03be27286762a5cd646bb3a03c44}
      \strng{bibnamehash}{78cf6bc9a4fec975e148edeabf5220c1}
      \strng{authorbibnamehash}{3ffc03be27286762a5cd646bb3a03c44}
      \strng{authornamehash}{3ffc03be27286762a5cd646bb3a03c44}
      \strng{authorfullhash}{3ffc03be27286762a5cd646bb3a03c44}
      \strng{shortauthorbibnamehash}{78cf6bc9a4fec975e148edeabf5220c1}
      \strng{shortauthornamehash}{78cf6bc9a4fec975e148edeabf5220c1}
      \strng{shortauthorfullhash}{78cf6bc9a4fec975e148edeabf5220c1}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{shortauthor}
      \field{labeltitlesource}{title}
      \field{title}{Levantamiento de Información de Pisciculturas en Chile y su Incorporación a la IDE de la División de Acuicultura}
      \field{type}{techreport}
      \field{year}{2017}
    \endentry
    \entry{Tonachella2022}{article}{}
      \name{author}{6}{}{%
        {{hash=09993522dff72a36d5a7948a951bb82a}{%
           family={Tonachella},
           familyi={T\bibinitperiod},
           given={Nicolò},
           giveni={N\bibinitperiod}}}%
        {{hash=b4ae08602c91f3e67e37f9f7f38e2698}{%
           family={Martini},
           familyi={M\bibinitperiod},
           given={Arianna},
           giveni={A\bibinitperiod}}}%
        {{hash=70f2c09a7a48e2b10efade731e559ae9}{%
           family={Martinoli},
           familyi={M\bibinitperiod},
           given={Marco},
           giveni={M\bibinitperiod}}}%
        {{hash=9806be5aef5b3bac34a4013ce43337d0}{%
           family={Pulcini},
           familyi={P\bibinitperiod},
           given={Domitilla},
           giveni={D\bibinitperiod}}}%
        {{hash=17264c4a357113709996c8d47578eb84}{%
           family={Romano},
           familyi={R\bibinitperiod},
           given={Andrea},
           giveni={A\bibinitperiod}}}%
        {{hash=91cfc64ff5054dd48004eabc674f294b}{%
           family={Capoccioni},
           familyi={C\bibinitperiod},
           given={Fabrizio},
           giveni={F\bibinitperiod}}}%
      }
      \strng{namehash}{c1d600e3fa3db5273524272a62ba90d2}
      \strng{fullhash}{4c2eb325fc6012b590c8f4c8383c2d3d}
      \strng{bibnamehash}{c1d600e3fa3db5273524272a62ba90d2}
      \strng{authorbibnamehash}{c1d600e3fa3db5273524272a62ba90d2}
      \strng{authornamehash}{c1d600e3fa3db5273524272a62ba90d2}
      \strng{authorfullhash}{4c2eb325fc6012b590c8f4c8383c2d3d}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Common aquaculture practices involve measuring fish biometrics at different growth stages, which is crucial for feeding regime management and for improving farmed fish welfare. Fish measurements are usually carried out manually on individual fish. However, this process is laborious, time-consuming, and stressful to the fish. Therefore, the development of fast, precise, low cost and indirect measurement would be of great interest to the aquaculture sector. In this study, we explore a promising way to take fish measurements in a non-invasive approach through computer vision. Images captured by a stereoscopic camera are used by Artificial Intelligence algorithms in conjunction with computer vision to automatically obtain an accurate estimation of the characteristics of fish, such as body length and weight. We describe the development of a computer vision system for automated recognition of body traits through image processing and linear models for the measurement of fish length and prediction of body weight. The measurements are obtained through a relatively low-cost prototype consisting of a smart buoy equipped with stereo cameras, tested in a commercial mariculture cage in the Mediterranean Sea. Our findings suggest that this method can successfully estimate fish biometric parameters, with a mean error of ±1.15 cm.}
      \field{bookpagination}{page}
      \field{day}{19}
      \field{issn}{2045-2322}
      \field{journaltitle}{Scientific Reports}
      \field{month}{9}
      \field{number}{1}
      \field{pagination}{column}
      \field{title}{An affordable and easy-to-use tool for automatic fish length and weight estimation in mariculture}
      \field{volume}{12}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{pages}{15642}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1038/s41598-022-19932-9
      \endverb
    \endentry
    \entry{Ahmed2022}{article}{}
      \name{author}{3}{}{%
        {{hash=227b42547e1ee43c8e42653f104a3228}{%
           family={Ahmed},
           familyi={A\bibinitperiod},
           given={Md\bibnamedelima Shoaib},
           giveni={M\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=9cb6a3a8f240c61a9e21307d4fe74b16}{%
           family={Aurpa},
           familyi={A\bibinitperiod},
           given={Tanjim\bibnamedelima Taharat},
           giveni={T\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
        {{hash=d649b420fbfb013fe6784e6abc16eafa}{%
           family={Kalam\bibnamedelima Azad},
           familyi={K\bibinitperiod\bibinitdelim A\bibinitperiod},
           given={Md\bibnamedelima Abul},
           giveni={M\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \strng{namehash}{3380dc69667fecc156f2bd41a8281927}
      \strng{fullhash}{3380dc69667fecc156f2bd41a8281927}
      \strng{bibnamehash}{3380dc69667fecc156f2bd41a8281927}
      \strng{authorbibnamehash}{3380dc69667fecc156f2bd41a8281927}
      \strng{authornamehash}{3380dc69667fecc156f2bd41a8281927}
      \strng{authorfullhash}{3380dc69667fecc156f2bd41a8281927}
      \field{sortinit}{7}
      \field{sortinithash}{108d0be1b1bee9773a1173443802c0a3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Fish diseases in aquaculture constitute a significant hazard to nutriment security. Identification of infected fishes in aquaculture remains challenging to find out at the early stage due to the dearth of necessary infrastructure. The identification of infected fish timely is an obligatory step to thwart from spreading disease. In this work, we want to find out the salmon fish disease in aquaculture, as salmon aquaculture is the fastest-growing food production system globally, accounting for 70 percent (2.5 million tons) of the market. In the alliance of flawless image processing and machine learning mechanism, we identify the infected fishes caused by the various pathogen. This work divides into two portions. In the rudimentary portion, image pre-processing and segmentation have been applied to reduce noise and exaggerate the image, respectively. In the second portion, we extract the involved features to classify the diseases with the help of the Support Vector Machine (SVM) algorithm of machine learning with a kernel function. The processed images of the first portion have passed through this (SVM) model. Then we harmonize a comprehensive experiment with the proposed combination of techniques on the salmon fish image dataset used to examine the fish disease. We have conveyed this work on a novel dataset compromising with and without image augmentation. The results have bought a judgment of our applied SVM performs notably with 91.42 and 94.12 percent of accuracy, respectively, with and without augmentation.}
      \field{bookpagination}{page}
      \field{issn}{1319-1578}
      \field{journaltitle}{Journal of King Saud University - Computer and Information Sciences}
      \field{number}{8, Part A}
      \field{pagination}{column}
      \field{title}{Fish Disease Detection Using Image Based Machine Learning Technique in Aquaculture}
      \field{volume}{34}
      \field{year}{2022}
      \field{pages}{5170\bibrangedash 5182}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1016/j.jksuci.2021.05.003
      \endverb
      \keyw{Fish Diseases,Aquaculture,Image Processing,Machine Learning,Support Vector Machine,Salmon Fish}
    \endentry
    \entry{MaskRCNN}{article}{}
      \name{author}{4}{}{%
        {{hash=6b4b60e909e78633945f3f9c9dc83e01}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Kaiming},
           giveni={K\bibinitperiod}}}%
        {{hash=6e67eba235a8cccf3a69b746a53d5722}{%
           family={Gkioxari},
           familyi={G\bibinitperiod},
           given={Georgia},
           giveni={G\bibinitperiod}}}%
        {{hash=ecd149fdcb3e0503881d49e545744c3d}{%
           family={Dollár},
           familyi={D\bibinitperiod},
           given={Piotr},
           giveni={P\bibinitperiod}}}%
        {{hash=bd5dadbe57bedc5957c19a3154c4d424}{%
           family={Girshick},
           familyi={G\bibinitperiod},
           given={Ross},
           giveni={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{fullhash}{25317aa10282a4a31c271050ed66c455}
      \strng{bibnamehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{authorbibnamehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{authornamehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{authorfullhash}{25317aa10282a4a31c271050ed66c455}
      \field{extraname}{1}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron.}
      \field{bookpagination}{page}
      \field{day}{6}
      \field{issn}{1939-3539}
      \field{journaltitle}{IEEE Transactions on Pattern Analysis and Machine Intelligence}
      \field{month}{6}
      \field{number}{2}
      \field{pagination}{column}
      \field{title}{Mask R-CNN}
      \field{volume}{42}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{pages}{386\bibrangedash 397}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1109/TPAMI.2018.2844175
      \endverb
      \keyw{Task analysis;Semantics;Feature extraction;Object detection;Proposals;Image segmentation;Quantization (signal);Instance segmentation;object detection;pose estimation;convolutional neural network}
    \endentry
    \entry{FasterRCNN}{article}{}
      \name{author}{4}{}{%
        {{hash=bb295293acacd54387339079ebbe4ead}{%
           family={Ren},
           familyi={R\bibinitperiod},
           given={Shaoqing},
           giveni={S\bibinitperiod}}}%
        {{hash=6b4b60e909e78633945f3f9c9dc83e01}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Kaiming},
           giveni={K\bibinitperiod}}}%
        {{hash=bd5dadbe57bedc5957c19a3154c4d424}{%
           family={Girshick},
           familyi={G\bibinitperiod},
           given={Ross},
           giveni={R\bibinitperiod}}}%
        {{hash=f85751488058842b5777c7b4074077b5}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Jian},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{f086ca4da3e532e8a41cb758ea461825}
      \strng{fullhash}{008a132af3e2d4ff15eb01a8fb4b005c}
      \strng{bibnamehash}{f086ca4da3e532e8a41cb758ea461825}
      \strng{authorbibnamehash}{f086ca4da3e532e8a41cb758ea461825}
      \strng{authornamehash}{f086ca4da3e532e8a41cb758ea461825}
      \strng{authorfullhash}{008a132af3e2d4ff15eb01a8fb4b005c}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network(RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.}
      \field{bookpagination}{page}
      \field{day}{1}
      \field{issn}{1939-3539}
      \field{journaltitle}{IEEE Transactions on Pattern Analysis and Machine Intelligence}
      \field{month}{6}
      \field{number}{6}
      \field{pagination}{column}
      \field{title}{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}
      \field{volume}{39}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{pages}{1137\bibrangedash 1149}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1109/TPAMI.2016.2577031
      \endverb
      \keyw{Proposals;Object detection;Convolutional codes;Feature extraction;Search problems;Detectors;Training;Object detection;region proposal;convolutional neural network}
    \endentry
    \entry{MaskScoringRCNN}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=d31d2f6bfd14a82b2acd97f537f57644}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Zhaojin},
           giveni={Z\bibinitperiod}}}%
        {{hash=619d5d5922e17b00d07e19f1f5b2483a}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Lichao},
           giveni={L\bibinitperiod}}}%
        {{hash=7f1ecbea46310720f0079915a78a65be}{%
           family={Gong},
           familyi={G\bibinitperiod},
           given={Yongchao},
           giveni={Y\bibinitperiod}}}%
        {{hash=c579e88b7416fed9966902d191cd4775}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Chang},
           giveni={C\bibinitperiod}}}%
        {{hash=1bd616b774bd66d7d77e025ac08c962d}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Xinggang},
           giveni={X\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{7558a8fa3a4d812052f3aa81a39aa404}
      \strng{fullhash}{6768b2ada79b0c8f5d09d4ce25c5aff7}
      \strng{bibnamehash}{7558a8fa3a4d812052f3aa81a39aa404}
      \strng{authorbibnamehash}{7558a8fa3a4d812052f3aa81a39aa404}
      \strng{authornamehash}{7558a8fa3a4d812052f3aa81a39aa404}
      \strng{authorfullhash}{6768b2ada79b0c8f5d09d4ce25c5aff7}
      \field{extraname}{1}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Letting a deep network be aware of the quality of its own predictions is an interesting yet important problem. In the task of instance segmentation, the confidence of instance classification is used as mask quality score in most instance segmentation frameworks. However, the mask quality, quantified as the IoU between the instance mask and its ground truth, is usually not well correlated with classification score. In this paper, we study this problem and propose Mask Scoring R-CNN which contains a network block to learn the quality of the predicted instance masks. The proposed network block takes the instance feature and the corresponding predicted mask together to regress the mask IoU. The mask scoring strategy calibrates the misalignment between mask quality and mask score, and improves instance segmentation performance by prioritizing more accurate mask predictions during COCO AP evaluation. By extensive evaluations on the COCO dataset, Mask Scoring R-CNN brings consistent and noticeable gain with different models and outperforms the state-of-the-art Mask R-CNN. We hope our simple and effective approach will provide a new direction for improving instance segmentation. The source code of our method is available at \url{https://github.com/zjhuang22/maskscoring_rcnn}.}
      \field{booktitle}{2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
      \field{day}{9}
      \field{eventday}{15}
      \field{eventendday}{20}
      \field{eventendmonth}{6}
      \field{eventendyear}{2019}
      \field{eventmonth}{6}
      \field{eventyear}{2019}
      \field{isbn}{978-1-7281-3293-8}
      \field{issn}{2575-7075}
      \field{month}{1}
      \field{title}{Mask Scoring R-CNN}
      \field{venue}{Long Beach, CA, USA}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{6402\bibrangedash 6411}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/CVPR.2019.00657
      \endverb
      \keyw{Recognition: Detection;Categorization;Retrieval}
    \endentry
    \entry{SOLOv1}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=f0b30f46a1651ff938af38e8ee0ea476}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Xinlong},
           giveni={X\bibinitperiod}}}%
        {{hash=a8bab71af262719ac597cef5f9c64c2f}{%
           family={Kong},
           familyi={K\bibinitperiod},
           given={Tao},
           giveni={T\bibinitperiod}}}%
        {{hash=9e49384c78f6861746fe787b12339aed}{%
           family={Shen},
           familyi={S\bibinitperiod},
           given={Chunhua},
           giveni={C\bibinitperiod}}}%
        {{hash=50b19fa0d073b0e04e584a8b6277eb0b}{%
           family={Jiang},
           familyi={J\bibinitperiod},
           given={Yuning},
           giveni={Y\bibinitperiod}}}%
        {{hash=ad7517ebe80619f75e6261e4aae4f0e1}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Lei},
           giveni={L\bibinitperiod}}}%
      }
      \name{editor}{4}{}{%
        {{hash=85ab53268da1006b51b453d57d3566f2}{%
           family={Vedaldi},
           familyi={V\bibinitperiod},
           given={Andrea},
           giveni={A\bibinitperiod}}}%
        {{hash=a3e36d7b360de7388d78599c2446ac34}{%
           family={Bischof},
           familyi={B\bibinitperiod},
           given={Horst},
           giveni={H\bibinitperiod}}}%
        {{hash=b452a32296958371572717940f900884}{%
           family={Brox},
           familyi={B\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
        {{hash=4af0c62bceb93c143845555052179bc8}{%
           family={Frahm},
           familyi={F\bibinitperiod},
           given={Jan-Michael},
           giveni={J\bibinithyphendelim M\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer International Publishing}%
      }
      \strng{namehash}{2a57bcfd2a466636dd1e1290549b835a}
      \strng{fullhash}{cad9458aa3850104bf4188ac763eff80}
      \strng{bibnamehash}{2a57bcfd2a466636dd1e1290549b835a}
      \strng{authorbibnamehash}{2a57bcfd2a466636dd1e1290549b835a}
      \strng{authornamehash}{2a57bcfd2a466636dd1e1290549b835a}
      \strng{authorfullhash}{cad9458aa3850104bf4188ac763eff80}
      \strng{editorbibnamehash}{282d479612a43d3afb0ccff664d33d17}
      \strng{editornamehash}{282d479612a43d3afb0ccff664d33d17}
      \strng{editorfullhash}{7914c282424cc181ca1aa3c2103905d2}
      \field{extraname}{1}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a new, embarrassingly simple approach to instance segmentation. Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that have made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the ``detect-then-segment'' strategy (e.g., Mask R-CNN), or predict embedding vectors first then use clustering techniques to group pixels into individual instances. We view the task of instance segmentation from a completely new perspective by introducing the notion of ``instance categories'', which assigns categories to each pixel within an instance according to the instance's location and size, thus nicely converting instance segmentation into a single-shot classification-solvable problem. We demonstrate a much simpler and flexible instance segmentation framework with strong performance, achieving on par accuracy with Mask R-CNN and outperforming recent single-shot instance segmenters in accuracy. We hope that this simple and strong framework can serve as a baseline for many instance-level recognition tasks besides instance segmentation. Code is available at https://git.io/AdelaiDet.}
      \field{booktitle}{Computer Vision -- ECCV 2020}
      \field{day}{4}
      \field{eventday}{23}
      \field{eventendday}{28}
      \field{eventendmonth}{8}
      \field{eventendyear}{2020}
      \field{eventmonth}{8}
      \field{eventyear}{2020}
      \field{month}{12}
      \field{title}{SOLO: Segmenting Objects by Locations}
      \field{venue}{Glasgow, UK}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{649\bibrangedash 665}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1007/978-3-030-58523-5_38
      \endverb
    \endentry
    \entry{SOLOv2}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=f0b30f46a1651ff938af38e8ee0ea476}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Xinlong},
           giveni={X\bibinitperiod}}}%
        {{hash=0c76febdec093d8395932fe075e1591a}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Rufeng},
           giveni={R\bibinitperiod}}}%
        {{hash=a8bab71af262719ac597cef5f9c64c2f}{%
           family={Kong},
           familyi={K\bibinitperiod},
           given={Tao},
           giveni={T\bibinitperiod}}}%
        {{hash=ad7517ebe80619f75e6261e4aae4f0e1}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Lei},
           giveni={L\bibinitperiod}}}%
        {{hash=9e49384c78f6861746fe787b12339aed}{%
           family={Shen},
           familyi={S\bibinitperiod},
           given={Chunhua},
           giveni={C\bibinitperiod}}}%
      }
      \name{editor}{5}{}{%
        {{hash=6c15b92d4ebdbfae091f6548dc7543ae}{%
           family={Larochelle},
           familyi={L\bibinitperiod},
           given={H.},
           giveni={H\bibinitperiod}}}%
        {{hash=6cb5af0e649387de5bbfdb150cc7a786}{%
           family={Ranzato},
           familyi={R\bibinitperiod},
           given={M.},
           giveni={M\bibinitperiod}}}%
        {{hash=ae95119be0d9d86ffdec9f3142af353e}{%
           family={Hadsell},
           familyi={H\bibinitperiod},
           given={R.},
           giveni={R\bibinitperiod}}}%
        {{hash=20f36d8aae4f5467e0c03ccdaee4076a}{%
           family={Balcan},
           familyi={B\bibinitperiod},
           given={M.F.},
           giveni={M\bibinitperiod}}}%
        {{hash=006d527428ae3b86c2093d6d28e16072}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={H.},
           giveni={H\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{2a57bcfd2a466636dd1e1290549b835a}
      \strng{fullhash}{ae0d7e6f3fc26b04928e3cd7d058a676}
      \strng{bibnamehash}{2a57bcfd2a466636dd1e1290549b835a}
      \strng{authorbibnamehash}{2a57bcfd2a466636dd1e1290549b835a}
      \strng{authornamehash}{2a57bcfd2a466636dd1e1290549b835a}
      \strng{authorfullhash}{ae0d7e6f3fc26b04928e3cd7d058a676}
      \strng{editorbibnamehash}{1bb757b97fffae621331dd700b5a7ca9}
      \strng{editornamehash}{1bb757b97fffae621331dd700b5a7ca9}
      \strng{editorfullhash}{01684a8bcecde64514068030e781dc4f}
      \field{extraname}{2}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this work, we design a simple, direct, and fast framework for instance segmentation with strong performance. To this end, we propose a novel and effective approach, termed SOLOv2, following the principle of the SOLO method [32]. First, our new framework is empowered by an efficient and holistic instance mask representation scheme, which dynamically segments each instance in the image, without resorting to bounding box detection. Specifically, the object mask generation is decoupled into a mask kernel prediction and mask feature learning, which are responsible for generating convolution kernels and the feature maps to be convolved with, respectively. Second, SOLOv2 significantly reduces inference overhead with our novel matrix non-maximum suppression (NMS) technique. Our Matrix NMS performs NMS with parallel matrix operations in one shot, and yields better results. We demonstrate that the proposed SOLOv2 achieves the state-of-the- art performance with high efficiency, making it suitable for both mobile and cloud applications. A light-weight version of SOLOv2 executes at 31.3 FPS and yields 37.1\% AP on COCO test-dev. Moreover, our state-of-the-art results in object detection (from our mask byproduct) and panoptic segmentation show the potential of SOLOv2 to serve as a new strong baseline for many instance-level recognition tasks. Code is available at https://git.io/AdelaiDet}
      \field{booktitle}{Advances in Neural Information Processing Systems}
      \field{eventday}{6}
      \field{eventendday}{12}
      \field{eventendmonth}{12}
      \field{eventendyear}{2020}
      \field{eventmonth}{12}
      \field{eventyear}{2020}
      \field{title}{SOLOv2: Dynamic and Fast Instance Segmentation}
      \field{volume}{33}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{17721\bibrangedash 17732}
      \range{pages}{12}
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper_files/paper/2020/file/cd3afef9b8b89558cd56638c3631868a-Paper.pdf
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper_files/paper/2020/file/cd3afef9b8b89558cd56638c3631868a-Paper.pdf
      \endverb
    \endentry
    \entry{yolov8_ultralytics}{online}{}
      \name{author}{3}{}{%
        {{hash=c5c5e5cd7f4f477f617289b32613deb0}{%
           family={Jocher},
           familyi={J\bibinitperiod},
           given={Glenn},
           giveni={G\bibinitperiod}}}%
        {{hash=ec281637802454e8df4de72f3b1ee401}{%
           family={Chaurasia},
           familyi={C\bibinitperiod},
           given={Ayush},
           giveni={A\bibinitperiod}}}%
        {{hash=b8739ebbf3f871d471b6720a02ed541f}{%
           family={Qiu},
           familyi={Q\bibinitperiod},
           given={Jing},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{dde928f96d7839eed6effd0fbfbe5acc}
      \strng{fullhash}{dde928f96d7839eed6effd0fbfbe5acc}
      \strng{bibnamehash}{dde928f96d7839eed6effd0fbfbe5acc}
      \strng{authorbibnamehash}{dde928f96d7839eed6effd0fbfbe5acc}
      \strng{authornamehash}{dde928f96d7839eed6effd0fbfbe5acc}
      \strng{authorfullhash}{dde928f96d7839eed6effd0fbfbe5acc}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Ultralytics YOLOv8}
      \field{version}{8.0.0}
      \field{year}{2023}
      \verb{urlraw}
      \verb https://github.com/ultralytics/ultralytics
      \endverb
      \verb{url}
      \verb https://github.com/ultralytics/ultralytics
      \endverb
    \endentry
    \entry{YOLOv8}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=d92a8f1e7a8c9052a7cbb096b9ada008}{%
           family={Sohan},
           familyi={S\bibinitperiod},
           given={Mupparaju},
           giveni={M\bibinitperiod}}}%
        {{hash=409ed8a8b6e210411bd3bce460ddd023}{%
           family={Ram},
           familyi={R\bibinitperiod},
           given={Thotakura},
           giveni={T\bibinitperiod}}}%
        {{hash=e06dfd708d97a98062b7efb63a9e7d5a}{%
           family={Ch},
           familyi={C\bibinitperiod},
           given={Venkata},
           giveni={V\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer Nature Singapore}%
      }
      \strng{namehash}{645add1fb4c7ab7af053c813bcdeaf24}
      \strng{fullhash}{645add1fb4c7ab7af053c813bcdeaf24}
      \strng{bibnamehash}{645add1fb4c7ab7af053c813bcdeaf24}
      \strng{authorbibnamehash}{645add1fb4c7ab7af053c813bcdeaf24}
      \strng{authornamehash}{645add1fb4c7ab7af053c813bcdeaf24}
      \strng{authorfullhash}{645add1fb4c7ab7af053c813bcdeaf24}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Object detection is a crucial task in computer vision that has its application in various fields like robotics, medical imaging, surveillance systems, and autonomous vehicles. The newest version of the YOLO model, YOLOv8 is an advanced real-time object detection framework, which has attracted the attention of the research community. Of all the popular object identification methods and machine-learning models such as Faster RCNN, SSD, and RetinaNet, YOLO is the most popularly known method in terms of accuracy, speed, and efficiency. This research study provides an analysis of YOLO v8 by highlighting its innovative features, improvements, applicability in different environments, and a detailed comparison of its performance metrics to other versions and models.}
      \field{booktitle}{Data Intelligence and Cognitive Informatics}
      \field{eventday}{27}
      \field{eventendday}{28}
      \field{eventendmonth}{6}
      \field{eventendyear}{2023}
      \field{eventmonth}{6}
      \field{eventyear}{2023}
      \field{month}{1}
      \field{title}{A Review on YOLOv8 and Its Advancements}
      \field{venue}{SCAD College of Engineering and Technology, Tirunelveli, India}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{529\bibrangedash 545}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1007/978-981-99-7962-2_39
      \endverb
    \endentry
    \entry{FastSAM}{article}{}
      \name{author}{8}{}{%
        {{hash=f79215132dad5094a6a084f1445d13ef}{%
           family={Zhao},
           familyi={Z\bibinitperiod},
           given={Xu},
           giveni={X\bibinitperiod}}}%
        {{hash=fd6eddc4fec8e197dc4dc93c198036e4}{%
           family={Ding},
           familyi={D\bibinitperiod},
           given={Wenchao},
           giveni={W\bibinitperiod}}}%
        {{hash=0bc4a1f2bfc5bf7a1f86d9067b8485f6}{%
           family={An},
           familyi={A\bibinitperiod},
           given={Yongqi},
           giveni={Y\bibinitperiod}}}%
        {{hash=c1e1777b69041487ce830bef16a62d0f}{%
           family={Du},
           familyi={D\bibinitperiod},
           given={Yinglong},
           giveni={Y\bibinitperiod}}}%
        {{hash=fe5435f0fc61cb65658815c3b6163c49}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Tao},
           giveni={T\bibinitperiod}}}%
        {{hash=dd037b3b861a870d96b7329da5d30d28}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Min},
           giveni={M\bibinitperiod}}}%
        {{hash=49a8682754fe9a71c0af7456b84b0199}{%
           family={Tang},
           familyi={T\bibinitperiod},
           given={Ming},
           giveni={M\bibinitperiod}}}%
        {{hash=ab1349dd88d6f6c9fdf115069f659dff}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Jinqiao},
           giveni={J\bibinitperiod}}}%
      }
      \list{institution}{1}{%
        {Institute of Automation, Chinese Academy of Sciences}%
      }
      \list{location}{1}{%
        {Beijing, China}%
      }
      \strng{namehash}{97c011fd189e2ba3767f1e3e56df425b}
      \strng{fullhash}{bd32a662f1dbe265226be0c0a6073e8c}
      \strng{bibnamehash}{97c011fd189e2ba3767f1e3e56df425b}
      \strng{authorbibnamehash}{97c011fd189e2ba3767f1e3e56df425b}
      \strng{authornamehash}{97c011fd189e2ba3767f1e3e56df425b}
      \strng{authorfullhash}{bd32a662f1dbe265226be0c0a6073e8c}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The recently proposed segment anything model (SAM) has made a significant influence in many computer vision tasks. It is becoming a foundation step for many high-level tasks, like image segmentation, image caption, and image editing. However, its huge computation costs prevent it from wider applications in industry scenarios. The computation mainly comes from the Transformer architecture at high-resolution inputs. In this paper, we propose a speed-up alternative method for this fundamental task with comparable performance. By reformulating the task as segments-generation and prompting, we find that a regular CNN detector with an instance segmentation branch can also accomplish this task well. Specifically, we convert this task to the well-studied instance segmentation task and directly train the existing instance segmentation method using only 1/50 of the SA-1B dataset published by SAM authors. With our method, we achieve a comparable performance with the SAM method at 50 times higher run-time speed. We give sufficient experimental results to demonstrate its effectiveness. The codes and demos will be released at https://github.com/CASIA-IVA-Lab/FastSAM.}
      \field{bookpagination}{page}
      \field{day}{21}
      \field{journaltitle}{ArXiv}
      \field{month}{6}
      \field{pagination}{column}
      \field{title}{Fast Segment Anything}
      \field{year}{2023}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2306.12156
      \endverb
    \endentry
    \entry{YOLOv9}{article}{}
      \name{author}{2}{}{%
        {{hash=053d491932411950eccf2bb9a1025d11}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Chien-Yao},
           giveni={C\bibinithyphendelim Y\bibinitperiod}}}%
        {{hash=c81478c691c1bc1cc01404c44b5bd5bf}{%
           family={Liao},
           familyi={L\bibinitperiod},
           given={Hong-Yuan\bibnamedelima Mark},
           giveni={H\bibinithyphendelim Y\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \strng{namehash}{18e0d504e047f12a16c4429a5aaf2908}
      \strng{fullhash}{18e0d504e047f12a16c4429a5aaf2908}
      \strng{bibnamehash}{18e0d504e047f12a16c4429a5aaf2908}
      \strng{authorbibnamehash}{18e0d504e047f12a16c4429a5aaf2908}
      \strng{authornamehash}{18e0d504e047f12a16c4429a5aaf2908}
      \strng{authorfullhash}{18e0d504e047f12a16c4429a5aaf2908}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Today's deep learning methods focus on how to design the most appropriate objective functions so that the prediction results of the model can be closest to the ground truth. Meanwhile, an appropriate architecture that can facilitate acquisition of enough information for prediction has to be designed. Existing methods ignore a fact that when input data undergoes layer-by-layer feature extraction and spatial transformation, large amount of information will be lost. This paper will delve into the important issues of data loss when data is transmitted through deep networks, namely information bottleneck and reversible functions. We proposed the concept of programmable gradient information (PGI) to cope with the various changes required by deep networks to achieve multiple objectives. PGI can provide complete input information for the target task to calculate objective function, so that reliable gradient information can be obtained to update network weights. In addition, a new lightweight network architecture -- Generalized Efficient Layer Aggregation Network (GELAN), based on gradient path planning is designed. GELAN's architecture confirms that PGI has gained superior results on lightweight models. We verified the proposed GELAN and PGI on MS COCO dataset based object detection. The results show that GELAN only uses conventional convolution operators to achieve better parameter utilization than the state-of-the-art methods developed based on depth-wise convolution. PGI can be used for variety of models from lightweight to large. It can be used to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the comparison results are shown in Figure 1. The source codes are at: https://github.com/WongKinYiu/yolov9.}
      \field{bookpagination}{page}
      \field{day}{29}
      \field{journaltitle}{ArXiv}
      \field{month}{2}
      \field{pagination}{column}
      \field{title}{YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information}
      \field{year}{2024}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2402.13616
      \endverb
    \endentry
    \entry{yolo11_ultralytics}{online}{}
      \name{author}{2}{}{%
        {{hash=c5c5e5cd7f4f477f617289b32613deb0}{%
           family={Jocher},
           familyi={J\bibinitperiod},
           given={Glenn},
           giveni={G\bibinitperiod}}}%
        {{hash=b8739ebbf3f871d471b6720a02ed541f}{%
           family={Qiu},
           familyi={Q\bibinitperiod},
           given={Jing},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{7ffa0dad9709ac21fa40cfa507db39d1}
      \strng{fullhash}{7ffa0dad9709ac21fa40cfa507db39d1}
      \strng{bibnamehash}{7ffa0dad9709ac21fa40cfa507db39d1}
      \strng{authorbibnamehash}{7ffa0dad9709ac21fa40cfa507db39d1}
      \strng{authornamehash}{7ffa0dad9709ac21fa40cfa507db39d1}
      \strng{authorfullhash}{7ffa0dad9709ac21fa40cfa507db39d1}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Ultralytics YOLO11}
      \field{version}{11.0.0}
      \field{year}{2024}
      \verb{urlraw}
      \verb https://github.com/ultralytics/ultralytics
      \endverb
      \verb{url}
      \verb https://github.com/ultralytics/ultralytics
      \endverb
    \endentry
    \entry{YOLOv11}{misc}{}
      \name{author}{2}{}{%
        {{hash=a94f326244d9ab506d3da2d9aa20e3c0}{%
           family={Khanam},
           familyi={K\bibinitperiod},
           given={Rahima},
           giveni={R\bibinitperiod}}}%
        {{hash=506b587186b760b2f884e19c703f7ffd}{%
           family={Hussain},
           familyi={H\bibinitperiod},
           given={Muhammad},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{4d2c72e575a86de750b3c33375e767d9}
      \strng{fullhash}{4d2c72e575a86de750b3c33375e767d9}
      \strng{bibnamehash}{4d2c72e575a86de750b3c33375e767d9}
      \strng{authorbibnamehash}{4d2c72e575a86de750b3c33375e767d9}
      \strng{authornamehash}{4d2c72e575a86de750b3c33375e767d9}
      \strng{authorfullhash}{4d2c72e575a86de750b3c33375e767d9}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This study presents an architectural analysis of YOLOv11, the latest iteration in the YOLO (You Only Look Once) series of object detection models. We examine the models architectural innovations, including the introduction of the C3k2 (Cross Stage Partial with kernel size 2) block, SPPF (Spatial Pyramid Pooling - Fast), and C2PSA (Convolutional block with Parallel Spatial Attention) components, which contribute in improving the models performance in several ways such as enhanced feature extraction. The paper explores YOLOv11's expanded capabilities across various computer vision tasks, including object detection, instance segmentation, pose estimation, and oriented object detection (OBB). We review the model's performance improvements in terms of mean Average Precision (mAP) and computational efficiency compared to its predecessors, with a focus on the trade-off between parameter count and accuracy. Additionally, the study discusses YOLOv11's versatility across different model sizes, from nano to extra-large, catering to diverse application needs from edge devices to high-performance computing environments. Our research provides insights into YOLOv11's position within the broader landscape of object detection and its potential impact on real-time computer vision applications.}
      \field{day}{23}
      \field{month}{10}
      \field{title}{YOLOv11: An Overview of the Key Architectural Enhancements}
      \field{year}{2024}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2410.17725
      \endverb
    \endentry
    \entry{Goodfellow-et-al-2016}{book}{}
      \name{author}{3}{}{%
        {{hash=5d2585c11210cf1d4512e6e0a03ec315}{%
           family={Goodfellow},
           familyi={G\bibinitperiod},
           given={Ian},
           giveni={I\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
        {{hash=ccec1ccd2e1aa86960eb2e872c6b7020}{%
           family={Courville},
           familyi={C\bibinitperiod},
           given={Aaron},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {MIT Press}%
      }
      \strng{namehash}{3ae53fe582e8a815b118d26947eaa326}
      \strng{fullhash}{3ae53fe582e8a815b118d26947eaa326}
      \strng{bibnamehash}{3ae53fe582e8a815b118d26947eaa326}
      \strng{authorbibnamehash}{3ae53fe582e8a815b118d26947eaa326}
      \strng{authornamehash}{3ae53fe582e8a815b118d26947eaa326}
      \strng{authorfullhash}{3ae53fe582e8a815b118d26947eaa326}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Deep Learning}
      \field{year}{2016}
      \field{dateera}{ce}
      \verb{urlraw}
      \verb http://www.deeplearningbook.org
      \endverb
      \verb{url}
      \verb http://www.deeplearningbook.org
      \endverb
    \endentry
    \entry{Keskar2017}{article}{}
      \name{author}{2}{}{%
        {{hash=15ea9bc75db0deb69067396346fde185}{%
           family={Keskar},
           familyi={K\bibinitperiod},
           given={Nitish\bibnamedelima Shirish},
           giveni={N\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=d5670b2600fea169724521e252d9d09d}{%
           family={Socher},
           familyi={S\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{85ad087f52382c3d3c1109270b6bf4ea}
      \strng{fullhash}{85ad087f52382c3d3c1109270b6bf4ea}
      \strng{bibnamehash}{85ad087f52382c3d3c1109270b6bf4ea}
      \strng{authorbibnamehash}{85ad087f52382c3d3c1109270b6bf4ea}
      \strng{authornamehash}{85ad087f52382c3d3c1109270b6bf4ea}
      \strng{authorfullhash}{85ad087f52382c3d3c1109270b6bf4ea}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{bookpagination}{page}
      \field{journaltitle}{ArXiv}
      \field{month}{12}
      \field{pagination}{column}
      \field{title}{Improving Generalization Performance by Switching from Adam to SGD}
      \field{year}{2017}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1712.07628
      \endverb
    \endentry
    \entry{Deng2009}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=0ae7fdc13773f928525f673b05f37149}{%
           family={Deng},
           familyi={D\bibinitperiod},
           given={Jia},
           giveni={J\bibinitperiod}}}%
        {{hash=7d87c5957b07153c7f18918b92830bf8}{%
           family={Dong},
           familyi={D\bibinitperiod},
           given={Wei},
           giveni={W\bibinitperiod}}}%
        {{hash=d5670b2600fea169724521e252d9d09d}{%
           family={Socher},
           familyi={S\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
        {{hash=2afdae52015b97674d81efea449edce2}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Li-Jia},
           giveni={L\bibinithyphendelim J\bibinitperiod}}}%
        {{hash=4838f7fdd28d5cefb28f3b3c734976d4}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Kai},
           giveni={K\bibinitperiod}}}%
        {{hash=cd00ce5bc45f687c432e52e0fa1a7aa6}{%
           family={Fei-Fei},
           familyi={F\bibinithyphendelim F\bibinitperiod},
           given={Li},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{990420f755e01028377fcad1464c9706}
      \strng{fullhash}{a16fdd05c52c264b99fe98f4a5e24c60}
      \strng{bibnamehash}{990420f755e01028377fcad1464c9706}
      \strng{authorbibnamehash}{990420f755e01028377fcad1464c9706}
      \strng{authornamehash}{990420f755e01028377fcad1464c9706}
      \strng{authorfullhash}{a16fdd05c52c264b99fe98f4a5e24c60}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.}
      \field{booktitle}{2009 IEEE Conference on Computer Vision and Pattern Recognition}
      \field{day}{18}
      \field{eventday}{20}
      \field{eventendday}{25}
      \field{eventendmonth}{6}
      \field{eventendyear}{2009}
      \field{eventmonth}{6}
      \field{eventyear}{2009}
      \field{isbn}{978-1-4244-3992-8}
      \field{issn}{1063-6919}
      \field{month}{8}
      \field{title}{ImageNet: A large-scale hierarchical image database}
      \field{venue}{Miami, FL, USA}
      \field{year}{2009}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{248\bibrangedash 255}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1109/CVPR.2009.5206848
      \endverb
      \keyw{Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine}
    \endentry
    \entry{Goyal2017}{article}{}
      \name{author}{9}{}{%
        {{hash=d87be7c19fa16049347907c9820816c6}{%
           family={Goyal},
           familyi={G\bibinitperiod},
           given={Priya},
           giveni={P\bibinitperiod}}}%
        {{hash=ecd149fdcb3e0503881d49e545744c3d}{%
           family={Dollár},
           familyi={D\bibinitperiod},
           given={Piotr},
           giveni={P\bibinitperiod}}}%
        {{hash=c2c60ceb4a241891e128295a5ae80aef}{%
           family={Girshick},
           familyi={G\bibinitperiod},
           given={Ross\bibnamedelima B.},
           giveni={R\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=c3ca2ee41cf25a9ed55b70a29aca9c9a}{%
           family={Noordhuis},
           familyi={N\bibinitperiod},
           given={Pieter},
           giveni={P\bibinitperiod}}}%
        {{hash=fe2c91d7f7f1eb9fba5b4b2349a02fbb}{%
           family={Wesolowski},
           familyi={W\bibinitperiod},
           given={Lukasz},
           giveni={L\bibinitperiod}}}%
        {{hash=fb0c5ef5bd0623316e6c5b00f3202e24}{%
           family={Kyrola},
           familyi={K\bibinitperiod},
           given={Aapo},
           giveni={A\bibinitperiod}}}%
        {{hash=ead37c072da8d4bf054810a1c3011177}{%
           family={Tulloch},
           familyi={T\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
        {{hash=9fce03efe6b3331a1b93ed2e7c0da9d5}{%
           family={Jia},
           familyi={J\bibinitperiod},
           given={Yangqing},
           giveni={Y\bibinitperiod}}}%
        {{hash=6b4b60e909e78633945f3f9c9dc83e01}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Kaiming},
           giveni={K\bibinitperiod}}}%
      }
      \strng{namehash}{cf278d9271bb4e6539a9e3cdc5225acb}
      \strng{fullhash}{f5b759ce64ac42e78b8d51852b187c97}
      \strng{bibnamehash}{cf278d9271bb4e6539a9e3cdc5225acb}
      \strng{authorbibnamehash}{cf278d9271bb4e6539a9e3cdc5225acb}
      \strng{authornamehash}{cf278d9271bb4e6539a9e3cdc5225acb}
      \strng{authorfullhash}{f5b759ce64ac42e78b8d51852b187c97}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{bookpagination}{page}
      \field{journaltitle}{ArXiv}
      \field{pagination}{column}
      \field{title}{Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}
      \field{year}{2017}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1706.02677
      \endverb
    \endentry
    \entry{He2016}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=6b4b60e909e78633945f3f9c9dc83e01}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Kaiming},
           giveni={K\bibinitperiod}}}%
        {{hash=5e72bc22dbcf0984c6d113d280e36990}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Xiangyu},
           giveni={X\bibinitperiod}}}%
        {{hash=bb295293acacd54387339079ebbe4ead}{%
           family={Ren},
           familyi={R\bibinitperiod},
           given={Shaoqing},
           giveni={S\bibinitperiod}}}%
        {{hash=f85751488058842b5777c7b4074077b5}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Jian},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{fullhash}{42c4b52dc3a62cebabbc11c73e1afb53}
      \strng{bibnamehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{authorbibnamehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{authornamehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{authorfullhash}{42c4b52dc3a62cebabbc11c73e1afb53}
      \field{extraname}{2}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.}
      \field{booktitle}{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
      \field{day}{12}
      \field{eventday}{27}
      \field{eventendday}{30}
      \field{eventendmonth}{6}
      \field{eventendyear}{2016}
      \field{eventmonth}{6}
      \field{eventyear}{2016}
      \field{isbn}{978-1-4673-8851-1}
      \field{issn}{1063-6919}
      \field{month}{12}
      \field{title}{Deep Residual Learning for Image Recognition}
      \field{venue}{Las Vegas, NV, USA}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{770\bibrangedash 778}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1109/CVPR.2016.90
      \endverb
      \keyw{Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation}
    \endentry
    \entry{Guerrero2024}{thesis}{}
      \name{author}{1}{}{%
        {{hash=d26afb3db6973d3b0cbfc7d5a90dfed2}{%
           family={Guerrero\bibnamedelima Loyola},
           familyi={G\bibinitperiod\bibinitdelim L\bibinitperiod},
           given={Alejandro\bibnamedelima Felipe},
           giveni={A\bibinitperiod\bibinitdelim F\bibinitperiod}}}%
      }
      \list{institution}{1}{%
        {UTFSM}%
      }
      \list{location}{1}{%
        {Valparaíso, Chile}%
      }
      \strng{namehash}{d26afb3db6973d3b0cbfc7d5a90dfed2}
      \strng{fullhash}{d26afb3db6973d3b0cbfc7d5a90dfed2}
      \strng{bibnamehash}{d26afb3db6973d3b0cbfc7d5a90dfed2}
      \strng{authorbibnamehash}{d26afb3db6973d3b0cbfc7d5a90dfed2}
      \strng{authornamehash}{d26afb3db6973d3b0cbfc7d5a90dfed2}
      \strng{authorfullhash}{d26afb3db6973d3b0cbfc7d5a90dfed2}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{month}{2}
      \field{note}{Advisor: Marcos Zuñiga; Prof. Correferente: Gonzalo Carvajal}
      \field{title}{Análisis del modelo YOLO-V8 para la segmentación de salmones en jaulas de cultivo en proceso estimador de biomasa, en tiempo real}
      \field{titleaddon}{Ing. Civil Electrónica}
      \field{type}{Memoria de titulación}
      \field{year}{2024}
      \field{dateera}{ce}
      \verb{urlraw}
      \verb https://repositorio.usm.cl/handle/123456789/74331
      \endverb
      \verb{url}
      \verb https://repositorio.usm.cl/handle/123456789/74331
      \endverb
    \endentry
    \entry{Lopez2024}{online}{}
      \name{author}{1}{}{%
        {{hash=516d9a66a3909b20591ffedf36cac3bf}{%
           family={López\bibnamedelima Blanche},
           familyi={L\bibinitperiod\bibinitdelim B\bibinitperiod},
           given={Julio\bibnamedelima Eduardo},
           giveni={J\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \strng{namehash}{516d9a66a3909b20591ffedf36cac3bf}
      \strng{fullhash}{516d9a66a3909b20591ffedf36cac3bf}
      \strng{bibnamehash}{516d9a66a3909b20591ffedf36cac3bf}
      \strng{authorbibnamehash}{516d9a66a3909b20591ffedf36cac3bf}
      \strng{authornamehash}{516d9a66a3909b20591ffedf36cac3bf}
      \strng{authorfullhash}{516d9a66a3909b20591ffedf36cac3bf}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{day}{10}
      \field{month}{7}
      \field{title}{Segmentación de Salmones para estimación de masa en salmonicultura.}
      \field{type}{Informe \textit{Benchmark} del proyecto final en \textit{Visión por Computador}}
      \field{urlday}{18}
      \field{urlmonth}{6}
      \field{urlyear}{2025}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://github.com/juliopchile/CV_Project
      \endverb
      \verb{url}
      \verb https://github.com/juliopchile/CV_Project
      \endverb
    \endentry
    \entry{Bengio2012}{article}{}
      \name{author}{1}{}{%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{40a8e4774982146adc2688546f54efb2}
      \strng{fullhash}{40a8e4774982146adc2688546f54efb2}
      \strng{bibnamehash}{40a8e4774982146adc2688546f54efb2}
      \strng{authorbibnamehash}{40a8e4774982146adc2688546f54efb2}
      \strng{authornamehash}{40a8e4774982146adc2688546f54efb2}
      \strng{authorfullhash}{40a8e4774982146adc2688546f54efb2}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{bookpagination}{page}
      \field{day}{24}
      \field{journaltitle}{Arxiv}
      \field{month}{6}
      \field{pagination}{column}
      \field{title}{Practical recommendations for gradient-based training of deep architectures}
      \field{year}{2012}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1206.5533
      \endverb
    \endentry
    \entry{Shallue2018}{article}{}
      \name{author}{6}{}{%
        {{hash=8217f686e554cb86b6cad9ebba97bff4}{%
           family={Shallue},
           familyi={S\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod}}}%
        {{hash=1e0fa9713f9b203d0b34967a5570c302}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Jaehoon},
           giveni={J\bibinitperiod}}}%
        {{hash=59ccc4326950257432cec204d64787e6}{%
           family={Antognini},
           familyi={A\bibinitperiod},
           given={Joseph},
           giveni={J\bibinitperiod}}}%
        {{hash=9a44e86ee2d63cb840b7579ee0b56068}{%
           family={Sohl-dickstein},
           familyi={S\bibinithyphendelim d\bibinitperiod},
           given={Jascha},
           giveni={J\bibinitperiod}}}%
        {{hash=29563c986154ca2b45d286b4dd5ef92a}{%
           family={Frostig},
           familyi={F\bibinitperiod},
           given={Roy},
           giveni={R\bibinitperiod}}}%
        {{hash=ba040554cec389bc8b5dc4b20d44218c}{%
           family={Dahl},
           familyi={D\bibinitperiod},
           given={George},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{f9e7751a88512b392878e12f8db7fb50}
      \strng{fullhash}{00ad649e4e26691f9e220e8f8d7e8eb8}
      \strng{bibnamehash}{f9e7751a88512b392878e12f8db7fb50}
      \strng{authorbibnamehash}{f9e7751a88512b392878e12f8db7fb50}
      \strng{authornamehash}{f9e7751a88512b392878e12f8db7fb50}
      \strng{authorfullhash}{00ad649e4e26691f9e220e8f8d7e8eb8}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{bookpagination}{page}
      \field{day}{19}
      \field{journaltitle}{Journal of Machine Learning Research (JMLR)}
      \field{month}{7}
      \field{pagination}{column}
      \field{title}{Measuring the Effects of Data Parallelism on Neural Network Training}
      \field{year}{2019}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1811.03600
      \endverb
    \endentry
    \entry{Breuel2015}{article}{}
      \name{author}{1}{}{%
        {{hash=866060a7e97418d9c531536fe95fe150}{%
           family={Breuel},
           familyi={B\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{866060a7e97418d9c531536fe95fe150}
      \strng{fullhash}{866060a7e97418d9c531536fe95fe150}
      \strng{bibnamehash}{866060a7e97418d9c531536fe95fe150}
      \strng{authorbibnamehash}{866060a7e97418d9c531536fe95fe150}
      \strng{authornamehash}{866060a7e97418d9c531536fe95fe150}
      \strng{authorfullhash}{866060a7e97418d9c531536fe95fe150}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{bookpagination}{page}
      \field{journaltitle}{ArXiv}
      \field{month}{08}
      \field{pagination}{column}
      \field{title}{The Effects of Hyperparameters on SGD Training of Neural Networks}
      \field{year}{2015}
    \endentry
    \entry{Cheng2018}{article}{}
      \name{author}{4}{}{%
        {{hash=f90fa099ce585c3d1d009abdb2ccf07c}{%
           family={Cheng},
           familyi={C\bibinitperiod},
           given={Yu},
           giveni={Y\bibinitperiod}}}%
        {{hash=c12798db59d44b2a8b3efd995f20f9a8}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Duo},
           giveni={D\bibinitperiod}}}%
        {{hash=40506594b8236a27837c19c503917ea4}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Pan},
           giveni={P\bibinitperiod}}}%
        {{hash=228dd687961f3612985beeddec81f72a}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Tao},
           giveni={T\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{7ab8807fe5afc8e9e1605342482eea14}
      \strng{fullhash}{a1ad0fb7c456cb633a7405adea24f7a7}
      \strng{bibnamehash}{7ab8807fe5afc8e9e1605342482eea14}
      \strng{authorbibnamehash}{7ab8807fe5afc8e9e1605342482eea14}
      \strng{authornamehash}{7ab8807fe5afc8e9e1605342482eea14}
      \strng{authorfullhash}{a1ad0fb7c456cb633a7405adea24f7a7}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In recent years, deep neural networks (DNNs) have received increased attention, have been applied to different applications, and achieved dramatic accuracy improvements in many tasks. These works rely on deep networks with millions or even billions of parameters, and the availability of graphics processing units (GPUs) with very high computation capability plays a key role in their success. For example, Krizhevsky et al. [1] achieved breakthrough results in the 2012 ImageNet Challenge using a network containing 60 million parameters with five convolutional layers and three fully connected layers. Usually, it takes two to three days to train the whole model on the ImagetNet data set with an NVIDIA K40 machine. In another example, the top face-verification results from the Labeled Faces in the Wild (LFW) data set were obtained with networks containing hundreds of millions of parameters, using a mix of convolutional, locally connected, and fully connected layers [2], [3]. It is also very time-consuming to train such a model to obtain a reasonable performance. In architectures that only rely on fully connected layers, the number of parameters can grow to billions [4].}
      \field{bookpagination}{page}
      \field{issn}{1558-0792}
      \field{journaltitle}{IEEE Signal Processing Magazine}
      \field{number}{1}
      \field{pagination}{column}
      \field{title}{Model Compression and Acceleration for Deep Neural Networks: The Principles, Progress, and Challenges}
      \field{volume}{35}
      \field{pages}{126\bibrangedash 136}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1109/MSP.2017.2765695
      \endverb
      \keyw{Convolution;Training data;Neural networks;Quantization (signal);Convolutional codes;Computational modeling;Machine learning}
      \warn{\item article entry 'Cheng2018' (references.bib): Invalid format '2018-0-10' of date field 'date' - ignoring}
    \endentry
    \entry{Jacob2018}{inproceedings}{}
      \name{author}{8}{}{%
        {{hash=73f11c7544252601f7e8c5f614c660f2}{%
           family={Jacob},
           familyi={J\bibinitperiod},
           given={Benoit},
           giveni={B\bibinitperiod}}}%
        {{hash=aa58f2ae95227046f2fbba114f0fb625}{%
           family={Kligys},
           familyi={K\bibinitperiod},
           given={Skirmantas},
           giveni={S\bibinitperiod}}}%
        {{hash=31960f03389184b7f052f5b197cc9fdf}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Bo},
           giveni={B\bibinitperiod}}}%
        {{hash=d767e8e4d733bcf728bcdf2c193462f7}{%
           family={Zhu},
           familyi={Z\bibinitperiod},
           given={Menglong},
           giveni={M\bibinitperiod}}}%
        {{hash=436cfd289182e9d6b0a46d79b8e91b6e}{%
           family={Tang},
           familyi={T\bibinitperiod},
           given={Matthew},
           giveni={M\bibinitperiod}}}%
        {{hash=315c4166fc1f7cb66324a7f0d82827cd}{%
           family={Howard},
           familyi={H\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
        {{hash=c707ec5b5997dc408a14a34a8380166c}{%
           family={Adam},
           familyi={A\bibinitperiod},
           given={Hartwig},
           giveni={H\bibinitperiod}}}%
        {{hash=6cbb997a11c6922af719c32863261918}{%
           family={Kalenichenko},
           familyi={K\bibinitperiod},
           given={Dmitry},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{be1e8607db7344f1c8e395537c88d55a}
      \strng{fullhash}{3ad38e526037fc692f33aef3160b0293}
      \strng{bibnamehash}{be1e8607db7344f1c8e395537c88d55a}
      \strng{authorbibnamehash}{be1e8607db7344f1c8e395537c88d55a}
      \strng{authornamehash}{be1e8607db7344f1c8e395537c88d55a}
      \strng{authorfullhash}{3ad38e526037fc692f33aef3160b0293}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based models call for efficient and accurate on-device inference schemes. We propose a quantization scheme that allows inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point inference on commonly available integer-only hardware. We also co-design a training procedure to preserve end-to-end model accuracy post quantization. As a result, the proposed quantization scheme improves the tradeoff between accuracy and on-device latency. The improvements are significant even on MobileNets, a model family known for run-time efficiency, and are demonstrated in ImageNet classification and COCO detection on popular CPUs.}
      \field{booktitle}{2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}
      \field{day}{16}
      \field{eventday}{18}
      \field{eventendday}{23}
      \field{eventendmonth}{5}
      \field{eventendyear}{2018}
      \field{eventmonth}{6}
      \field{eventyear}{2018}
      \field{issn}{2575--7075}
      \field{month}{12}
      \field{title}{Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference}
      \field{venue}{Salt Lake City, UT, USA}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{2704\bibrangedash 2713}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/CVPR.2018.00286
      \endverb
      \keyw{Quantization (signal);Training;Arrays;Computational modeling;Hardware;Neural networks}
    \endentry
    \entry{Yao2024}{article}{}
      \name{author}{5}{}{%
        {{hash=e53d4d0172865556a0b0a5989a7f5c6f}{%
           family={Yao},
           familyi={Y\bibinitperiod},
           given={Zhewei},
           giveni={Z\bibinitperiod}}}%
        {{hash=dbad9062baf2a17217c7c318d929c981}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Xiaoxia},
           giveni={X\bibinitperiod}}}%
        {{hash=0fa86ce76f38abb4293116ef56b787fa}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Cheng},
           giveni={C\bibinitperiod}}}%
        {{hash=fa499f930fb7148318ef8e3502336f63}{%
           family={Youn},
           familyi={Y\bibinitperiod},
           given={Stephen},
           giveni={S\bibinitperiod}}}%
        {{hash=2a1be1c510b34082b3cb846cf8147969}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Yuxiong},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{cc7f86555e19ba4a3122c450a3d5a6db}
      \strng{fullhash}{739f8ccb26689c0316d54c66892963f4}
      \strng{bibnamehash}{cc7f86555e19ba4a3122c450a3d5a6db}
      \strng{authorbibnamehash}{cc7f86555e19ba4a3122c450a3d5a6db}
      \strng{authornamehash}{cc7f86555e19ba4a3122c450a3d5a6db}
      \strng{authorfullhash}{739f8ccb26689c0316d54c66892963f4}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{bookpagination}{page}
      \field{journaltitle}{Proceedings of the AAAI Conference on Artificial Intelligence}
      \field{month}{03}
      \field{pagination}{column}
      \field{title}{Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation}
      \field{volume}{38}
      \field{year}{2024}
      \field{pages}{19377\bibrangedash 19385}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1609/aaai.v38i17.29908
      \endverb
    \endentry
    \entry{Yilmaz2006}{article}{}
      \name{author}{3}{}{%
        {{hash=f7331824d0502c1f76e47c856be64a71}{%
           family={Yilmaz},
           familyi={Y\bibinitperiod},
           given={Alper},
           giveni={A\bibinitperiod}}}%
        {{hash=b4d757f7e1650e89378a800f72de7e60}{%
           family={Javed},
           familyi={J\bibinitperiod},
           given={Omar},
           giveni={O\bibinitperiod}}}%
        {{hash=c75751f493070716733608d20f74e3af}{%
           family={Shah},
           familyi={S\bibinitperiod},
           given={Mubarak},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{62ebdbf58b28eb78bf1300ce02c9c199}
      \strng{fullhash}{62ebdbf58b28eb78bf1300ce02c9c199}
      \strng{bibnamehash}{62ebdbf58b28eb78bf1300ce02c9c199}
      \strng{authorbibnamehash}{62ebdbf58b28eb78bf1300ce02c9c199}
      \strng{authornamehash}{62ebdbf58b28eb78bf1300ce02c9c199}
      \strng{authorfullhash}{62ebdbf58b28eb78bf1300ce02c9c199}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{bookpagination}{page}
      \field{journaltitle}{ACM Comput. Surv.}
      \field{month}{12}
      \field{pagination}{column}
      \field{title}{Object tracking: a survey. ACM Comput Surv}
      \field{volume}{38}
      \field{year}{2006}
      \verb{doi}
      \verb 10.1145/1177352.1177355
      \endverb
    \endentry
    \entry{BotSORT}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=719907501e7c9162dc0806c2851d2b72}{%
           family={Zhao},
           familyi={Z\bibinitperiod},
           given={Junbo},
           giveni={J\bibinitperiod}}}%
        {{hash=ab5450faa81be02a538a8901d5866606}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Jinxiang},
           giveni={J\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {New York, NY, USA}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery}%
      }
      \strng{namehash}{627dd1aad75ba48d6222e2ffa6ce7648}
      \strng{fullhash}{627dd1aad75ba48d6222e2ffa6ce7648}
      \strng{bibnamehash}{627dd1aad75ba48d6222e2ffa6ce7648}
      \strng{authorbibnamehash}{627dd1aad75ba48d6222e2ffa6ce7648}
      \strng{authornamehash}{627dd1aad75ba48d6222e2ffa6ce7648}
      \strng{authorfullhash}{627dd1aad75ba48d6222e2ffa6ce7648}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Iron ladles play a significant role in the industrial intelligence upgrade of steel plants. Accurate recognition and tracking for moving iron ladles can provide the location, speed, and operations information of iron ladles, which are essential for making scheduling plans for steel production. YOLOv8 detection and state-of-the-art (SOTA) tracking algorithms for iron ladles are presented in this paper. The Video data sets with or without shelters are constructed by collecting the actual iron ladles working data. Some own image and video datasets are added to the above datasets by using Segment Anything (SAM) and DarkLabel due to lack of iron ladles data. The YOLOv8 detection model is applied to detect the iron ladles, and three trackers, which are the StrongSORT, OC-SORT, and BOT-SORT, are applied to achieve real-time position information of iron ladles, respectively. In order to improve the identification and tracking accuracy for iron ladles, a genetic algorithm is used to optimize the parameter of the above three trackers. The training and testing results of the above model show that the BOT-SORT tracking model with genetic optimization achieves the highest accuracy that HOTA score is 97.49, both MOTA and IDF1 are 100.}
      \field{booktitle}{Proceedings of the 2024 7th International Conference on Image and Graphics Processing}
      \field{day}{3}
      \field{eventday}{19}
      \field{eventendday}{21}
      \field{eventendmonth}{1}
      \field{eventendyear}{2024}
      \field{eventmonth}{1}
      \field{eventyear}{2024}
      \field{isbn}{9798400716720}
      \field{month}{5}
      \field{series}{ICIGP '24}
      \field{title}{YOLOv8 Detection and Improved BOT-SORT Tracking Algorithm for Iron Ladles}
      \field{venue}{Beijing, China}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{409\bibrangedash 415}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1145/3647649.3647713
      \endverb
      \keyw{BOT-SORT,SAM,YOLOv8,iron ladle tracking,object detection}
    \endentry
    \entry{OCSORT}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=9a844591c023ff26be25e9c6e4ab6072}{%
           family={Cao},
           familyi={C\bibinitperiod},
           given={Jinkun},
           giveni={J\bibinitperiod}}}%
        {{hash=78c3efd65257b83eadb84e53bb73b894}{%
           family={Pang},
           familyi={P\bibinitperiod},
           given={Jiangmiao},
           giveni={J\bibinitperiod}}}%
        {{hash=22194fdd30efd05aeaabbe5bdf2a5f02}{%
           family={Weng},
           familyi={W\bibinitperiod},
           given={Xinshuo},
           giveni={X\bibinitperiod}}}%
        {{hash=b3d42acb4fe90e85875ffce482ee2a52}{%
           family={Khirodkar},
           familyi={K\bibinitperiod},
           given={Rawal},
           giveni={R\bibinitperiod}}}%
        {{hash=4e4b95cc7070ea5a03c13bea9ba5c5e5}{%
           family={Kitani},
           familyi={K\bibinitperiod},
           given={Kris},
           giveni={K\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{e1acf7f6e5f33fe10197b1b4d24238e7}
      \strng{fullhash}{55d70d7bf81b7ed0b646da67b87fc4d3}
      \strng{bibnamehash}{e1acf7f6e5f33fe10197b1b4d24238e7}
      \strng{authorbibnamehash}{e1acf7f6e5f33fe10197b1b4d24238e7}
      \strng{authornamehash}{e1acf7f6e5f33fe10197b1b4d24238e7}
      \strng{authorfullhash}{55d70d7bf81b7ed0b646da67b87fc4d3}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Kalman filter (KF) based methods for multi-object tracking (MOT) make an assumption that objects move linearly. While this assumption is acceptable for very short periods of occlusion, linear estimates of motion for prolonged time can be highly inaccurate. Moreover, when there is no measurement available to update Kalman filter parameters, the standard convention is to trust the priori state estimations for posteriori update. This leads to the accumulation of errors during a period of occlusion. The error causes significant motion direction variance in practice. In this work, we show that a basic Kalman filter can still obtain state-of-the-art tracking performance if proper care is taken to fix the noise accumulated during occlusion. Instead of relying only on the linear state estimate (i.e., estimation-centric approach), we use object observations (i.e., the measurements by object detector) to compute a virtual trajectory over the occlusion period to fix the error accumulation of filter parameters. This allows more time steps to correct errors accumulated during occlusion. We name our method Observation-Centric SORT (OC-SORT). It remains Simple, Online, and Real-Time but improves robustness during occlusion and non-linear motion. Given off-the-shelf detections as input, OC-SORT runs at 700+ FPS on a single CPU. It achieves state-of-the-art on multiple datasets, including MOT17, MOT20, KITTI, head tracking, and especially DanceTrack where the object motion is highly non-linear. The code and models are available at \url{https://github.com/noahcao/OC_SORT}.}
      \field{booktitle}{2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
      \field{day}{22}
      \field{eventday}{17}
      \field{eventendday}{24}
      \field{eventendmonth}{6}
      \field{eventendyear}{2023}
      \field{eventmonth}{6}
      \field{eventyear}{2023}
      \field{isbn}{979-8-3503-0129-8}
      \field{issn}{2575-7075}
      \field{month}{8}
      \field{title}{Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking}
      \field{venue}{Vancouver, BC, Canada}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{9686\bibrangedash 9696}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1109/CVPR52729.2023.00934
      \endverb
      \keyw{Target tracking;Detectors;Nonlinear filters;Real-time systems;Robustness;Trajectory;Pattern recognition;Vision applications and systems}
    \endentry
    \entry{SORT}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=bcbacb46da933474a6ca4e4a69b7bfdc}{%
           family={Bewley},
           familyi={B\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=dbfc282073c3c0a824d475bb3e3dfca4}{%
           family={Ge},
           familyi={G\bibinitperiod},
           given={Zongyuan},
           giveni={Z\bibinitperiod}}}%
        {{hash=fb8eb6ad8616d52c1a1ec4235cd9e9cc}{%
           family={Ott},
           familyi={O\bibinitperiod},
           given={Lionel},
           giveni={L\bibinitperiod}}}%
        {{hash=506f85a881fe35bf140a4b7d845c70d7}{%
           family={Ramos},
           familyi={R\bibinitperiod},
           given={Fabio},
           giveni={F\bibinitperiod}}}%
        {{hash=5c5358fbf9113fcf41a77a4b847721c7}{%
           family={Upcroft},
           familyi={U\bibinitperiod},
           given={Ben},
           giveni={B\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{ce33a25ec7d15f29e5c6b6d4524d072d}
      \strng{fullhash}{2a677301068897509d01d3287d9a96f2}
      \strng{bibnamehash}{ce33a25ec7d15f29e5c6b6d4524d072d}
      \strng{authorbibnamehash}{ce33a25ec7d15f29e5c6b6d4524d072d}
      \strng{authornamehash}{ce33a25ec7d15f29e5c6b6d4524d072d}
      \strng{authorfullhash}{2a677301068897509d01d3287d9a96f2}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications. To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9\%. Despite only using a rudimentary combination of familiar techniques such as the Kalman Filter and Hungarian algorithm for the tracking components, this approach achieves an accuracy comparable to state-of-the-art online trackers. Furthermore, due to the simplicity of our tracking method, the tracker updates at a rate of 260 Hz which is over 20x faster than other state-of-the-art trackers.}
      \field{booktitle}{2016 IEEE International Conference on Image Processing (ICIP)}
      \field{day}{19}
      \field{eventday}{25}
      \field{eventendday}{28}
      \field{eventendmonth}{9}
      \field{eventendyear}{2016}
      \field{eventmonth}{9}
      \field{eventyear}{2016}
      \field{isbn}{978-1-4673-9961-6}
      \field{issn}{2381-8549}
      \field{month}{8}
      \field{title}{Simple online and realtime tracking}
      \field{venue}{Phoenix, AZ, USA}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{3464\bibrangedash 3468}
      \range{pages}{5}
      \verb{doi}
      \verb 10.1109/ICIP.2016.7533003
      \endverb
      \keyw{Target tracking;Detectors;Benchmark testing;Kalman filters;Visualization;Complexity theory;Computer Vision;Multiple Object Tracking;Detection;Data Association}
    \endentry
    \entry{DeepOCSORT}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=b6c219e09b2190e621d0eea306595310}{%
           family={Maggiolino},
           familyi={M\bibinitperiod},
           given={Gerard},
           giveni={G\bibinitperiod}}}%
        {{hash=8131b456e2bf69cf5c5a47b8adf318de}{%
           family={Ahmad},
           familyi={A\bibinitperiod},
           given={Adnan},
           giveni={A\bibinitperiod}}}%
        {{hash=9a844591c023ff26be25e9c6e4ab6072}{%
           family={Cao},
           familyi={C\bibinitperiod},
           given={Jinkun},
           giveni={J\bibinitperiod}}}%
        {{hash=4e4b95cc7070ea5a03c13bea9ba5c5e5}{%
           family={Kitani},
           familyi={K\bibinitperiod},
           given={Kris},
           giveni={K\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{f2f314c9407dfb04a698bd2f49494f3c}
      \strng{fullhash}{b6b475e226179c1621a518b76fd78008}
      \strng{bibnamehash}{f2f314c9407dfb04a698bd2f49494f3c}
      \strng{authorbibnamehash}{f2f314c9407dfb04a698bd2f49494f3c}
      \strng{authornamehash}{f2f314c9407dfb04a698bd2f49494f3c}
      \strng{authorfullhash}{b6b475e226179c1621a518b76fd78008}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Motion-based association for Multi-Object Tracking (MOT) has recently re-achieved prominence with the rise of powerful object detectors. Despite this, little work has been done to incorporate appearance cues beyond simple heuristic models that lack robustness to feature degradation. In this paper, we propose a novel way to leverage objects’ appearances to adaptively integrate appearance matching into existing high-performance motion-based methods. Building upon the pure motion-based method OC-SORT, we achieve 1st place on MOT20 and 2nd place on MOT17 with 63.9 and 64.9 HOTA, respectively. We also achieve 61.3 HOTA on the challenging DanceTrack benchmark as a new state-of-the-art even compared to more heavily-designed methods. The code and models are available at https://github.com/GerardMaggiolino/Deep-OC-SORT.}
      \field{booktitle}{2023 IEEE International Conference on Image Processing (ICIP)}
      \field{day}{11}
      \field{eventday}{8}
      \field{eventendday}{11}
      \field{eventendmonth}{10}
      \field{eventendyear}{2023}
      \field{eventmonth}{10}
      \field{eventyear}{2023}
      \field{isbn}{978-1-7281-9835-4}
      \field{month}{9}
      \field{title}{Deep OC-Sort: Multi-Pedestrian Tracking by Adaptive Re-Identification}
      \field{venue}{Kuala Lumpur, Malaysia}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{3025\bibrangedash 3029}
      \range{pages}{5}
      \verb{doi}
      \verb 10.1109/ICIP49359.2023.10222576
      \endverb
      \keyw{Degradation;Visualization;Adaptation models;Tracking;Image processing;Image matching;Detectors;multi-object tracking;Kalman filter}
    \endentry
    \entry{HybridSORT}{inproceedings}{}
      \name{author}{7}{}{%
        {{hash=09ce54d8ea2af0eabca80af53e8fd68d}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Mingzhan},
           giveni={M\bibinitperiod}}}%
        {{hash=2c31a361f9ba0a2ff370b0bb0552b80c}{%
           family={Han},
           familyi={H\bibinitperiod},
           given={Guangxin},
           giveni={G\bibinitperiod}}}%
        {{hash=7af97bcd62ef25515076d74e281e9194}{%
           family={Yan},
           familyi={Y\bibinitperiod},
           given={Bin},
           giveni={B\bibinitperiod}}}%
        {{hash=e1580150c31e9a7bd594ac0a6d6cf1ee}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Wenhua},
           giveni={W\bibinitperiod}}}%
        {{hash=8de0159641874f8bbbbaf93c65ce365e}{%
           family={Qi},
           familyi={Q\bibinitperiod},
           given={Jinqing},
           giveni={J\bibinitperiod}}}%
        {{hash=15ccd9c47146b2cdcaa35ef973f920fa}{%
           family={Lu},
           familyi={L\bibinitperiod},
           given={Huchuan},
           giveni={H\bibinitperiod}}}%
        {{hash=4695c12fc5e5638d83abfb67ab9d459c}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Dong},
           giveni={D\bibinitperiod}}}%
      }
      \strng{namehash}{c98cb4b0a70046d57f633f921ad51cd9}
      \strng{fullhash}{f131a4a7308b48267db4694a1060da52}
      \strng{bibnamehash}{c98cb4b0a70046d57f633f921ad51cd9}
      \strng{authorbibnamehash}{c98cb4b0a70046d57f633f921ad51cd9}
      \strng{authornamehash}{c98cb4b0a70046d57f633f921ad51cd9}
      \strng{authorfullhash}{f131a4a7308b48267db4694a1060da52}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multi-Object Tracking (MOT) aims to detect and associate all desired objects across frames. Most methods accomplish the task by explicitly or implicitly leveraging strong cues (i.e., spatial and appearance information), which exhibit powerful instance-level discrimination. However, when object occlusion and clustering occur, spatial and appearance information will become ambiguous simultaneously due to the high overlap among objects. In this paper, we demonstrate this long-standing challenge in MOT can be efficiently and effectively resolved by incorporating weak cues to compensate for strong cues. Along with velocity direction, we introduce the confidence and height state as potential weak cues. With superior performance, our method still maintains Simple, Online and Real-Time (SORT) characteristics. Also, our method shows strong generalization for diverse trackers and scenarios in a plug-and-play and training-free manner. Significant and consistent improvements are observed when applying our method to 5 different representative trackers. Further, with both strong and weak cues, our method Hybrid-SORT achieves superior performance on diverse benchmarks, including MOT17, MOT20, and especially DanceTrack where interaction and severe occlusion frequently happen with complex motions. The code and models are available at https://github.com/ymzis69/HybridSORT.}
      \field{booktitle}{Proceedings of the AAAI conference on artificial intelligence}
      \field{day}{25}
      \field{eventday}{20}
      \field{eventendday}{27}
      \field{eventendmonth}{2}
      \field{eventendyear}{2024}
      \field{eventmonth}{2}
      \field{eventyear}{2024}
      \field{month}{3}
      \field{number}{7}
      \field{title}{Hybrid-SORT: Weak Cues Matter for Online Multi-Object Tracking}
      \field{volume}{38}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{6504\bibrangedash 6512}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1609/aaai.v38i7.28471
      \endverb
    \endentry
    \entry{ByteTrack}{inproceedings}{}
      \name{author}{9}{}{%
        {{hash=fe8b7f3911294516753b2d51a6140997}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Yifu},
           giveni={Y\bibinitperiod}}}%
        {{hash=d52526d5deefaf143df9015f52a0d3cb}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Peize},
           giveni={P\bibinitperiod}}}%
        {{hash=aefed70e3b6aeee186d622fd3721feb8}{%
           family={Jiang},
           familyi={J\bibinitperiod},
           given={Yi},
           giveni={Y\bibinitperiod}}}%
        {{hash=64c26663564ad20c85dd9b1f421ca849}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Dongdong},
           giveni={D\bibinitperiod}}}%
        {{hash=34da69027b467871688c97975837692a}{%
           family={Weng},
           familyi={W\bibinitperiod},
           given={Fucheng},
           giveni={F\bibinitperiod}}}%
        {{hash=88f645de868bcc47896cdd2386e82512}{%
           family={Yuan},
           familyi={Y\bibinitperiod},
           given={Zehuan},
           giveni={Z\bibinitperiod}}}%
        {{hash=e85079235b2a8c88ee9360e55b37523a}{%
           family={Luo},
           familyi={L\bibinitperiod},
           given={Ping},
           giveni={P\bibinitperiod}}}%
        {{hash=d2354226a305a694e082d0c47c299a00}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Wenyu},
           giveni={W\bibinitperiod}}}%
        {{hash=1bd616b774bd66d7d77e025ac08c962d}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Xinggang},
           giveni={X\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cham, Switzerland}%
      }
      \list{publisher}{1}{%
        {Springer Nature}%
      }
      \strng{namehash}{8670678d0226ac56309c13a114bfaa24}
      \strng{fullhash}{a54e26a59836c8695ca5ddf0e9f1f17d}
      \strng{bibnamehash}{8670678d0226ac56309c13a114bfaa24}
      \strng{authorbibnamehash}{8670678d0226ac56309c13a114bfaa24}
      \strng{authornamehash}{8670678d0226ac56309c13a114bfaa24}
      \strng{authorfullhash}{a54e26a59836c8695ca5ddf0e9f1f17d}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multi-object tracking (MOT) aims at estimating bounding boxes and identities of objects in videos. Most methods obtain identities by associating detection boxes whose scores are higher than a threshold. The objects with low detection scores, e.g. occluded objects, are simply thrown away, which brings non-negligible true object missing and fragmented trajectories. To solve this problem, we present a simple, effective and generic association method, tracking by associating almost every detection box instead of only the high score ones. For the low score detection boxes, we utilize their similarities with tracklets to recover true objects and filter out the background detections. When applied to 9 different state-of-the-art trackers, our method achieves consistent improvement on IDF1 score ranging from 1 to 10 points. To put forwards the state-of-the-art performance of MOT, we design a simple and strong tracker, named ByteTrack. For the first time, we achieve 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA on the test set of MOT17 with 30 FPS running speed on a single V100 GPU. ByteTrack also achieves state-of-the-art performance on MOT20, HiEve and BDD100K tracking benchmarks. The source code, pre-trained models with deploy versions and tutorials of applying to other trackers are released at https://github.com/ifzhang/ByteTrack.}
      \field{booktitle}{Computer Vision -- ECCV 2022}
      \field{day}{23}
      \field{eventday}{23}
      \field{eventendday}{27}
      \field{eventendmonth}{10}
      \field{eventendyear}{2022}
      \field{eventmonth}{10}
      \field{eventyear}{2022}
      \field{month}{10}
      \field{title}{ByteTrack: Multi-object Tracking by Associating Every Detection Box}
      \field{venue}{Tel Aviv, Israel}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{1\bibrangedash 21}
      \range{pages}{21}
      \verb{doi}
      \verb 10.1007/978-3-031-20047-2_1
      \endverb
    \endentry
    \entry{TensorFlow}{online}{}
      \name{author}{1}{}{%
        {{hash=2fa89da095cc52494fbb2066d3f2bfc7}{%
           family={{TensorFlow Developers}},
           familyi={T\bibinitperiod}}}%
      }
      \strng{namehash}{2fa89da095cc52494fbb2066d3f2bfc7}
      \strng{fullhash}{2fa89da095cc52494fbb2066d3f2bfc7}
      \strng{bibnamehash}{2fa89da095cc52494fbb2066d3f2bfc7}
      \strng{authorbibnamehash}{2fa89da095cc52494fbb2066d3f2bfc7}
      \strng{authornamehash}{2fa89da095cc52494fbb2066d3f2bfc7}
      \strng{authorfullhash}{2fa89da095cc52494fbb2066d3f2bfc7}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{TensorFlow}
      \field{type}{Software}
      \verb{doi}
      \verb 10.5281/zenodo.4724125
      \endverb
    \endentry
    \entry{TensorFlow-whitepaper}{report}{}
      \name{author}{41}{}{%
        {{hash=9a04ae935da573a7aa9c83afdf2fd845}{%
           family={Abadi},
           familyi={A\bibinitperiod},
           given={Martín},
           giveni={M\bibinitperiod}}}%
        {{hash=37e1df772f97a2e492755e2006294387}{%
           family={Agarwal},
           familyi={A\bibinitperiod},
           given={Ashish},
           giveni={A\bibinitperiod}}}%
        {{hash=08cf514c08137f94d73b590060797f5b}{%
           family={Barham},
           familyi={B\bibinitperiod},
           given={Paul},
           giveni={P\bibinitperiod}}}%
        {{hash=a79c0c4b084471b2c8e0e9a38b4cb2b7}{%
           family={Brevdo},
           familyi={B\bibinitperiod},
           given={Eugene},
           giveni={E\bibinitperiod}}}%
        {{hash=c0d10aaf985cebf8d0497e1828f9313f}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Zhifeng},
           giveni={Z\bibinitperiod}}}%
        {{hash=6f712ddfd730736ee54fdc87a4abf7a2}{%
           family={Citro},
           familyi={C\bibinitperiod},
           given={Craig},
           giveni={C\bibinitperiod}}}%
        {{hash=3863b5ab9ace148f5d3c06eb6e153d4a}{%
           family={S.\bibnamedelimi Corrado},
           familyi={S\bibinitperiod\bibinitdelim C\bibinitperiod},
           given={Greg},
           giveni={G\bibinitperiod}}}%
        {{hash=a4c859221cc41e2db6d63b486f955ba2}{%
           family={Davis},
           familyi={D\bibinitperiod},
           given={Andy},
           giveni={A\bibinitperiod}}}%
        {{hash=4aecfb0cc2e1e3b7899129fa2a94e2b8}{%
           family={Dean},
           familyi={D\bibinitperiod},
           given={Jeffrey},
           giveni={J\bibinitperiod}}}%
        {{hash=bd7f88635b51bc5abb3d5a2cab967724}{%
           family={Devin},
           familyi={D\bibinitperiod},
           given={Matthieu},
           giveni={M\bibinitperiod}}}%
        {{hash=193bcec5240237591ad8fb697869f013}{%
           family={Ghemawat},
           familyi={G\bibinitperiod},
           given={Sanjay},
           giveni={S\bibinitperiod}}}%
        {{hash=5d2585c11210cf1d4512e6e0a03ec315}{%
           family={Goodfellow},
           familyi={G\bibinitperiod},
           given={Ian},
           giveni={I\bibinitperiod}}}%
        {{hash=a9385c3940c7e3dc9dfad01621d190d3}{%
           family={Harp},
           familyi={H\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
        {{hash=edc6964c6549e1839ca94d3d61e03f76}{%
           family={Irving},
           familyi={I\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod}}}%
        {{hash=3b62e7eedeec9f7a7f7f3c3ec88637f1}{%
           family={Isard},
           familyi={I\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=9fce03efe6b3331a1b93ed2e7c0da9d5}{%
           family={Jia},
           familyi={J\bibinitperiod},
           given={Yangqing},
           giveni={Y\bibinitperiod}}}%
        {{hash=b243d8e26a729c55dd5af5f97763dded}{%
           family={Jozefowicz},
           familyi={J\bibinitperiod},
           given={Rafal},
           giveni={R\bibinitperiod}}}%
        {{hash=f2bc899b1160163417da7bf510f15d33}{%
           family={Kaiser},
           familyi={K\bibinitperiod},
           given={Lukasz},
           giveni={L\bibinitperiod}}}%
        {{hash=73fc2bbbbbe06838174fc0490ee66f67}{%
           family={Kudlur},
           familyi={K\bibinitperiod},
           given={Manjunath},
           giveni={M\bibinitperiod}}}%
        {{hash=a1a07ba9db1e853a9dd9623d81e8f26a}{%
           family={Levenberg},
           familyi={L\bibinitperiod},
           given={Josh},
           giveni={J\bibinitperiod}}}%
        {{hash=b9646cb975a64f853702acf062460ddf}{%
           family={Mané},
           familyi={M\bibinitperiod},
           given={Dandelion},
           giveni={D\bibinitperiod}}}%
        {{hash=f34904ef0a6e0864175e4c446f9fcf14}{%
           family={Monga},
           familyi={M\bibinitperiod},
           given={Rajat},
           giveni={R\bibinitperiod}}}%
        {{hash=1f07f887fdbaea3ca8281745735256d1}{%
           family={Moore},
           familyi={M\bibinitperiod},
           given={Sherry},
           giveni={S\bibinitperiod}}}%
        {{hash=d83fd9fc0c2a25f04b7525cfe4ac3ab8}{%
           family={Murray},
           familyi={M\bibinitperiod},
           given={Derek},
           giveni={D\bibinitperiod}}}%
        {{hash=78be47e08348476a254b1217fd9041ca}{%
           family={Olah},
           familyi={O\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod}}}%
        {{hash=29ea22df63f174ac629e9ef100b40484}{%
           family={Schuster},
           familyi={S\bibinitperiod},
           given={Mike},
           giveni={M\bibinitperiod}}}%
        {{hash=8f128e70084608a2c29c497ebd794f87}{%
           family={Shlens},
           familyi={S\bibinitperiod},
           given={Jonathon},
           giveni={J\bibinitperiod}}}%
        {{hash=0a0b028c6b85c46f368317d0c5bfe3a0}{%
           family={Steiner},
           familyi={S\bibinitperiod},
           given={Benoit},
           giveni={B\bibinitperiod}}}%
        {{hash=8d569d1d5b8b5a7836017a98b430f959}{%
           family={Sutskever},
           familyi={S\bibinitperiod},
           given={Ilya},
           giveni={I\bibinitperiod}}}%
        {{hash=1754a40f9d600dea756c1dd1047ce170}{%
           family={Talwar},
           familyi={T\bibinitperiod},
           given={Kunal},
           giveni={K\bibinitperiod}}}%
        {{hash=c7360a7ebeba8420b8c7e0eeb0cb81ac}{%
           family={Tucker},
           familyi={T\bibinitperiod},
           given={Paul},
           giveni={P\bibinitperiod}}}%
        {{hash=8051922e7bd286f884bfbd1023ef62f5}{%
           family={Vanhoucke},
           familyi={V\bibinitperiod},
           given={Vincent},
           giveni={V\bibinitperiod}}}%
        {{hash=bab2011f2b9e7af47ab45308470b9faf}{%
           family={Vasudevan},
           familyi={V\bibinitperiod},
           given={Vijay},
           giveni={V\bibinitperiod}}}%
        {{hash=7fd60cad5301e91d54be24e6d15594a8}{%
           family={Viégas},
           familyi={V\bibinitperiod},
           given={Fernanda},
           giveni={F\bibinitperiod}}}%
        {{hash=494b568c5dc85ba8f3f409635f9c5f25}{%
           family={Vinyals},
           familyi={V\bibinitperiod},
           given={Oriol},
           giveni={O\bibinitperiod}}}%
        {{hash=cfc28a19549f0d8ff117cf79be90d213}{%
           family={Warden},
           familyi={W\bibinitperiod},
           given={Pete},
           giveni={P\bibinitperiod}}}%
        {{hash=c66133f96104e3c0dcd2f73e11469ab0}{%
           family={Wattenberg},
           familyi={W\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=8f71c897f89022a26511a6fbbd6108b6}{%
           family={Wicke},
           familyi={W\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=b8e2e9f67019f464cefe5db1c822ffd9}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Yuan},
           giveni={Y\bibinitperiod}}}%
        {{hash=fdfa73146952ceef0abc202cc397781c}{%
           family={Zheng},
           familyi={Z\bibinitperiod},
           given={Xiaoqiang},
           giveni={X\bibinitperiod}}}%
        {{hash=5b9e27c341713586dafd7304f6db987d}{%
           family={Research},
           familyi={R\bibinitperiod},
           given={Google},
           giveni={G\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Google}%
      }
      \strng{namehash}{24b1e0a436b566c814359b41f12b3ce3}
      \strng{fullhash}{a954cc941b1f09763b511ef5a3879984}
      \strng{bibnamehash}{24b1e0a436b566c814359b41f12b3ce3}
      \strng{authorbibnamehash}{24b1e0a436b566c814359b41f12b3ce3}
      \strng{authornamehash}{24b1e0a436b566c814359b41f12b3ce3}
      \strng{authorfullhash}{a954cc941b1f09763b511ef5a3879984}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{TensorFlow is an interface for expressing machine learning algorithms and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.}
      \field{day}{9}
      \field{month}{11}
      \field{title}{{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems}
      \field{type}{Whitepaper}
      \field{year}{2015}
      \field{dateera}{ce}
      \verb{urlraw}
      \verb https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45166.pdf
      \endverb
      \verb{url}
      \verb https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45166.pdf
      \endverb
    \endentry
    \entry{Pytorch}{online}{}
      \name{author}{2}{}{%
        {{hash=145cb942e2c6e0e141b825eb8cebbdb1}{%
           family={Bergmann},
           familyi={B\bibinitperiod},
           given={Dave},
           giveni={D\bibinitperiod}}}%
        {{hash=3ad6590e5b4a21e93d302b7e0bf0d768}{%
           family={Stryker},
           familyi={S\bibinitperiod},
           given={Cole},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{3ea8c48ca06aa76c67d5af9f1419310f}
      \strng{fullhash}{3ea8c48ca06aa76c67d5af9f1419310f}
      \strng{bibnamehash}{3ea8c48ca06aa76c67d5af9f1419310f}
      \strng{authorbibnamehash}{3ea8c48ca06aa76c67d5af9f1419310f}
      \strng{authornamehash}{3ea8c48ca06aa76c67d5af9f1419310f}
      \strng{authorfullhash}{3ea8c48ca06aa76c67d5af9f1419310f}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{What is PyTorch?}
      \field{urlday}{1}
      \field{urlmonth}{7}
      \field{urlyear}{2025}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://www.ibm.com/think/topics/pytorch
      \endverb
      \verb{url}
      \verb https://www.ibm.com/think/topics/pytorch
      \endverb
    \endentry
    \entry{Pytorch2}{inproceedings}{}
      \name{author}{49}{}{%
        {{hash=e78fa9bb024897b05487b4d0e3aab461}{%
           family={Ansel},
           familyi={A\bibinitperiod},
           given={Jason},
           giveni={J\bibinitperiod}}}%
        {{hash=b9e701339e56fd0b171145b08288a1b7}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Edward},
           giveni={E\bibinitperiod}}}%
        {{hash=339d072cae7a701881476cb4e4958adc}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Horace},
           giveni={H\bibinitperiod}}}%
        {{hash=6e45f49ec618e619efad90c8e8a61f0c}{%
           family={Gimelshein},
           familyi={G\bibinitperiod},
           given={Natalia},
           giveni={N\bibinitperiod}}}%
        {{hash=aa93352ce718dfc2ab5fa1cd8e50acdd}{%
           family={Jain},
           familyi={J\bibinitperiod},
           given={Animesh},
           giveni={A\bibinitperiod}}}%
        {{hash=73f61d2d529e845db2937d2141b8831b}{%
           family={Voznesensky},
           familyi={V\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=f78289a128eb158da2d42b485a9e8027}{%
           family={Bao},
           familyi={B\bibinitperiod},
           given={Bin},
           giveni={B\bibinitperiod}}}%
        {{hash=5c5131d4f5264c3c5c9b3999661099d4}{%
           family={Bell},
           familyi={B\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
        {{hash=13798c299036ee606ca381b7aca40d33}{%
           family={Berard},
           familyi={B\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=09a667aa6a26526bfcccb2676a494e55}{%
           family={Burovski},
           familyi={B\bibinitperiod},
           given={Evgeni},
           giveni={E\bibinitperiod}}}%
        {{hash=93fe4ee5367badd822e112322753c865}{%
           family={Chauhan},
           familyi={C\bibinitperiod},
           given={Geeta},
           giveni={G\bibinitperiod}}}%
        {{hash=4f8244207606507d74292cee3c9d25d8}{%
           family={Chourdia},
           familyi={C\bibinitperiod},
           given={Anjali},
           giveni={A\bibinitperiod}}}%
        {{hash=4766072c6524173c5b352a44b4e81ea2}{%
           family={Constable},
           familyi={C\bibinitperiod},
           given={Will},
           giveni={W\bibinitperiod}}}%
        {{hash=954cf7680b6ce14813973eccdca3c4bc}{%
           family={Desmaison},
           familyi={D\bibinitperiod},
           given={Alban},
           giveni={A\bibinitperiod}}}%
        {{hash=3f9535be511fd2fa346093e63b8e61a0}{%
           family={DeVito},
           familyi={D\bibinitperiod},
           given={Zachary},
           giveni={Z\bibinitperiod}}}%
        {{hash=d120a738223a9fb5012c14b630afaaa3}{%
           family={Ellison},
           familyi={E\bibinitperiod},
           given={Elias},
           giveni={E\bibinitperiod}}}%
        {{hash=3f29cb06fc723a0c40d6ad5e7607b422}{%
           family={Feng},
           familyi={F\bibinitperiod},
           given={Will},
           giveni={W\bibinitperiod}}}%
        {{hash=a039196851c8aa0e865ed50ca42da6ca}{%
           family={Gong},
           familyi={G\bibinitperiod},
           given={Jiong},
           giveni={J\bibinitperiod}}}%
        {{hash=d41895e4fca09a2ffb0c95190f6d6aca}{%
           family={Gschwind},
           familyi={G\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=059736a3b347ddab8a72c8935276b5b8}{%
           family={Hirsh},
           familyi={H\bibinitperiod},
           given={Brian},
           giveni={B\bibinitperiod}}}%
        {{hash=03203988e8dec6b1f770d21267d1a5c2}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Sherlock},
           giveni={S\bibinitperiod}}}%
        {{hash=f64c72869ade69fe532d502fa23aa717}{%
           family={Kalambarkar},
           familyi={K\bibinitperiod},
           given={Kshiteej},
           giveni={K\bibinitperiod}}}%
        {{hash=faae2596b01f225f8d35fbc13429b809}{%
           family={Kirsch},
           familyi={K\bibinitperiod},
           given={Laurent},
           giveni={L\bibinitperiod}}}%
        {{hash=72df5f29945de824df5bb27806e51ff3}{%
           family={Lazos},
           familyi={L\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=a4623402ada65204ba5cfe26c7d6ea19}{%
           family={Lezcano},
           familyi={L\bibinitperiod},
           given={Mario},
           giveni={M\bibinitperiod}}}%
        {{hash=e6eb092eac01d09fca25a6f3863114be}{%
           family={Liang},
           familyi={L\bibinitperiod},
           given={Yanbo},
           giveni={Y\bibinitperiod}}}%
        {{hash=1e883e862ce7bc886d867171f106b465}{%
           family={Liang},
           familyi={L\bibinitperiod},
           given={Jason},
           giveni={J\bibinitperiod}}}%
        {{hash=4eadf32831509d81ab5104f8d3d113da}{%
           family={Lu},
           familyi={L\bibinitperiod},
           given={Yinghai},
           giveni={Y\bibinitperiod}}}%
        {{hash=621e3d5b442b1067863530ab2015cdd4}{%
           family={Luk},
           familyi={L\bibinitperiod},
           given={CK},
           giveni={C\bibinitperiod}}}%
        {{hash=7cc4922468f3d5b85a3967299da0a4aa}{%
           family={Maher},
           familyi={M\bibinitperiod},
           given={Bert},
           giveni={B\bibinitperiod}}}%
        {{hash=4155ee12962c958909411f6438bed201}{%
           family={Pan},
           familyi={P\bibinitperiod},
           given={Yunjie},
           giveni={Y\bibinitperiod}}}%
        {{hash=23ae0370ddd373ba5ecfa35f0751f0e8}{%
           family={Puhrsch},
           familyi={P\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod}}}%
        {{hash=525237a0a8567b38319596dc54130c17}{%
           family={Reso},
           familyi={R\bibinitperiod},
           given={Matthias},
           giveni={M\bibinitperiod}}}%
        {{hash=eb82b008b83c88d432b4e5167f3acf06}{%
           family={Saroufim},
           familyi={S\bibinitperiod},
           given={Mark},
           giveni={M\bibinitperiod}}}%
        {{hash=1d05f589be91663bdcbd3b2ec1ad95cc}{%
           family={Siraichi},
           familyi={S\bibinitperiod},
           given={Marcos\bibnamedelima Yukio},
           giveni={M\bibinitperiod\bibinitdelim Y\bibinitperiod}}}%
        {{hash=8bf7efa7375ec8441db1315cb6341021}{%
           family={Suk},
           familyi={S\bibinitperiod},
           given={Helen},
           giveni={H\bibinitperiod}}}%
        {{hash=7b35fb6ae66b844d2fe7bfe0ba98886b}{%
           family={Suo},
           familyi={S\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=8593192db45495d7768162b3630207b1}{%
           family={Tillet},
           familyi={T\bibinitperiod},
           given={Phil},
           giveni={P\bibinitperiod}}}%
        {{hash=f5e6228a84a34d66724cf3fe7dd0fef5}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Eikan},
           giveni={E\bibinitperiod}}}%
        {{hash=cd26424dac955ebe267f1a2f0325d9f3}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Xiaodong},
           giveni={X\bibinitperiod}}}%
        {{hash=2dfd5b92a759b60ae50920845c9a5c84}{%
           family={Wen},
           familyi={W\bibinitperiod},
           given={William},
           giveni={W\bibinitperiod}}}%
        {{hash=a7f60c4b0ae0a2345562857e0db066b0}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Shunting},
           giveni={S\bibinitperiod}}}%
        {{hash=f79215132dad5094a6a084f1445d13ef}{%
           family={Zhao},
           familyi={Z\bibinitperiod},
           given={Xu},
           giveni={X\bibinitperiod}}}%
        {{hash=520990ed6ffb6d01df7e906edcaf3e92}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Keren},
           giveni={K\bibinitperiod}}}%
        {{hash=25ddb7ef91f9d2807c51ab020400ca44}{%
           family={Zou},
           familyi={Z\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
        {{hash=95cced5af9d65baaa27928c8874c1cfe}{%
           family={Mathews},
           familyi={M\bibinitperiod},
           given={Ajit},
           giveni={A\bibinitperiod}}}%
        {{hash=f897ed422c34d95af2e22778dfc2607e}{%
           family={Chanan},
           familyi={C\bibinitperiod},
           given={Gregory},
           giveni={G\bibinitperiod}}}%
        {{hash=8ee2464c666949638e7727e1d1d00985}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Peng},
           giveni={P\bibinitperiod}}}%
        {{hash=8ef51a0906e47d2b4472c4e714ed598f}{%
           family={Chintala},
           familyi={C\bibinitperiod},
           given={Soumith},
           giveni={S\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {New York, NY, USA}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery}%
      }
      \strng{namehash}{a3808305ad2695511c53901a6b9071d0}
      \strng{fullhash}{7f09690a82bc14e3737ee59a653ee52c}
      \strng{bibnamehash}{a3808305ad2695511c53901a6b9071d0}
      \strng{authorbibnamehash}{a3808305ad2695511c53901a6b9071d0}
      \strng{authornamehash}{a3808305ad2695511c53901a6b9071d0}
      \strng{authorfullhash}{7f09690a82bc14e3737ee59a653ee52c}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper introduces two extensions to the popular PyTorch machine learning framework, TorchDynamo and TorchInductor, which implement the torch.compile feature released in PyTorch 2. TorchDynamo is a Python-level just-in-time (JIT) compiler that enables graph compilation in PyTorch programs without sacrificing the flexibility of Python. It achieves this by dynamically modifying Python bytecode before execution and extracting sequences of PyTorch operations into an FX graph, which is then JIT compiled using one of many extensible backends. TorchInductor is the default compiler backend for TorchDynamo, which translates PyTorch programs into OpenAI's Triton for GPUs and C++ for CPUs. Results show that TorchDynamo is able to capture graphs more robustly than prior approaches while adding minimal overhead, and TorchInductor is able to provide a 2.27\texttimes{} inference and 1.41\texttimes{} training geometric mean speedup on an NVIDIA A100 GPU across 180+ real-world models, which outperforms six other compilers. These extensions provide a new way to apply optimizations through compilers in eager mode frameworks like PyTorch.}
      \field{booktitle}{29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS ’24)}
      \field{eventday}{27}
      \field{eventendday}{1}
      \field{eventendmonth}{5}
      \field{eventendyear}{2024}
      \field{eventmonth}{4}
      \field{eventyear}{2024}
      \field{isbn}{9798400703850}
      \field{month}{4}
      \field{title}{PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation}
      \field{venue}{La Jolla, CA, USA}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{929\bibrangedash 947}
      \range{pages}{19}
      \verb{doi}
      \verb 10.1145/3620665.3640366
      \endverb
    \endentry
    \entry{Supergradients}{online}{}
      \name{author}{13}{}{%
        {{hash=ce69b2bfac1cf1cfc2672a0dcfb6636d}{%
           family={Aharon},
           familyi={A\bibinitperiod},
           given={Shay},
           giveni={S\bibinitperiod}}}%
        {{hash=8dd570d96c22a6acb26f75b462bfb619}{%
           family={{Louis-Dupont}},
           familyi={L\bibinitperiod}}}%
        {{hash=c28f5831c6b5ccab70edf20bab016547}{%
           family={{Ofri Masad}},
           familyi={O\bibinitperiod}}}%
        {{hash=281499cbb97e2063fd1d89049c552aed}{%
           family={Yurkova},
           familyi={Y\bibinitperiod},
           given={Kate},
           giveni={K\bibinitperiod}}}%
        {{hash=5f5d796d791ce92f48f8ef31881e695c}{%
           family={{Lotem Fridman}},
           familyi={L\bibinitperiod}}}%
        {{hash=47150dca66aeaf8665a9121b328165f1}{%
           family={{Lkdci}},
           familyi={L\bibinitperiod}}}%
        {{hash=964995eaf98a368c4e89dc0ec962a16b}{%
           family={Khvedchenya},
           familyi={K\bibinitperiod},
           given={Eugene},
           giveni={E\bibinitperiod}}}%
        {{hash=47102dfb9ce250714857c5ed991ac554}{%
           family={Rubin},
           familyi={R\bibinitperiod},
           given={Ran},
           giveni={R\bibinitperiod}}}%
        {{hash=3637662e09910e23713cbb7a478739c3}{%
           family={Bagrov},
           familyi={B\bibinitperiod},
           given={Natan},
           giveni={N\bibinitperiod}}}%
        {{hash=97df0bee3d24f6d1a0ac876ed1eace44}{%
           family={Tymchenko},
           familyi={T\bibinitperiod},
           given={Borys},
           giveni={B\bibinitperiod}}}%
        {{hash=09b4a847f883293b9119e713bc30b327}{%
           family={Keren},
           familyi={K\bibinitperiod},
           given={Tomer},
           giveni={T\bibinitperiod}}}%
        {{hash=ac08819706b921b49eec7f6ebf69cb67}{%
           family={Zhilko},
           familyi={Z\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
        {{hash=31dddf99463305b3c04d23ac7eac01fa}{%
           family={{Eran-Deci}},
           familyi={E\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {GitHub}%
      }
      \strng{namehash}{f77c1d5931ac91cc5d51d6a146b86421}
      \strng{fullhash}{40bfb3771e1ff660d7a81328e06af234}
      \strng{bibnamehash}{f77c1d5931ac91cc5d51d6a146b86421}
      \strng{authorbibnamehash}{f77c1d5931ac91cc5d51d6a146b86421}
      \strng{authornamehash}{f77c1d5931ac91cc5d51d6a146b86421}
      \strng{authorfullhash}{40bfb3771e1ff660d7a81328e06af234}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{GitHub repository}
      \field{title}{Super-Gradients}
      \field{type}{Software}
      \field{year}{2021}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.5281/ZENODO.7789328
      \endverb
    \endentry
    \entry{Ultralytics}{online}{}
      \name{author}{3}{}{%
        {{hash=c5c5e5cd7f4f477f617289b32613deb0}{%
           family={Jocher},
           familyi={J\bibinitperiod},
           given={Glenn},
           giveni={G\bibinitperiod}}}%
        {{hash=b8739ebbf3f871d471b6720a02ed541f}{%
           family={Qiu},
           familyi={Q\bibinitperiod},
           given={Jing},
           giveni={J\bibinitperiod}}}%
        {{hash=ec281637802454e8df4de72f3b1ee401}{%
           family={Chaurasia},
           familyi={C\bibinitperiod},
           given={Ayush},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{1d270278cb86ffcfd6356ed5db820a38}
      \strng{fullhash}{1d270278cb86ffcfd6356ed5db820a38}
      \strng{bibnamehash}{1d270278cb86ffcfd6356ed5db820a38}
      \strng{authorbibnamehash}{1d270278cb86ffcfd6356ed5db820a38}
      \strng{authornamehash}{1d270278cb86ffcfd6356ed5db820a38}
      \strng{authorfullhash}{1d270278cb86ffcfd6356ed5db820a38}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{month}{1}
      \field{title}{{Ultralytics YOLO}}
      \field{version}{8.0.0}
      \field{year}{2023}
      \field{dateera}{ce}
      \verb{urlraw}
      \verb https://github.com/ultralytics/ultralytics
      \endverb
      \verb{url}
      \verb https://github.com/ultralytics/ultralytics
      \endverb
    \endentry
    \entry{Roboflow}{online}{}
      \name{author}{4}{}{%
        {{hash=4ba836e29263899981d9e72e613b6ec5}{%
           family={Dwyer},
           familyi={D\bibinitperiod},
           given={B.},
           giveni={B\bibinitperiod}}}%
        {{hash=031f8ef4b5f282b9d98ff600a372a402}{%
           family={Nelson},
           familyi={N\bibinitperiod},
           given={J.},
           giveni={J\bibinitperiod}}}%
        {{hash=3ccca539db58a751783cdf45cdf90bd4}{%
           family={Hansen},
           familyi={H\bibinitperiod},
           given={T.},
           giveni={T\bibinitperiod}}}%
        {{hash=38026ba4ee6b102b47bf3ffb41325c8f}{%
           family={al.},
           familyi={a\bibinitperiod},
           prefix={et},
           prefixi={e\bibinitperiod}}}%
      }
      \strng{namehash}{37aa7314dd8e793f822519cc0e2d10f7}
      \strng{fullhash}{dfa51cae6b32fd987fef819a38be019b}
      \strng{bibnamehash}{37aa7314dd8e793f822519cc0e2d10f7}
      \strng{authorbibnamehash}{37aa7314dd8e793f822519cc0e2d10f7}
      \strng{authornamehash}{37aa7314dd8e793f822519cc0e2d10f7}
      \strng{authorfullhash}{dfa51cae6b32fd987fef819a38be019b}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{note}{Computer vision}
      \field{title}{Roboflow (Version 1.0)}
      \field{type}{Software}
      \field{year}{2025}
      \field{dateera}{ce}
      \verb{urlraw}
      \verb https://roboflow.com
      \endverb
      \verb{url}
      \verb https://roboflow.com
      \endverb
    \endentry
    \entry{CVAT}{online}{}
      \name{author}{1}{}{%
        {{hash=7f1cdc01509c46bab36db24d8f8fb92f}{%
           family={Corporation},
           familyi={C\bibinitperiod},
           given={CVAT.ai},
           giveni={C\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Zenodo}%
      }
      \strng{namehash}{7f1cdc01509c46bab36db24d8f8fb92f}
      \strng{fullhash}{7f1cdc01509c46bab36db24d8f8fb92f}
      \strng{bibnamehash}{7f1cdc01509c46bab36db24d8f8fb92f}
      \strng{authorbibnamehash}{7f1cdc01509c46bab36db24d8f8fb92f}
      \strng{authornamehash}{7f1cdc01509c46bab36db24d8f8fb92f}
      \strng{authorfullhash}{7f1cdc01509c46bab36db24d8f8fb92f}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{month}{6}
      \field{title}{Computer Vision Annotation Tool (CVAT)}
      \field{version}{v2.16.1}
      \field{year}{2024}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.5281/zenodo.12771595
      \endverb
    \endentry
    \entry{V7Labs}{online}{}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labeltitlesource}{title}
      \field{title}{{V7 Labs - Darwin}}
      \field{type}{Computer Vision Tools}
      \verb{urlraw}
      \verb https://www.v7labs.com/darwin
      \endverb
      \verb{url}
      \verb https://www.v7labs.com/darwin
      \endverb
    \endentry
    \entry{Labellerr}{online}{}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labeltitlesource}{title}
      \field{title}{Labellerr}
      \field{type}{Computer Vision Tools}
      \verb{urlraw}
      \verb https://www.labellerr.com
      \endverb
      \verb{url}
      \verb https://www.labellerr.com
      \endverb
    \endentry
    \entry{Supervisely}{online}{}
      \name{author}{1}{}{%
        {{hash=c6ef583c59689fb6846359d5be04ddf6}{%
           family={Supervisely},
           familyi={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Supervisely}%
      }
      \strng{namehash}{c6ef583c59689fb6846359d5be04ddf6}
      \strng{fullhash}{c6ef583c59689fb6846359d5be04ddf6}
      \strng{bibnamehash}{c6ef583c59689fb6846359d5be04ddf6}
      \strng{authorbibnamehash}{c6ef583c59689fb6846359d5be04ddf6}
      \strng{authornamehash}{c6ef583c59689fb6846359d5be04ddf6}
      \strng{authorfullhash}{c6ef583c59689fb6846359d5be04ddf6}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Supervisely Ecosystem}
      \field{month}{7}
      \field{title}{Supervisely Computer Vision platform}
      \field{type}{Computer Vision Tools}
      \field{year}{2023}
      \field{dateera}{ce}
      \verb{urlraw}
      \verb https://supervisely.com
      \endverb
      \verb{url}
      \verb https://supervisely.com
      \endverb
    \endentry
    \entry{SAM2}{article}{}
      \name{author}{18}{}{%
        {{hash=29e72fba9e2b3c5537682a7be7bc14e4}{%
           family={Ravi},
           familyi={R\bibinitperiod},
           given={Nikhila},
           giveni={N\bibinitperiod}}}%
        {{hash=e8045f6960cde077294b85eff849d8e1}{%
           family={Gabeur},
           familyi={G\bibinitperiod},
           given={Valentin},
           giveni={V\bibinitperiod}}}%
        {{hash=2746f6639cf6eeca31796efb752d3143}{%
           family={Hu},
           familyi={H\bibinitperiod},
           given={Yuan-Ting},
           giveni={Y\bibinithyphendelim T\bibinitperiod}}}%
        {{hash=9f2c3c903f2037997ff0d834aca07fe8}{%
           family={Hu},
           familyi={H\bibinitperiod},
           given={Ronghang},
           giveni={R\bibinitperiod}}}%
        {{hash=83e13f54ab022aa655b26c9010b1d40a}{%
           family={Ryali},
           familyi={R\bibinitperiod},
           given={Chaitanya},
           giveni={C\bibinitperiod}}}%
        {{hash=5bd4a91e09b499d15e82bc929acf1e34}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Tengyu},
           giveni={T\bibinitperiod}}}%
        {{hash=ef266bf11aefb1b744d9c035c88f8c07}{%
           family={Khedr},
           familyi={K\bibinitperiod},
           given={Haitham},
           giveni={H\bibinitperiod}}}%
        {{hash=b21f962ef4fde8d206b12240b5fc4eb9}{%
           family={Rädle},
           familyi={R\bibinitperiod},
           given={Roman},
           giveni={R\bibinitperiod}}}%
        {{hash=026df7884d750eef80a162ac4d80a24d}{%
           family={Rolland},
           familyi={R\bibinitperiod},
           given={Chloe},
           giveni={C\bibinitperiod}}}%
        {{hash=34706d6f5a4acd2ebc4ccdc87ff94abf}{%
           family={Gustafson},
           familyi={G\bibinitperiod},
           given={Laura},
           giveni={L\bibinitperiod}}}%
        {{hash=190ac13c22d694c31877cdf31c0db652}{%
           family={Mintun},
           familyi={M\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod}}}%
        {{hash=6156d68b681040705ec43c7902c9f197}{%
           family={Pan},
           familyi={P\bibinitperiod},
           given={Junting},
           giveni={J\bibinitperiod}}}%
        {{hash=31b49c6a8ac57b864a8f786dbd352066}{%
           family={Alwala},
           familyi={A\bibinitperiod},
           given={Kalyan\bibnamedelima Vasudev},
           giveni={K\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
        {{hash=0cb223da593b5653feac6f6eabcbe82d}{%
           family={Carion},
           familyi={C\bibinitperiod},
           given={Nicolas},
           giveni={N\bibinitperiod}}}%
        {{hash=0616e6c3dae5ba93fee86f3d88778be4}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Chao-Yuan},
           giveni={C\bibinithyphendelim Y\bibinitperiod}}}%
        {{hash=bd5dadbe57bedc5957c19a3154c4d424}{%
           family={Girshick},
           familyi={G\bibinitperiod},
           given={Ross},
           giveni={R\bibinitperiod}}}%
        {{hash=ecd149fdcb3e0503881d49e545744c3d}{%
           family={Dollár},
           familyi={D\bibinitperiod},
           given={Piotr},
           giveni={P\bibinitperiod}}}%
        {{hash=bd2fff33d3a5433188d2662695e79eb5}{%
           family={Feichtenhofer},
           familyi={F\bibinitperiod},
           given={Christoph},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{122b0c0c258a2e9f0862d559d3369d99}
      \strng{fullhash}{1d075f09ffd1d1e0ac0f2e0e502b680f}
      \strng{bibnamehash}{122b0c0c258a2e9f0862d559d3369d99}
      \strng{authorbibnamehash}{122b0c0c258a2e9f0862d559d3369d99}
      \strng{authornamehash}{122b0c0c258a2e9f0862d559d3369d99}
      \strng{authorfullhash}{1d075f09ffd1d1e0ac0f2e0e502b680f}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3\texttimes{} fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6\texttimes{} faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.}
      \field{bookpagination}{page}
      \field{journaltitle}{ArXiv}
      \field{pagination}{column}
      \field{title}{SAM 2: Segment Anything in Images and Videos}
      \field{year}{2024}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2408.00714
      \endverb
    \endentry
    \entry{Hafiz2020}{article}{}
      \name{author}{2}{}{%
        {{hash=2519a7a602501696878944968aafd1b1}{%
           family={Hafiz},
           familyi={H\bibinitperiod},
           given={Abdul\bibnamedelima Mueed},
           giveni={A\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=9aeeb71e3119b39fb6fedef8d4f601d7}{%
           family={Bhat},
           familyi={B\bibinitperiod},
           given={Ghulam\bibnamedelima Mohiuddin},
           giveni={G\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \strng{namehash}{f75e035211040bc214877ed5f998e839}
      \strng{fullhash}{f75e035211040bc214877ed5f998e839}
      \strng{bibnamehash}{f75e035211040bc214877ed5f998e839}
      \strng{authorbibnamehash}{f75e035211040bc214877ed5f998e839}
      \strng{authornamehash}{f75e035211040bc214877ed5f998e839}
      \strng{authorfullhash}{f75e035211040bc214877ed5f998e839}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Object detection or localization is an incremental step in progression from coarse to fine digital image inference. It not only provides the classes of the image objects, but also provides the location of the image objects which have been classified. The location is given in the form of bounding boxes or centroids. Semantic segmentation gives fine inference by predicting labels for every pixel in the input image. Each pixel is labelled according to the object class within which it is enclosed. Furthering this evolution, instance segmentation gives different labels for separate instances of objects belonging to the same class. Hence, instance segmentation may be defined as the technique of simultaneously solving the problem of object detection as well as that of semantic segmentation. In this survey paper on instance segmentation, its background, issues, techniques, evolution, popular datasets, related work up to the state of the art and future scope have been discussed. The paper provides valuable information for those who want to do research in the field of instance segmentation.}
      \field{bookpagination}{page}
      \field{day}{1}
      \field{issn}{2192-662X}
      \field{journaltitle}{International Journal of Multimedia Information Retrieval}
      \field{month}{9}
      \field{number}{3}
      \field{pagination}{column}
      \field{title}{A survey on instance segmentation: state of the art}
      \field{volume}{9}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{pages}{171\bibrangedash 189}
      \range{pages}{19}
      \verb{doi}
      \verb 10.1007/s13735-020-00195-x
      \endverb
    \endentry
    \entry{Szeliski2022}{book}{}
      \name{author}{1}{}{%
        {{hash=f2e556c93256e7d70895d67702e14f1a}{%
           family={Szeliski},
           familyi={S\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer Cham}%
      }
      \strng{namehash}{f2e556c93256e7d70895d67702e14f1a}
      \strng{fullhash}{f2e556c93256e7d70895d67702e14f1a}
      \strng{bibnamehash}{f2e556c93256e7d70895d67702e14f1a}
      \strng{authorbibnamehash}{f2e556c93256e7d70895d67702e14f1a}
      \strng{authornamehash}{f2e556c93256e7d70895d67702e14f1a}
      \strng{authorfullhash}{f2e556c93256e7d70895d67702e14f1a}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{day}{3}
      \field{edition}{2}
      \field{month}{1}
      \field{series}{Texts in Computer Science}
      \field{title}{Computer Vision: Algorithms and Applications}
      \field{year}{2022}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.1007/978-3-030-34372-9
      \endverb
      \keyw{Image Processing and Computer Vision,Computer Imaging,Vision,Pattern Recognition and Graphics,Machine Learning,Signal,Image and Speech Processing,Materials Science}
    \endentry
    \entry{SIFT}{inproceedings}{}
      \name{author}{1}{}{%
        {{hash=282604e42f1f3c8f1347283c98e7a7b8}{%
           family={Lowe},
           familyi={L\bibinitperiod},
           given={D.G.},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{282604e42f1f3c8f1347283c98e7a7b8}
      \strng{fullhash}{282604e42f1f3c8f1347283c98e7a7b8}
      \strng{bibnamehash}{282604e42f1f3c8f1347283c98e7a7b8}
      \strng{authorbibnamehash}{282604e42f1f3c8f1347283c98e7a7b8}
      \strng{authornamehash}{282604e42f1f3c8f1347283c98e7a7b8}
      \strng{authorfullhash}{282604e42f1f3c8f1347283c98e7a7b8}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds.}
      \field{booktitle}{Proceedings of the Seventh IEEE International Conference on Computer Vision}
      \field{day}{6}
      \field{eventday}{20}
      \field{eventendday}{27}
      \field{eventendmonth}{9}
      \field{eventendyear}{1999}
      \field{eventmonth}{9}
      \field{eventyear}{1999}
      \field{month}{8}
      \field{title}{Object recognition from local scale-invariant features}
      \field{venue}{Kerkyra, Greece}
      \field{volume}{2}
      \field{year}{2002}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{1150\bibrangedash 1157}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1109/ICCV.1999.790410
      \endverb
      \keyw{Object recognition;Electrical capacitance tomography;Image recognition;Lighting;Neurons;Computer science;Reactive power;Filters;Programmable logic arrays;Layout}
    \endentry
    \entry{HOG}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=521763e082baf91e4935f956c6c0f8b3}{%
           family={Dalal},
           familyi={D\bibinitperiod},
           given={N.},
           giveni={N\bibinitperiod}}}%
        {{hash=7edc70f9ce27f4a800aa68cbd1625f51}{%
           family={Triggs},
           familyi={T\bibinitperiod},
           given={Bill},
           giveni={B\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{4ae85adc057e8f75dade7f2f4223567b}
      \strng{fullhash}{4ae85adc057e8f75dade7f2f4223567b}
      \strng{bibnamehash}{4ae85adc057e8f75dade7f2f4223567b}
      \strng{authorbibnamehash}{4ae85adc057e8f75dade7f2f4223567b}
      \strng{authornamehash}{4ae85adc057e8f75dade7f2f4223567b}
      \strng{authorfullhash}{4ae85adc057e8f75dade7f2f4223567b}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.}
      \field{booktitle}{2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}
      \field{day}{25}
      \field{eventday}{20}
      \field{eventendday}{25}
      \field{eventendmonth}{6}
      \field{eventendyear}{2005}
      \field{eventmonth}{6}
      \field{eventyear}{2005}
      \field{isbn}{0-7695-2372-2}
      \field{issn}{1063-6919}
      \field{month}{7}
      \field{title}{Histograms of oriented gradients for human detection}
      \field{venue}{San Diego, CA, USA}
      \field{volume}{1}
      \field{year}{2005}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{886\bibrangedash 893}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1109/CVPR.2005.177
      \endverb
      \keyw{Histograms;Humans;Robustness;Object recognition;Support vector machines;Object detection;Testing;Image edge detection;High performance computing;Image databases}
    \endentry
    \entry{BagOfWords}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=13752211bfbae548e9ed263f52fb706a}{%
           family={Sivic},
           familyi={S\bibinitperiod},
           given={J.},
           giveni={J\bibinitperiod}}}%
        {{hash=e77fd9dd5d9af3c798867760f8828744}{%
           family={Zisserman},
           familyi={Z\bibinitperiod},
           given={A.},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{941387bc926b9e947257967a5e303a30}
      \strng{fullhash}{941387bc926b9e947257967a5e303a30}
      \strng{bibnamehash}{941387bc926b9e947257967a5e303a30}
      \strng{authorbibnamehash}{941387bc926b9e947257967a5e303a30}
      \strng{authornamehash}{941387bc926b9e947257967a5e303a30}
      \strng{authorfullhash}{941387bc926b9e947257967a5e303a30}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieved is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching in two full length feature films.}
      \field{booktitle}{Proceedings Ninth IEEE International Conference on Computer Vision}
      \field{day}{3}
      \field{eventday}{13}
      \field{eventendday}{16}
      \field{eventendmonth}{11}
      \field{eventendyear}{2003}
      \field{eventmonth}{11}
      \field{eventyear}{2003}
      \field{isbn}{0-7695-1950-4}
      \field{month}{4}
      \field{title}{Video Google: a text retrieval approach to object matching in videos}
      \field{venue}{Nice, France}
      \field{volume}{2}
      \field{year}{2008}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{1470\bibrangedash 1477}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1109/ICCV.2003.1238663
      \endverb
      \keyw{Web pages;Lighting;Vector quantization;Image databases;Robots;Layout;Noise reduction;File systems;Object recognition;Visual databases}
    \endentry
    \entry{FisherVector}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=5a3c11e2931f5c99d33069b6ed09adca}{%
           family={Perronnin},
           familyi={P\bibinitperiod},
           given={Florent},
           giveni={F\bibinitperiod}}}%
        {{hash=b887124724c989cb9c4c2bdd2d7e639d}{%
           family={Sánchez},
           familyi={S\bibinitperiod},
           given={Jorge},
           giveni={J\bibinitperiod}}}%
        {{hash=ffc3c50f7f2f5814db998defff4b2ccd}{%
           family={Mensink},
           familyi={M\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
      }
      \name{editor}{3}{}{%
        {{hash=26789faf98e6ba54553cd620a8da71b2}{%
           family={Daniilidis},
           familyi={D\bibinitperiod},
           given={Kostas},
           giveni={K\bibinitperiod}}}%
        {{hash=d5eaaddb78f9e969a9d037a5d2ae6df3}{%
           family={Maragos},
           familyi={M\bibinitperiod},
           given={Petros},
           giveni={P\bibinitperiod}}}%
        {{hash=8392955ddce1cfbdf31363a688094e1e}{%
           family={Paragios},
           familyi={P\bibinitperiod},
           given={Nikos},
           giveni={N\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer Berlin Heidelberg}%
      }
      \strng{namehash}{97a00735297127cf8c83ffa018f95ede}
      \strng{fullhash}{97a00735297127cf8c83ffa018f95ede}
      \strng{bibnamehash}{97a00735297127cf8c83ffa018f95ede}
      \strng{authorbibnamehash}{97a00735297127cf8c83ffa018f95ede}
      \strng{authornamehash}{97a00735297127cf8c83ffa018f95ede}
      \strng{authorfullhash}{97a00735297127cf8c83ffa018f95ede}
      \strng{editorbibnamehash}{a08b689b21195dfbf0498e1cd30d1681}
      \strng{editornamehash}{a08b689b21195dfbf0498e1cd30d1681}
      \strng{editorfullhash}{a08b689b21195dfbf0498e1cd30d1681}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The Fisher kernel (FK) is a generic framework which combines the benefits of generative and discriminative approaches. In the context of image classification the FK was shown to extend the popular bag-of-visual-words (BOV) by going beyond count statistics. However, in practice, this enriched representation has not yet shown its superiority over the BOV. In the first part we show that with several well-motivated modifications over the original framework we can boost the accuracy of the FK. On PASCAL VOC 2007 we increase the Average Precision (AP) from 47.9{\%} to 58.3{\%}. Similarly, we demonstrate state-of-the-art accuracy on CalTech 256. A major advantage is that these results are obtained using only SIFT descriptors and costless linear classifiers. Equipped with this representation, we can now explore image classification on a larger scale. In the second part, as an application, we compare two abundant resources of labeled images to learn classifiers: ImageNet and Flickr groups. In an evaluation involving hundreds of thousands of training images we show that classifiers learned on Flickr groups perform surprisingly well (although they were not intended for this purpose) and that they can complement classifiers learned on more carefully annotated datasets.}
      \field{booktitle}{Proceedings of IEEE European Conference on Computer Vision, 2010}
      \field{eventday}{5}
      \field{eventendday}{11}
      \field{eventendmonth}{9}
      \field{eventendyear}{2010}
      \field{eventmonth}{9}
      \field{eventtitle}{Computer Vision -- ECCV 2010}
      \field{eventyear}{2010}
      \field{title}{Improving the Fisher Kernel for Large-Scale Image Classification}
      \field{venue}{Heraklion, Crete, Greece}
      \field{volume}{6314}
      \field{year}{2010}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{143\bibrangedash 156}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1007/978-3-642-15561-1_11
      \endverb
    \endentry
    \entry{AlexNet}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=c5e3a676e2ac1164b3afcd539c131fc9}{%
           family={Krizhevsky},
           familyi={K\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=8d569d1d5b8b5a7836017a98b430f959}{%
           family={Sutskever},
           familyi={S\bibinitperiod},
           given={Ilya},
           giveni={I\bibinitperiod}}}%
        {{hash=813bd95fe553e6079cd53a567b238287}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey\bibnamedelima E},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \name{editor}{4}{}{%
        {{hash=128472f7930be2fd0645b02dc02aaeb8}{%
           family={Pereira},
           familyi={P\bibinitperiod},
           given={F.},
           giveni={F\bibinitperiod}}}%
        {{hash=270087258d6a002033f05032fbdf6fad}{%
           family={Burges},
           familyi={B\bibinitperiod},
           given={C.J.},
           giveni={C\bibinitperiod}}}%
        {{hash=bbfb0f3936c83b7b099561e6f0e32ef3}{%
           family={Bottou},
           familyi={B\bibinitperiod},
           given={L.},
           giveni={L\bibinitperiod}}}%
        {{hash=ad5ed31dbb8d37755c6cb48bedfdfe1d}{%
           family={Weinberger},
           familyi={W\bibinitperiod},
           given={K.Q.},
           giveni={K\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{1a23c09aa65b3c2ade45ed18d8127375}
      \strng{fullhash}{1a23c09aa65b3c2ade45ed18d8127375}
      \strng{bibnamehash}{1a23c09aa65b3c2ade45ed18d8127375}
      \strng{authorbibnamehash}{1a23c09aa65b3c2ade45ed18d8127375}
      \strng{authornamehash}{1a23c09aa65b3c2ade45ed18d8127375}
      \strng{authorfullhash}{1a23c09aa65b3c2ade45ed18d8127375}
      \strng{editorbibnamehash}{6d86a4a427d6d8717a2b98ed4d40921c}
      \strng{editornamehash}{6d86a4a427d6d8717a2b98ed4d40921c}
      \strng{editorfullhash}{7d78cfe0216557561ed3a11320faaa87}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\% and 18.9\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.}
      \field{booktitle}{Advances in Neural Information Processing Systems}
      \field{eid}{534}
      \field{eventyear}{2012}
      \field{isbn}{9781627480031}
      \field{title}{ImageNet Classification with Deep Convolutional Neural Networks}
      \field{volume}{25}
      \field{year}{2012}
      \field{eventdateera}{ce}
      \field{pages}{1097\bibrangedash 1105}
      \range{pages}{9}
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
      \endverb
    \endentry
    \entry{VGGNet}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=9d16b7284df92c9adaee86c37ab992df}{%
           family={Simonyan},
           familyi={S\bibinitperiod},
           given={Karen},
           giveni={K\bibinitperiod}}}%
        {{hash=c72fc39e94030f67717052309266a44d}{%
           family={Zisserman},
           familyi={Z\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
      }
      \name{editor}{2}{}{%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
        {{hash=6a1aa6b7eab12b931ca7c7e3f927231d}{%
           family={LeCun},
           familyi={L\bibinitperiod},
           given={Yann},
           giveni={Y\bibinitperiod}}}%
      }
      \list{publisher}{2}{%
        {Computational}%
        {Biological Learning Society}%
      }
      \strng{namehash}{25d2f3c4577a6632d37f0126cc781232}
      \strng{fullhash}{25d2f3c4577a6632d37f0126cc781232}
      \strng{bibnamehash}{25d2f3c4577a6632d37f0126cc781232}
      \strng{authorbibnamehash}{25d2f3c4577a6632d37f0126cc781232}
      \strng{authornamehash}{25d2f3c4577a6632d37f0126cc781232}
      \strng{authorfullhash}{25d2f3c4577a6632d37f0126cc781232}
      \strng{editorbibnamehash}{afe3c75951d1843ce2846d6c16647f41}
      \strng{editornamehash}{afe3c75951d1843ce2846d6c16647f41}
      \strng{editorfullhash}{afe3c75951d1843ce2846d6c16647f41}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.}
      \field{booktitle}{{ICLR} 2015 Conference Track Proceedings}
      \field{eventday}{7}
      \field{eventendday}{9}
      \field{eventendmonth}{5}
      \field{eventendyear}{2015}
      \field{eventmonth}{5}
      \field{eventtitle}{3rd International Conference on Learning Representations (ICLR 2015)}
      \field{eventyear}{2015}
      \field{title}{Very Deep Convolutional Networks for Large-Scale Image Recognition}
      \field{venue}{San Diego, CA, USA}
      \field{year}{2015}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{1\bibrangedash 14}
      \range{pages}{14}
      \verb{doi}
      \verb 10.48550/arXiv.1409.1556
      \endverb
    \endentry
    \entry{ResNet}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=6b4b60e909e78633945f3f9c9dc83e01}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Kaiming},
           giveni={K\bibinitperiod}}}%
        {{hash=5e72bc22dbcf0984c6d113d280e36990}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Xiangyu},
           giveni={X\bibinitperiod}}}%
        {{hash=bb295293acacd54387339079ebbe4ead}{%
           family={Ren},
           familyi={R\bibinitperiod},
           given={Shaoqing},
           giveni={S\bibinitperiod}}}%
        {{hash=f85751488058842b5777c7b4074077b5}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Jian},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{fullhash}{42c4b52dc3a62cebabbc11c73e1afb53}
      \strng{bibnamehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{authorbibnamehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{authornamehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{authorfullhash}{42c4b52dc3a62cebabbc11c73e1afb53}
      \field{extraname}{3}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.}
      \field{booktitle}{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
      \field{eventday}{27}
      \field{eventendday}{30}
      \field{eventendmonth}{6}
      \field{eventendyear}{2016}
      \field{eventmonth}{6}
      \field{eventyear}{2016}
      \field{isbn}{978-1-4673-8851-1}
      \field{issn}{1063-6919}
      \field{title}{Deep Residual Learning for Image Recognition}
      \field{venue}{Las Vegas, NV, USA}
      \field{year}{2016}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{770\bibrangedash 778}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1109/CVPR.2016.90
      \endverb
      \keyw{Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation}
    \endentry
    \entry{DenseNet}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=0d12f90a1d1945cb1b98f583c9dc9572}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Gao},
           giveni={G\bibinitperiod}}}%
        {{hash=f7eda249dc15ce9f452344ec95bbcc67}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Zhuang},
           giveni={Z\bibinitperiod}}}%
        {{hash=a33ababcca5b83a7777452957fb2eef2}{%
           family={Van\bibnamedelimb Der\bibnamedelima Maaten},
           familyi={V\bibinitperiod\bibinitdelim D\bibinitperiod\bibinitdelim M\bibinitperiod},
           given={Laurens},
           giveni={L\bibinitperiod}}}%
        {{hash=68a0238356fbd88b34be8886f25938a7}{%
           family={Weinberger},
           familyi={W\bibinitperiod},
           given={Kilian\bibnamedelima Q.},
           giveni={K\bibinitperiod\bibinitdelim Q\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{3dcda21c08021918fcbe6ecf2e003538}
      \strng{fullhash}{ec6d0d953a00cc76cc77fbf9d9e1ec27}
      \strng{bibnamehash}{3dcda21c08021918fcbe6ecf2e003538}
      \strng{authorbibnamehash}{3dcda21c08021918fcbe6ecf2e003538}
      \strng{authornamehash}{3dcda21c08021918fcbe6ecf2e003538}
      \strng{authorfullhash}{ec6d0d953a00cc76cc77fbf9d9e1ec27}
      \field{extraname}{2}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections-one between each layer and its subsequent layer-our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.}
      \field{booktitle}{2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
      \field{day}{9}
      \field{eventday}{21}
      \field{eventendday}{26}
      \field{eventendmonth}{7}
      \field{eventendyear}{2017}
      \field{eventmonth}{7}
      \field{eventyear}{2017}
      \field{isbn}{978-1-5386-0457-1}
      \field{issn}{1063-6919}
      \field{month}{11}
      \field{title}{Densely Connected Convolutional Networks}
      \field{venue}{Honolulu, HI, USA}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{2261\bibrangedash 2269}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1109/CVPR.2017.243
      \endverb
      \keyw{Training;Convolution;Network architecture;Convolutional codes;Neural networks;Road transportation}
    \endentry
    \entry{GoogleNet}{inproceedings}{}
      \name{author}{9}{}{%
        {{hash=ed568d9c3bb059e6bf22899fbf170f86}{%
           family={Szegedy},
           familyi={S\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod}}}%
        {{hash=c0e0d23e2d09e45e6f51cc2bcea6d9f9}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Wei},
           giveni={W\bibinitperiod}}}%
        {{hash=9fce03efe6b3331a1b93ed2e7c0da9d5}{%
           family={Jia},
           familyi={J\bibinitperiod},
           given={Yangqing},
           giveni={Y\bibinitperiod}}}%
        {{hash=15f5333df96deaf51c72d065bded37d8}{%
           family={Sermanet},
           familyi={S\bibinitperiod},
           given={Pierre},
           giveni={P\bibinitperiod}}}%
        {{hash=698ee61a2f3fa29734204496d2d36aef}{%
           family={Reed},
           familyi={R\bibinitperiod},
           given={Scott},
           giveni={S\bibinitperiod}}}%
        {{hash=c1826f3465579186aff299a9b0e16ed7}{%
           family={Anguelov},
           familyi={A\bibinitperiod},
           given={Dragomir},
           giveni={D\bibinitperiod}}}%
        {{hash=8bbc4c5d96f205bada839e74e0202146}{%
           family={Erhan},
           familyi={E\bibinitperiod},
           given={Dumitru},
           giveni={D\bibinitperiod}}}%
        {{hash=8051922e7bd286f884bfbd1023ef62f5}{%
           family={Vanhoucke},
           familyi={V\bibinitperiod},
           given={Vincent},
           giveni={V\bibinitperiod}}}%
        {{hash=aa04c4d6213a1e867b1650e298cb2668}{%
           family={Rabinovich},
           familyi={R\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{80f8e6bfc3aff3e75b2807a6f6962740}
      \strng{fullhash}{64fbaf3c8a6b53523f74f0087b58e7e6}
      \strng{bibnamehash}{80f8e6bfc3aff3e75b2807a6f6962740}
      \strng{authorbibnamehash}{80f8e6bfc3aff3e75b2807a6f6962740}
      \strng{authornamehash}{80f8e6bfc3aff3e75b2807a6f6962740}
      \strng{authorfullhash}{64fbaf3c8a6b53523f74f0087b58e7e6}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.}
      \field{booktitle}{2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
      \field{day}{15}
      \field{eventday}{7}
      \field{eventendday}{12}
      \field{eventendmonth}{6}
      \field{eventendyear}{2015}
      \field{eventmonth}{6}
      \field{eventyear}{2015}
      \field{isbn}{978-1-4673-6964-0}
      \field{issn}{1063-6919}
      \field{month}{10}
      \field{title}{Going deeper with convolutions}
      \field{venue}{Boston, MA, USA}
      \field{year}{2015}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{1\bibrangedash 9}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1109/CVPR.2015.7298594
      \endverb
      \keyw{Computer architecture;Convolutional codes;Sparse matrices;Neural networks;Visualization;Object detection;Computer vision}
    \endentry
    \entry{OverFeat}{article}{}
      \name{author}{6}{}{%
        {{hash=15f5333df96deaf51c72d065bded37d8}{%
           family={Sermanet},
           familyi={S\bibinitperiod},
           given={Pierre},
           giveni={P\bibinitperiod}}}%
        {{hash=19ac0399a087af1f10403369b2bf52b0}{%
           family={Eigen},
           familyi={E\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=87f628db6da99dc0a2a20f276e51b3ac}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Xiang},
           giveni={X\bibinitperiod}}}%
        {{hash=c8e0f85f89c92f05a8a7ee998755b202}{%
           family={Mathieu},
           familyi={M\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=a6784304d1cc890b2cb6c6c7f2f3fd76}{%
           family={Fergus},
           familyi={F\bibinitperiod},
           given={Rob},
           giveni={R\bibinitperiod}}}%
        {{hash=d0ab8cbca75df7aa3e0b6024d60ac5f8}{%
           family={Lecun},
           familyi={L\bibinitperiod},
           given={Yann},
           giveni={Y\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {719 Broadway, 12th Floor, New York, NY 10003}%
      }
      \strng{namehash}{b25e91534b92f8a61f0fb523d489acb2}
      \strng{fullhash}{565b269f02ce355a40b0a009419cb0b9}
      \strng{bibnamehash}{b25e91534b92f8a61f0fb523d489acb2}
      \strng{authorbibnamehash}{b25e91534b92f8a61f0fb523d489acb2}
      \strng{authornamehash}{b25e91534b92f8a61f0fb523d489acb2}
      \strng{authorfullhash}{565b269f02ce355a40b0a009419cb0b9}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{bookpagination}{page}
      \field{day}{24}
      \field{journaltitle}{International Conference on Learning Representations (ICLR) (Banff)}
      \field{month}{2}
      \field{pagination}{column}
      \field{title}{OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}
      \field{year}{2014}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1312.6229
      \endverb
    \endentry
    \entry{ZFNet}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=90d345b65e166a9cfc0c6ddec4ddc3d6}{%
           family={Zeiler},
           familyi={Z\bibinitperiod},
           given={Matthew\bibnamedelima D.},
           giveni={M\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=a6784304d1cc890b2cb6c6c7f2f3fd76}{%
           family={Fergus},
           familyi={F\bibinitperiod},
           given={Rob},
           giveni={R\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer International Publishing}%
      }
      \strng{namehash}{85ff4f1b7cd8e8f00fa953ed2f69136a}
      \strng{fullhash}{85ff4f1b7cd8e8f00fa953ed2f69136a}
      \strng{bibnamehash}{85ff4f1b7cd8e8f00fa953ed2f69136a}
      \strng{authorbibnamehash}{85ff4f1b7cd8e8f00fa953ed2f69136a}
      \strng{authornamehash}{85ff4f1b7cd8e8f00fa953ed2f69136a}
      \strng{authorfullhash}{85ff4f1b7cd8e8f00fa953ed2f69136a}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.}
      \field{booktitle}{Computer Vision -- ECCV 2014}
      \field{isbn}{978-3-319-10590-1}
      \field{title}{Visualizing and Understanding Convolutional Networks}
      \field{year}{2014}
      \field{pages}{818\bibrangedash 833}
      \range{pages}{16}
      \verb{urlraw}
      \verb https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf
      \endverb
      \verb{url}
      \verb https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf
      \endverb
    \endentry
    \entry{YOLO}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=99bced2e56a5253f3fe98a5f04e6d9b2}{%
           family={Redmon},
           familyi={R\bibinitperiod},
           given={Joseph},
           giveni={J\bibinitperiod}}}%
        {{hash=05ca9f19da9ecbd2def4e5514f8043c8}{%
           family={Divvala},
           familyi={D\bibinitperiod},
           given={Santosh},
           giveni={S\bibinitperiod}}}%
        {{hash=bd5dadbe57bedc5957c19a3154c4d424}{%
           family={Girshick},
           familyi={G\bibinitperiod},
           given={Ross},
           giveni={R\bibinitperiod}}}%
        {{hash=396c6ddedb6f986906fc3e4994d19974}{%
           family={Farhadi},
           familyi={F\bibinitperiod},
           given={Ali},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{e1203a0044715040adeb8c5079ee645a}
      \strng{fullhash}{b5530443e433a4da53dbe3cf155225b4}
      \strng{bibnamehash}{e1203a0044715040adeb8c5079ee645a}
      \strng{authorbibnamehash}{e1203a0044715040adeb8c5079ee645a}
      \strng{authornamehash}{e1203a0044715040adeb8c5079ee645a}
      \strng{authorfullhash}{b5530443e433a4da53dbe3cf155225b4}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.}
      \field{booktitle}{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
      \field{day}{12}
      \field{eventday}{27}
      \field{eventendday}{30}
      \field{eventendmonth}{6}
      \field{eventendyear}{2016}
      \field{eventmonth}{6}
      \field{eventyear}{2016}
      \field{isbn}{978-1-4673-8851-1}
      \field{issn}{1063-6919}
      \field{month}{12}
      \field{title}{You Only Look Once: Unified, Real-Time Object Detection}
      \field{venue}{Las Vegas, NV, USA}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{779\bibrangedash 788}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/CVPR.2016.91
      \endverb
      \keyw{Computer architecture;Microprocessors;Object detection;Training;Real-time systems;Neural networks;Pipelines}
    \endentry
    \entry{RCNN}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=bd5dadbe57bedc5957c19a3154c4d424}{%
           family={Girshick},
           familyi={G\bibinitperiod},
           given={Ross},
           giveni={R\bibinitperiod}}}%
        {{hash=913090a0d65c7a300c602f5a5a5a61e3}{%
           family={Donahue},
           familyi={D\bibinitperiod},
           given={Jeff},
           giveni={J\bibinitperiod}}}%
        {{hash=90180e1a30742e0d15328bfe637c2ef4}{%
           family={Darrell},
           familyi={D\bibinitperiod},
           given={Trevor},
           giveni={T\bibinitperiod}}}%
        {{hash=c75a5377b6dc5213831576e88ffe553c}{%
           family={Malik},
           familyi={M\bibinitperiod},
           given={Jitendra},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{5aa9f085e8eccf547d1eb54a05cb1beb}
      \strng{fullhash}{9147daa2f6a058d5b8880ff5cf38abd6}
      \strng{bibnamehash}{5aa9f085e8eccf547d1eb54a05cb1beb}
      \strng{authorbibnamehash}{5aa9f085e8eccf547d1eb54a05cb1beb}
      \strng{authornamehash}{5aa9f085e8eccf547d1eb54a05cb1beb}
      \strng{authorfullhash}{9147daa2f6a058d5b8880ff5cf38abd6}
      \field{sortinit}{7}
      \field{sortinithash}{108d0be1b1bee9773a1173443802c0a3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.}
      \field{booktitle}{2014 IEEE Conference on Computer Vision and Pattern Recognition}
      \field{day}{25}
      \field{eventday}{23}
      \field{eventendday}{28}
      \field{eventendmonth}{6}
      \field{eventendyear}{2014}
      \field{eventmonth}{6}
      \field{eventyear}{2014}
      \field{isbn}{978-1-4799-5118-5}
      \field{issn}{1063-6919}
      \field{month}{9}
      \field{title}{Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation}
      \field{venue}{Columbus, OH, USA}
      \field{year}{2014}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{580\bibrangedash 587}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1109/CVPR.2014.81
      \endverb
      \keyw{Proposals;Feature extraction;Training;Visualization;Object detection;Vectors;Support vector machines}
    \endentry
    \entry{FastRCNN}{inproceedings}{}
      \name{author}{1}{}{%
        {{hash=bd5dadbe57bedc5957c19a3154c4d424}{%
           family={Girshick},
           familyi={G\bibinitperiod},
           given={Ross},
           giveni={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{bd5dadbe57bedc5957c19a3154c4d424}
      \strng{fullhash}{bd5dadbe57bedc5957c19a3154c4d424}
      \strng{bibnamehash}{bd5dadbe57bedc5957c19a3154c4d424}
      \strng{authorbibnamehash}{bd5dadbe57bedc5957c19a3154c4d424}
      \strng{authornamehash}{bd5dadbe57bedc5957c19a3154c4d424}
      \strng{authorfullhash}{bd5dadbe57bedc5957c19a3154c4d424}
      \field{sortinit}{7}
      \field{sortinithash}{108d0be1b1bee9773a1173443802c0a3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.}
      \field{booktitle}{2015 IEEE International Conference on Computer Vision (ICCV)}
      \field{day}{18}
      \field{eventday}{7}
      \field{eventendday}{13}
      \field{eventendmonth}{12}
      \field{eventendyear}{2015}
      \field{eventmonth}{12}
      \field{eventyear}{2015}
      \field{isbn}{978-1-4673-8391-2}
      \field{issn}{2380-7504}
      \field{month}{2}
      \field{title}{Fast R-CNN}
      \field{venue}{Santiago, Chile}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{1440\bibrangedash 1448}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1109/ICCV.2015.169
      \endverb
      \keyw{Training;Proposals;Feature extraction;Object detection;Pipelines;Computer architecture;Open source software}
    \endentry
    \entry{PANet}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=71a0fe4c49a825f2c7caf4d51d2b8603}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Shu},
           giveni={S\bibinitperiod}}}%
        {{hash=db9ae9f8b01f502cd5ebe689c86bcf4b}{%
           family={Qi},
           familyi={Q\bibinitperiod},
           given={Lu},
           giveni={L\bibinitperiod}}}%
        {{hash=c6f6c305fda5d865072c7f30c3c62b92}{%
           family={Qin},
           familyi={Q\bibinitperiod},
           given={Haifang},
           giveni={H\bibinitperiod}}}%
        {{hash=3797f666351febe439e04520ec708726}{%
           family={Shi},
           familyi={S\bibinitperiod},
           given={Jianping},
           giveni={J\bibinitperiod}}}%
        {{hash=c919379669a576a7dc662e44abbbcd6c}{%
           family={Jia},
           familyi={J\bibinitperiod},
           given={Jiaya},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{c9d612a094f098fd6f3726fbb30753d5}
      \strng{fullhash}{55e53b7c47c03fcdbe94090628be09ef}
      \strng{bibnamehash}{c9d612a094f098fd6f3726fbb30753d5}
      \strng{authorbibnamehash}{c9d612a094f098fd6f3726fbb30753d5}
      \strng{authornamehash}{c9d612a094f098fd6f3726fbb30753d5}
      \strng{authorfullhash}{55e53b7c47c03fcdbe94090628be09ef}
      \field{sortinit}{7}
      \field{sortinithash}{108d0be1b1bee9773a1173443802c0a3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction. These improvements are simple to implement, with subtle extra computational overhead. Yet they are useful and make our PANet reach the 1st place in the COCO 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. PANet is also state-of-the-art on MVD and Cityscapes.}
      \field{booktitle}{2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}
      \field{day}{16}
      \field{eventday}{18}
      \field{eventendday}{23}
      \field{eventendmonth}{6}
      \field{eventendyear}{2018}
      \field{eventmonth}{6}
      \field{eventyear}{2018}
      \field{isbn}{978-1-5386-6420-9}
      \field{issn}{2575-7075}
      \field{month}{12}
      \field{title}{Path Aggregation Network for Instance Segmentation}
      \field{venue}{Salt Lake City, UT, USA}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{8759\bibrangedash 8768}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/CVPR.2018.00913
      \endverb
      \keyw{Proposals;Feature extraction;Task analysis;Image segmentation;Object detection;Training;Semantics}
    \endentry
    \entry{YOLACT}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=13cb4fa272db1b16ecf6166d6688dd14}{%
           family={Bolya},
           familyi={B\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=f59fb4c18b2234fa50461d2359a83b7d}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Chong},
           giveni={C\bibinitperiod}}}%
        {{hash=4eec7b4b02885ea567be3cf7f92a7f05}{%
           family={Xiao},
           familyi={X\bibinitperiod},
           given={Fanyi},
           giveni={F\bibinitperiod}}}%
        {{hash=0058780d2044b7ad95d263c45e1a1b15}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Yong\bibnamedelima Jae},
           giveni={Y\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{601fb5dd5842c24ab1d4c96488c24288}
      \strng{fullhash}{1877b4701b7c873cac03b545fb7406d9}
      \strng{bibnamehash}{601fb5dd5842c24ab1d4c96488c24288}
      \strng{authorbibnamehash}{601fb5dd5842c24ab1d4c96488c24288}
      \strng{authornamehash}{601fb5dd5842c24ab1d4c96488c24288}
      \strng{authorfullhash}{1877b4701b7c873cac03b545fb7406d9}
      \field{sortinit}{7}
      \field{sortinithash}{108d0be1b1bee9773a1173443802c0a3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a simple, fully-convolutional model for real-time instance segmentation that achieves 29.8 mAP on MS COCO at 33.5 fps evaluated on a single Titan Xp, which is significantly faster than any previous competitive approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn't depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. Finally, we also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty.}
      \field{booktitle}{2019 IEEE/CVF International Conference on Computer Vision (ICCV)}
      \field{day}{17}
      \field{eventday}{27}
      \field{eventendday}{2}
      \field{eventendmonth}{11}
      \field{eventendyear}{2019}
      \field{eventmonth}{10}
      \field{eventyear}{2019}
      \field{isbn}{978-1-7281-4803-8}
      \field{issn}{2380-7504}
      \field{month}{2}
      \field{title}{YOLACT: Real-Time Instance Segmentation}
      \field{venue}{Seoul, Korea}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{9156\bibrangedash 9165}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/ICCV.2019.00925
      \endverb
      \keyw{Prototypes;Real-time systems;Image segmentation;Object detection;Detectors;Computational modeling;Task analysis}
    \endentry
    \entry{DeepWatershed}{article}{}
      \name{author}{2}{}{%
        {{hash=b2c6e382429e61fe370192dddc891d04}{%
           family={Bai},
           familyi={B\bibinitperiod},
           given={Min},
           giveni={M\bibinitperiod}}}%
        {{hash=bc58c1e920023b071a4d0695299cf804}{%
           family={Urtasun},
           familyi={U\bibinitperiod},
           given={Raquel},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{38dfd022f4bc03a34f38f001d17d8679}
      \strng{fullhash}{38dfd022f4bc03a34f38f001d17d8679}
      \strng{bibnamehash}{38dfd022f4bc03a34f38f001d17d8679}
      \strng{authorbibnamehash}{38dfd022f4bc03a34f38f001d17d8679}
      \strng{authornamehash}{38dfd022f4bc03a34f38f001d17d8679}
      \strng{authorfullhash}{38dfd022f4bc03a34f38f001d17d8679}
      \field{sortinit}{8}
      \field{sortinithash}{a231b008ebf0ecbe0b4d96dcc159445f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Most contemporary approaches to instance segmentation use complex pipelines involving conditional random fields, recurrent neural networks, object proposals, or template matching schemes. In our paper, we present a simple yet powerful end-to-end convolutional neural network to tackle this task. Our approach combines intuitions from the classical watershed transform and modern deep learning to produce an energy map of the image where object instances are unambiguously represented as basins in the energy map. We then perform a cut at a single energy level to directly yield connected components corresponding to object instances. Our model more than doubles the performance of the state-of-the-art on the challenging Cityscapes Instance Level Segmentation task.}
      \field{bookpagination}{page}
      \field{day}{24}
      \field{journaltitle}{CoRR}
      \field{month}{11}
      \field{pagination}{column}
      \field{title}{Deep Watershed Transform for Instance Segmentation}
      \field{year}{2016}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1611.08303
      \endverb
    \endentry
    \entry{DeepMask}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=6f4e878c2351426240a61ae23e34b56c}{%
           family={Pinheiro},
           familyi={P\bibinitperiod},
           given={Pedro\bibnamedelima O.},
           giveni={P\bibinitperiod\bibinitdelim O\bibinitperiod}}}%
        {{hash=133261178beda1dc3991e0e1dfd5a791}{%
           family={Collobert},
           familyi={C\bibinitperiod},
           given={Ronan},
           giveni={R\bibinitperiod}}}%
        {{hash=ecd149fdcb3e0503881d49e545744c3d}{%
           family={Dollár},
           familyi={D\bibinitperiod},
           given={Piotr},
           giveni={P\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cambridge, MA, USA}%
      }
      \list{publisher}{1}{%
        {MIT Press}%
      }
      \strng{namehash}{b1431f941c47a07c20f03df6748449f7}
      \strng{fullhash}{b1431f941c47a07c20f03df6748449f7}
      \strng{bibnamehash}{b1431f941c47a07c20f03df6748449f7}
      \strng{authorbibnamehash}{b1431f941c47a07c20f03df6748449f7}
      \strng{authornamehash}{b1431f941c47a07c20f03df6748449f7}
      \strng{authorfullhash}{b1431f941c47a07c20f03df6748449f7}
      \field{sortinit}{8}
      \field{sortinithash}{a231b008ebf0ecbe0b4d96dcc159445f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent object detection systems rely on two critical steps: (1) a set of object proposals is predicted as efficiently as possible, and (2) this set of candidate proposals is then passed to an object classifier. Such approaches have been shown they can be fast, while achieving the state of the art in detection performance. In this paper, we propose a new way to generate object proposals, introducing an approach based on a discriminative convolutional network. Our model is trained jointly with two objectives: given an image patch, the first part of the system outputs a class-agnostic segmentation mask, while the second part of the system outputs the likelihood of the patch being centered on a full object. At test time, the model is efficiently applied on the whole test image and generates a set of segmentation masks, each of them being assigned with a corresponding object likelihood score. We show that our model yields significant improvements over state-of-the-art object proposal algorithms. In particular, compared to previous approaches, our model obtains substantially higher object recall using fewer proposals. We also show that our model is able to generalize to unseen categories it has not seen during training. Unlike all previous approaches for generating object masks, we do not rely on edges, superpixels, or any other form of low-level segmentation.}
      \field{booktitle}{Proceedings of the 29th International Conference on Neural Information Processing Systems - Volume 2}
      \field{eventday}{1}
      \field{eventmonth}{6}
      \field{eventyear}{2015}
      \field{series}{NIPS'15}
      \field{title}{Learning to segment object candidates}
      \field{venue}{Montreal, Canada}
      \field{year}{2015}
      \field{dateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{1990\bibrangedash 1998}
      \range{pages}{9}
      \verb{doi}
      \verb 10.48550/arXiv.1506.06204
      \endverb
    \endentry
    \entry{InstanceFCN}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=c80dbb87ee6058c0b81ab8ee017f1170}{%
           family={Dai},
           familyi={D\bibinitperiod},
           given={Jifeng},
           giveni={J\bibinitperiod}}}%
        {{hash=6b4b60e909e78633945f3f9c9dc83e01}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Kaiming},
           giveni={K\bibinitperiod}}}%
        {{hash=cf1f0547c288eff0857572dcc12516bb}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Yi},
           giveni={Y\bibinitperiod}}}%
        {{hash=bb295293acacd54387339079ebbe4ead}{%
           family={Ren},
           familyi={R\bibinitperiod},
           given={Shaoqing},
           giveni={S\bibinitperiod}}}%
        {{hash=f85751488058842b5777c7b4074077b5}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Jian},
           giveni={J\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer International Publishing}%
      }
      \strng{namehash}{5e6d93db6ec8079f3e9ef43677e0e71c}
      \strng{fullhash}{61fb80a05fca154e93a2708bca39a27b}
      \strng{bibnamehash}{5e6d93db6ec8079f3e9ef43677e0e71c}
      \strng{authorbibnamehash}{5e6d93db6ec8079f3e9ef43677e0e71c}
      \strng{authornamehash}{5e6d93db6ec8079f3e9ef43677e0e71c}
      \strng{authorfullhash}{61fb80a05fca154e93a2708bca39a27b}
      \field{sortinit}{8}
      \field{sortinithash}{a231b008ebf0ecbe0b4d96dcc159445f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Fully convolutional networks (FCNs) have been proven very successful for semantic segmentation, but the FCN outputs are unaware of object instances. In this paper, we develop FCNs that are capable of proposing instance-level segment candidates. In contrast to the previous FCN that generates one score map, our FCN is designed to compute a small set of instance-sensitive score maps, each of which is the outcome of a pixel-wise classifier of a relative position to instances. On top of these instance-sensitive score maps, a simple assembling module is able to output instance candidate at each position. In contrast to the recent DeepMask method for segmenting instances, our method does not have any high-dimensional layer related to the mask resolution, but instead exploits image local coherence for estimating instances. We present competitive results of instance segment proposal on both PASCAL VOC and MS COCO.}
      \field{booktitle}{Computer Vision -- ECCV 2016}
      \field{day}{17}
      \field{eventday}{11}
      \field{eventendday}{14}
      \field{eventendmonth}{10}
      \field{eventendyear}{2016}
      \field{eventmonth}{10}
      \field{eventyear}{2016}
      \field{isbn}{978-3-319-46466-4}
      \field{month}{9}
      \field{title}{Instance-Sensitive Fully Convolutional Networks}
      \field{venue}{Amsterdam, The Netherlands}
      \field{volume}{9910}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{534\bibrangedash 549}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1007/978-3-319-46466-4_32
      \endverb
    \endentry
    \entry{TensorMask}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=ce10870c303bf2f78994acd2df305b39}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Xinlei},
           giveni={X\bibinitperiod}}}%
        {{hash=bd5dadbe57bedc5957c19a3154c4d424}{%
           family={Girshick},
           familyi={G\bibinitperiod},
           given={Ross},
           giveni={R\bibinitperiod}}}%
        {{hash=6b4b60e909e78633945f3f9c9dc83e01}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Kaiming},
           giveni={K\bibinitperiod}}}%
        {{hash=37843fd1a96680e7945ac1a059b6e8b8}{%
           family={Dollar},
           familyi={D\bibinitperiod},
           given={Piotr},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{c052b19c195b0460e4a518a266d2fdd4}
      \strng{fullhash}{beac81392e810c854566cb4b60bad0bb}
      \strng{bibnamehash}{c052b19c195b0460e4a518a266d2fdd4}
      \strng{authorbibnamehash}{c052b19c195b0460e4a518a266d2fdd4}
      \strng{authornamehash}{c052b19c195b0460e4a518a266d2fdd4}
      \strng{authorfullhash}{beac81392e810c854566cb4b60bad0bb}
      \field{sortinit}{8}
      \field{sortinithash}{a231b008ebf0ecbe0b4d96dcc159445f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Sliding-window object detectors that generate bounding-box object predictions over a dense, regular grid have advanced rapidly and proven popular. In contrast, modern instance segmentation approaches are dominated by methods that first detect object bounding boxes, and then crop and segment these regions, as popularized by Mask R-CNN. In this work, we investigate the paradigm of dense sliding-window instance segmentation, which is surprisingly under-explored. Our core observation is that this task is fundamentally different than other dense prediction tasks such as semantic segmentation or bounding-box object detection, as the output at every spatial location is itself a geometric structure with its own spatial dimensions. To formalize this, we treat dense instance segmentation as a prediction task over 4D tensors and present a general framework called TensorMask that explicitly captures this geometry and enables novel operators on 4D tensors. We demonstrate that the tensor view leads to large gains over baselines that ignore this structure, and leads to results comparable to Mask R-CNN. These promising results suggest that TensorMask can serve as a foundation for novel advances in dense mask prediction and a more complete understanding of the task. Code will be made available.}
      \field{booktitle}{2019 IEEE/CVF International Conference on Computer Vision (ICCV)}
      \field{day}{27}
      \field{eventday}{27}
      \field{eventendday}{2}
      \field{eventendmonth}{11}
      \field{eventendyear}{2019}
      \field{eventmonth}{10}
      \field{eventyear}{2019}
      \field{isbn}{978-1-7281-4803-8}
      \field{issn}{2380-7504}
      \field{month}{2}
      \field{title}{TensorMask: A Foundation for Dense Object Segmentation}
      \field{venue}{Seoul, Korea}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{2061\bibrangedash 2069}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1109/ICCV.2019.00215
      \endverb
      \keyw{Tensile stress;Task analysis;Windows;Shape;Proposals;Two dimensional displays;Image segmentation}
    \endentry
    \entry{Yang2019}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=97093fcd2042d9d5b91f723e6b9eea5d}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Linjie},
           giveni={L\bibinitperiod}}}%
        {{hash=6e919e331b7e42830750c951e4451ba7}{%
           family={Fan},
           familyi={F\bibinitperiod},
           given={Yuchen},
           giveni={Y\bibinitperiod}}}%
        {{hash=7d37b8eddbfa29411e3973fce2cd7773}{%
           family={Xu},
           familyi={X\bibinitperiod},
           given={Ning},
           giveni={N\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{43b17f10191561398bbbe049824b6561}
      \strng{fullhash}{43b17f10191561398bbbe049824b6561}
      \strng{bibnamehash}{43b17f10191561398bbbe049824b6561}
      \strng{authorbibnamehash}{43b17f10191561398bbbe049824b6561}
      \strng{authornamehash}{43b17f10191561398bbbe049824b6561}
      \strng{authorfullhash}{43b17f10191561398bbbe049824b6561}
      \field{sortinit}{8}
      \field{sortinithash}{a231b008ebf0ecbe0b4d96dcc159445f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper we present a new computer vision task, named video instance segmentation. The goal of this new task is simultaneous detection, segmentation and tracking of instances in videos. In words, it is the first time that the image instance segmentation problem is extended to the video domain. To facilitate research on this new task, we propose a large-scale benchmark called YouTube-VIS, which consists of 2,883 high-resolution YouTube videos, a 40-category label set and 131k high-quality instance masks. In addition, we propose a novel algorithm called MaskTrack R-CNN for this task. Our new method introduces a new tracking branch to Mask R-CNN to jointly perform the detection, segmentation and tracking tasks simultaneously. Finally, we evaluate the proposed method and several strong baselines on our new dataset. Experimental results clearly demonstrate the advantages of the proposed algorithm and reveal insight for future improvement. We believe the video instance segmentation task will motivate the community along the line of research for video understanding.}
      \field{booktitle}{2019 IEEE/CVF International Conference on Computer Vision (ICCV)}
      \field{day}{27}
      \field{issn}{2380-7504}
      \field{month}{2}
      \field{title}{Video Instance Segmentation}
      \field{venue}{Seoul, Korea}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{pages}{5187\bibrangedash 5196}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/ICCV.2019.00529
      \endverb
      \keyw{Image segmentation;Task analysis;Motion segmentation;Semantics;Benchmark testing;Object detection;Object segmentation}
    \endentry
    \entry{Voigtlaender2019}{inproceedings}{}
      \name{author}{7}{}{%
        {{hash=f216a3bd9b455b394f1010a5201ddf46}{%
           family={Voigtlaender},
           familyi={V\bibinitperiod},
           given={Paul},
           giveni={P\bibinitperiod}}}%
        {{hash=49e44ce25b783602ee9339c2695b1797}{%
           family={Krause},
           familyi={K\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=9eb84424ec2ae8b1fcdbc0fba52982b8}{%
           family={Osep},
           familyi={O\bibinitperiod},
           given={Aljosa},
           giveni={A\bibinitperiod}}}%
        {{hash=8c336c193892d2f1d5a6c952eb5b1d4c}{%
           family={Luiten},
           familyi={L\bibinitperiod},
           given={Jonathon},
           giveni={J\bibinitperiod}}}%
        {{hash=2af4e56d13e5ef272e230e3a97a09857}{%
           family={Sekar},
           familyi={S\bibinitperiod},
           given={Berin\bibnamedelimb Balachandar\bibnamedelima Gnana},
           giveni={B\bibinitperiod\bibinitdelim B\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=06133052ac3c14a188601b82783cd4f7}{%
           family={Geiger},
           familyi={G\bibinitperiod},
           given={Andreas},
           giveni={A\bibinitperiod}}}%
        {{hash=89e3f1ee83923a2c7a44b7597b35fccd}{%
           family={Leibe},
           familyi={L\bibinitperiod},
           given={Bastian},
           giveni={B\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{ea307cebf823767e2f78e68f127a67f8}
      \strng{fullhash}{acf33eaa9dd8dae4270877648c42e895}
      \strng{bibnamehash}{ea307cebf823767e2f78e68f127a67f8}
      \strng{authorbibnamehash}{ea307cebf823767e2f78e68f127a67f8}
      \strng{authornamehash}{ea307cebf823767e2f78e68f127a67f8}
      \strng{authorfullhash}{acf33eaa9dd8dae4270877648c42e895}
      \field{sortinit}{8}
      \field{sortinithash}{a231b008ebf0ecbe0b4d96dcc159445f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper extends the popular task of multi-object tracking to multi-object tracking and segmentation (MOTS). Towards this goal, we create dense pixel-level annotations for two existing tracking datasets using a semi-automatic annotation procedure. Our new annotations comprise 65,213 pixel masks for 977 distinct objects (cars and pedestrians) in 10,870 video frames. For evaluation, we extend existing multi-object tracking metrics to this new task. Moreover, we propose a new baseline method which jointly addresses detection, tracking, and segmentation with a single convolutional network. We demonstrate the value of our datasets by achieving improvements in performance when training on MOTS annotations. We believe that our datasets, metrics and baseline will become a valuable resource towards developing multi-object tracking approaches that go beyond 2D bounding boxes. We make our annotations, code, and models available at https://www.vision.rwth-aachen.de/page/mots.}
      \field{booktitle}{2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
      \field{day}{9}
      \field{eventday}{15}
      \field{eventendday}{20}
      \field{eventendmonth}{6}
      \field{eventendyear}{2019}
      \field{eventmonth}{6}
      \field{eventyear}{2019}
      \field{issn}{2575-7075}
      \field{month}{1}
      \field{title}{MOTS: Multi-Object Tracking and Segmentation}
      \field{venue}{Long Beach, CA, USA}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{eventenddateera}{ce}
      \field{eventdateera}{ce}
      \field{pages}{7934\bibrangedash 7943}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/CVPR.2019.00813
      \endverb
      \keyw{Motion and Tracking;Deep Learning ; Segmentation;Grouping and Shape}
    \endentry
    \entry{Zhou2023}{article}{}
      \name{author}{5}{}{%
        {{hash=a67dc09f1a945d62e5a2835608f8c490}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Tianfei},
           giveni={T\bibinitperiod}}}%
        {{hash=614198715471d146b77068d419298d26}{%
           family={Porikli},
           familyi={P\bibinitperiod},
           given={Fatih},
           giveni={F\bibinitperiod}}}%
        {{hash=711a2f7c261a07216e1fa1355b411a84}{%
           family={Crandall},
           familyi={C\bibinitperiod},
           given={David\bibnamedelima J.},
           giveni={D\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=d3bc7b9be5061411a87f19fcdf39499c}{%
           family={Van\bibnamedelima Gool},
           familyi={V\bibinitperiod\bibinitdelim G\bibinitperiod},
           given={Luc},
           giveni={L\bibinitperiod}}}%
        {{hash=1422e568eb32feb9f85af63cdb196905}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Wenguan},
           giveni={W\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{f310ded12371bb10d9513865f936cac7}
      \strng{fullhash}{642dd56a861c9190927c339fc8d50fad}
      \strng{bibnamehash}{f310ded12371bb10d9513865f936cac7}
      \strng{authorbibnamehash}{f310ded12371bb10d9513865f936cac7}
      \strng{authornamehash}{f310ded12371bb10d9513865f936cac7}
      \strng{authorfullhash}{642dd56a861c9190927c339fc8d50fad}
      \field{sortinit}{8}
      \field{sortinithash}{a231b008ebf0ecbe0b4d96dcc159445f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Video segmentation—partitioning video frames into multiple segments or objects—plays a critical role in a broad range of practical applications, from enhancing visual effects in movie, to understanding scenes in autonomous driving, to creating virtual background in video conferencing. Recently, with the renaissance of connectionism in computer vision, there has been an influx of deep learning based approaches for video segmentation that have delivered compelling performance. In this survey, we comprehensively review two basic lines of research — generic object segmentation (of unknown categories) in videos, and video semantic segmentation — by introducing their respective task settings, background concepts, perceived need, development history, and main challenges. We also offer a detailed overview of representative literature on both methods and datasets. We further benchmark the reviewed methods on several well-known datasets. Finally, we point out open issues in this field, and suggest opportunities for further research. We also provide a public website to continuously track developments in this fast advancing field: https://github.com/tfzhou/VS-Survey.}
      \field{bookpagination}{page}
      \field{day}{1}
      \field{issn}{1939-3539}
      \field{journaltitle}{IEEE Transactions on Pattern Analysis and Machine Intelligence}
      \field{month}{6}
      \field{number}{6}
      \field{pagination}{column}
      \field{title}{A Survey on Deep Learning Technique for Video Segmentation}
      \field{volume}{45}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{pages}{7099\bibrangedash 7122}
      \range{pages}{24}
      \verb{doi}
      \verb 10.1109/TPAMI.2022.3225573
      \endverb
      \keyw{Object segmentation;Automobiles;Semantic segmentation;Task analysis;Motion segmentation;Deep learning;Roads;Video segmentation;video object segmentation;video semantic segmentation;deep learning}
    \endentry
  \enddatalist
\endrefsection
\endinput

